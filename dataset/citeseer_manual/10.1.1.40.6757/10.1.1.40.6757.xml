<document id="10.1.1.40.6757"><title src="SVM HeaderParse 0.1">Fast Algorithms for Mining Association Rules</title><abstract src="SVM HeaderParse 0.1">We consider the problem of discovering association rules between items in a large database of sales transactions. We present two new algorithms for solving this problem that are fundamentally different from the known algorithms. Empirical evaluation shows that these algorithms outperform the known algorithms by factors ranging from three for small problems to more than an order of magnitude for large problems. We also show how the best features of the two proposed algorithms can be combined into a hybrid algorithm, called AprioriHybrid. Scale-up experiments show that AprioriHybrid scales linearly with the number of transactions. AprioriHybrid also has excellent scale-up properties with respect to the transaction size and the number of items in the database.</abstract><keywords></keywords><authors><author id="1018866"><name src="SVM HeaderParse 0.1">Rakesh Agrawal</name><address src="SVM HeaderParse 0.1">650 Harry Road, San Jose, CA 95120</address><order>1</order></author><author id="1018867"><name src="SVM HeaderParse 0.1">Ramakrishnan Srikant</name><address src="SVM HeaderParse 0.1">650 Harry Road, San Jose, CA 95120</address><order>2</order></author></authors><citations src="ParsCit 1.0"><citation id="7306061"><authors>R Agrawal,C Faloutsos,A Swami</authors><title>Efficient similarity search in sequence databases</title><venue>In Proc. of the Fourth International Conference on Foundations of Data Organization and Algorithms</venue><venType>CONFERENCE</venType><year>1993</year><pubAddress>Chicago</pubAddress><raw>R. Agrawal, C. Faloutsos, and A. Swami. Efficient similarity search in sequence databases. In Proc. of the Fourth International Conference on Foundations of Data Organization and Algorithms, Chicago, October 1993.</raw><contexts><context>corresponding transaction. We call the number of items in an itemset its size, and call an itemset of size k a k-itemset. Items within an itemset are kept in lexicographic order. We use the notation c[1] \Delta c[2] \Delta . . . \Delta c[k] to represent a kitemset c consisting of items c[1]; c[2]; . . .c[k], where c[1] ! c[2] ! . . . ! c[k]. If c = X \Delta Y and Y is an m-itemset, we also call Y an </context><context>vering association rules, some other problems that we have looked into include 12 the enhancement of the database capability with classification queries [2] and similarity queries over time sequences [1]. We believe that database mining is an important new application area for databases, combining commercial interest with intriguing research questions. Acknowledgment We wish to thank Mike Carey for h</context></contexts></citation><citation id="7306062"><authors>R Agrawal,S Ghosh,T Imielinski,B Iyer,A Swami</authors><title>An interval classifier for database mining applications</title><venue>In Proc. of the VLDB Conference</venue><venType>CONFERENCE</venType><year>1992</year><pages>560--573</pages><pubAddress>Vancouver, British Columbia, Canada</pubAddress><raw>R. Agrawal, S. Ghosh, T. Imielinski, B. Iyer, and A. Swami. An interval classifier for database mining applications. In Proc. of the VLDB Conference, pages 560--573, Vancouver, British Columbia, Canada, 1992.</raw><contexts><context>g transaction. We call the number of items in an itemset its size, and call an itemset of size k a k-itemset. Items within an itemset are kept in lexicographic order. We use the notation c[1] \Delta c[2] \Delta . . . \Delta c[k] to represent a kitemset c consisting of items c[1]; c[2]; . . .c[k], where c[1] ! c[2] ! . . . ! c[k]. If c = X \Delta Y and Y is an m-itemset, we also call Y an m-extension </context><context>se mining problem. Besides the problem of discovering association rules, some other problems that we have looked into include 12 the enhancement of the database capability with classification queries [2] and similarity queries over time sequences [1]. We believe that database mining is an important new application area for databases, combining commercial interest with intriguing research questions. A</context></contexts></citation><citation id="7306063"><authors>R Agrawal,T Imielinski,A Swami</authors><title>Database mining: A performance perspective</title><venue>IEEE Transactions on Knowledge and Data Engineering</venue><venType>JOURNAL</venType><year>1993</year><volume>5</volume><raw>R. Agrawal, T. Imielinski, and A. Swami. Database mining: A performance perspective. IEEE Transactions on Knowledge and Data Engineering, 5(6):914--925, December 1993. Special Issue on Learning and Discovery in KnowledgeBased Databases.</raw><contexts><context>s excellent scale-up properties, opening up the feasibility of mining association rules over very large databases. The problem of finding association rules falls within the purview of database mining [3] [12], also called knowledge discovery in databases [21]. Related, but not directly applicable, work includes the induction of classification rules [8] [11] [22], discovery of causal rules [19], learn</context></contexts></citation><citation id="7306064"><authors>R Agrawal,T Imielinski,A Swami</authors><title>Mining association rules between sets of items in large databases</title><venue>In Proc. of the ACM SIGMOD Conference on Management of Data</venue><venType>CONFERENCE</venType><year>1993</year><pubAddress>Washington, D.C</pubAddress><raw>R. Agrawal, T. Imielinski, and A. Swami. Mining association rules between sets of items in large databases. In Proc. of the ACM SIGMOD Conference on Management of Data, Washington, D.C., May 1993.</raw><contexts><context>ged by database technology, that enable marketers to develop and implement customized marketing programs and strategies [6]. The problem of mining association rules over basket data was introduced in [4]. An example of such a rule might be that 98% of customers that purchase Visiting from the Department of Computer Science, University of Wisconsin, Madison. Permission to copy without fee all or part </context><context>on buying patterns. The databases involved in these applications are very large. It is imperative, therefore, to have fast algorithms for this task. The following is a formal statement of the problem [4]: Let I = fi 1 ; i 2 ; . . . ; i m g be a set of literals, called items. Let D be a set of transactions, where each transaction T is a set of items such that T ` I. Associated with each transaction is</context><context>if c% of transactions in D that contain X also contain Y . The rule X =) Y has support s in the transaction set D if s% of transactions in D contain X [Y . Our rules are somewhat more general than in [4] in that we allow a consequent to have more than one item. Given a set of transactions D, the problem of mining association rules is to generate all association rules that have support and confidence </context><context>le, D could be a data file, a relational table, or the result of a relational expression. An algorithm for finding all association rules, henceforth referred to as the AIS algorithm, was presented in [4]. Another algorithm for this task, called the SETM algorithm, has been proposed in [13]. In this paper, we present two new algorithms, Apriori and AprioriTid, that differ fundamentally from these algo</context><context>may use our algorithms as the engine of the discovery process. 1.1 Problem Decomposition and Paper Organization The problem of discovering all association rules can be decomposed into two subproblems [4]: 1. Find all sets of items (itemsets) that have transaction support above minimum support. The support for an itemset is the number of transactions that contain the itemset. Itemsets with minimum sup</context></contexts></citation><citation id="7306065"><authors>R Agrawal,R Srikant</authors><title>Fast algorithms for mining association rules in large databases</title><venue>Research Report RJ 9839, IBM Almaden Research</venue><venType>CONFERENCE</venType><year>1994</year><pubAddress>Center, San Jose, California</pubAddress><raw>R. Agrawal and R. Srikant. Fast algorithms for mining association rules in large databases. Research Report RJ 9839, IBM Almaden Research Center, San Jose, California, June 1994.</raw><contexts><context>port(a) is at least minconf. We need to consider all subsets of l to generate rules with multiple consequents. Due to lack of space, we do not discuss this subproblem further, but refer the reader to [5] for a fast algorithm. In Section 3, we show the relative performance of the proposed Apriori and AprioriTid algorithms against the AIS [4] and SETM [13] algorithms. To make the paper self-contained, </context><context>didate k-itemsets when the TIDs C k of the generating transactions are kept associated with the candidates. given transaction t. Section 2.1.2 describes the subset function used for this purpose. See [5] for a discussion of buffer management. 1) L1 = flarge 1-itemsetsg; 2) for ( k = 2; Lk\Gamma1 6= ;; k++ ) do begin 3) Ck = apriori-gen(Lk\Gamma1 ); // New candidates 4) forall transactions t 2 D do be</context><context>he corresponding transaction because an entry in C k includes all candidate k-itemsets contained in the transaction. In Section 2.2.1, we give the data structures used to implement the algorithm. See [5] for a proof of correctness and a discussion of buffer management. 1) L1 = flarge 1-itemsetsg; 2) C 1 = database D; 3) for ( k = 2; Lk\Gamma1 6= ;; k++ ) do begin 4) Ck = apriori-gen(Lk\Gamma1 ); // N</context><context>n implemented on several data repositories, including the AIX file system, DB2/MVS, and DB2/6000. We have also tested these algorithms against real customer data, the details of which can be found in [5]. In the future, we plan to extend this work along the following dimensions: ffl Multiple taxonomies (is-a hierarchies) over items are often available. An example of such a hierarchy is that a dish wa</context></contexts></citation><citation id="7306066"><authors>D S Associates</authors><title>The new direct marketing. Business One</title><year>1990</year><pubAddress>Irwin, Illinois</pubAddress><raw>D. S. Associates. The new direct marketing. Business One Irwin, Illinois, 1990.</raw><contexts><context> They are interested in instituting information-driven marketing processes, managed by database technology, that enable marketers to develop and implement customized marketing programs and strategies [6]. The problem of mining association rules over basket data was introduced in [4]. An example of such a rule might be that 98% of customers that purchase Visiting from the Department of Computer Scienc</context></contexts></citation><citation id="7306067"><authors>R Brachman</authors><title>et al. Integrated support for data archeology</title><venue>In AAAI-93 Workshop on Knowledge Discovery in Databases</venue><venType>CONFERENCE</venType><year>1993</year><raw>R. Brachman et al. Integrated support for data archeology. In AAAI-93 Workshop on Knowledge Discovery in Databases, July 1993.</raw><contexts><context> useful or interesting is often application-dependent. The need for a human in the loop and providing tools to allow human guidance of the rule discovery process has been articulated, for example, in [7] [14]. We do not discuss these issues in this paper, except to point out that these are necessary features of a rule discovery system that may use our algorithms as the engine of the discovery process</context></contexts></citation><citation id="7306068"><authors>L Breiman,J H Friedman,R A Olshen,C J Stone</authors><title>Classification and Regression Trees</title><year>1984</year><pubAddress>Wadsworth, Belmont</pubAddress><raw>L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classification and Regression Trees. Wadsworth, Belmont, 1984.</raw><contexts><context>ules falls within the purview of database mining [3] [12], also called knowledge discovery in databases [21]. Related, but not directly applicable, work includes the induction of classification rules [8] [11] [22], discovery of causal rules [19], learning of logical definitions [18], fitting of functions to data [15], and clustering [9] [10]. The closest work in the machine learning literature is the</context></contexts></citation><citation id="7306069"><authors>P Cheeseman</authors><title>et al. Autoclass: A bayesian classification system</title><venue>In 5th Int'l Conf. on Machine Learning</venue><venType>CONFERENCE</venType><year>1988</year><publisher>Morgan</publisher><pubAddress>Kaufman</pubAddress><raw>P. Cheeseman et al. Autoclass: A bayesian classification system. In 5th Int'l Conf. on Machine Learning. Morgan Kaufman, June 1988.</raw><contexts><context> applicable, work includes the induction of classification rules [8] [11] [22], discovery of causal rules [19], learning of logical definitions [18], fitting of functions to data [15], and clustering [9] [10]. The closest work in the machine learning literature is the KID3 algorithm presented in [20]. If used for finding all association rules, this algorithm will make as many passes over the data as </context></contexts></citation><citation id="7306070"><authors>D H Fisher</authors><title>Knowledge acquisition via incremental conceptual clustering</title><venue>Machine Learning</venue><venType>CONFERENCE</venType><year>1987</year><volume>2</volume><raw>D. H. Fisher. Knowledge acquisition via incremental conceptual clustering. Machine Learning, 2(2), 1987.</raw><contexts><context>licable, work includes the induction of classification rules [8] [11] [22], discovery of causal rules [19], learning of logical definitions [18], fitting of functions to data [15], and clustering [9] [10]. The closest work in the machine learning literature is the KID3 algorithm presented in [20]. If used for finding all association rules, this algorithm will make as many passes over the data as the n</context></contexts></citation><citation id="7306071"><authors>J Han,Y Cai,N Cercone</authors><title>Knowledge discovery in databases: An attribute oriented approach</title><venue>In Proc. of the VLDB Conference</venue><venType>CONFERENCE</venType><year>1992</year><pages>547--559</pages><pubAddress>Vancouver, British Columbia, Canada</pubAddress><raw>J. Han, Y. Cai, and N. Cercone. Knowledge discovery in databases: An attribute oriented approach. In Proc. of the VLDB Conference, pages 547--559, Vancouver, British Columbia, Canada, 1992.</raw><contexts><context> falls within the purview of database mining [3] [12], also called knowledge discovery in databases [21]. Related, but not directly applicable, work includes the induction of classification rules [8] [11] [22], discovery of causal rules [19], learning of logical definitions [18], fitting of functions to data [15], and clustering [9] [10]. The closest work in the machine learning literature is the KID3</context></contexts></citation><citation id="7306072"><authors>M Holsheimer,A Siebes</authors><title>Data mining: The search for knowledge in databases</title><venType>TECHREPORT</venType><year>1994</year><pubAddress>CWI, Netherlands</pubAddress><tech>Technical Report CS-R9406</tech><raw>M. Holsheimer and A. Siebes. Data mining: The search for knowledge in databases. Technical Report CS-R9406, CWI, Netherlands, 1994.</raw><contexts><context>cellent scale-up properties, opening up the feasibility of mining association rules over very large databases. The problem of finding association rules falls within the purview of database mining [3] [12], also called knowledge discovery in databases [21]. Related, but not directly applicable, work includes the induction of classification rules [8] [11] [22], discovery of causal rules [19], learning o</context></contexts></citation><citation id="7306073"><authors>M Houtsma,A Swami</authors><title>Set-oriented mining of association rules</title><venue>Research Report RJ 9567, IBM Almaden Research</venue><venType>CONFERENCE</venType><year>1993</year><pubAddress>Center, San Jose, California</pubAddress><raw>M. Houtsma and A. Swami. Set-oriented mining of association rules. Research Report RJ 9567, IBM Almaden Research Center, San Jose, California, October 1993.</raw><contexts><context>on. An algorithm for finding all association rules, henceforth referred to as the AIS algorithm, was presented in [4]. Another algorithm for this task, called the SETM algorithm, has been proposed in [13]. In this paper, we present two new algorithms, Apriori and AprioriTid, that differ fundamentally from these algorithms. We present experimental results showing 1 that the proposed algorithms always o</context><context>his subproblem further, but refer the reader to [5] for a fast algorithm. In Section 3, we show the relative performance of the proposed Apriori and AprioriTid algorithms against the AIS [4] and SETM [13] algorithms. To make the paper self-contained, we include an overview of the AIS and SETM algorithms in this section. We also describe how the Apriori and AprioriTid algorithms can be combined into a </context><context>d they become the seed for the next pass. This process continues until no new large itemsets are found. The Apriori and AprioriTid algorithms we propose differ fundamentally from the AIS [4] and SETM [13] algorithms in terms of which candidate itemsets are counted in a pass and in the way that those candidates are generated. In both the AIS and SETM algorithms, candidate itemsets are generated on-the-</context><context>g AIX 3.2. The data resided in the AIX file system and was stored on a 2GB SCSI 3.5&amp;quot; drive, with measured sequential throughput of about 2 MB/second. We first give an overview of the AIS [4] and SETM [13] algorithms against which we compare the performance of the Apriori and AprioriTid algorithms. We then describe the synthetic datasets used in the performance evaluation and show the performance resul</context><context>s, or the counts of the corresponding entries are increased if they were created by an earlier transaction. See [4] for further details of the AIS algorithm. 3.2 The SETM Algorithm The SETM algorithm [13] was motivated by the desire to use SQL to compute large itemsets. Like AIS, the SETM algorithm also generates candidates onthe -fly based on transactions read from the database. It thus generates and</context></contexts></citation><citation id="7306074"><authors>R Krishnamurthy,T Imielinski</authors><title>Practitioner problems in need of database research: Research directions in knowledge discovery</title><venue>SIGMOD RECORD</venue><venType>JOURNAL</venType><year>1991</year><volume>20</volume><raw>R. Krishnamurthy and T. Imielinski. Practitioner problems in need of database research: Research directions in knowledge discovery. SIGMOD  RECORD, 20(3):76--78, September 1991.</raw></citation><citation id="7306075"><authors>P Langley,H Simon,G Bradshaw,J Zytkow</authors><title>Scientific Discovery: Computational Explorations of the Creative Process</title><year>1987</year><publisher>MIT Press</publisher><raw>P. Langley, H. Simon, G. Bradshaw, and J. Zytkow. Scientific Discovery: Computational Explorations of the Creative Process. MIT Press, 1987.</raw><contexts><context>ted, but not directly applicable, work includes the induction of classification rules [8] [11] [22], discovery of causal rules [19], learning of logical definitions [18], fitting of functions to data [15], and clustering [9] [10]. The closest work in the machine learning literature is the KID3 algorithm presented in [20]. If used for finding all association rules, this algorithm will make as many pass</context></contexts></citation><citation id="7306076"><authors>H Mannila,K-J Raiha</authors><title>Dependency inference</title><venue>In Proc. of the VLDB Conference</venue><venType>CONFERENCE</venType><year>1987</year><pages>155--158</pages><pubAddress>Brighton, England</pubAddress><raw>H. Mannila and K.-J. Raiha. Dependency inference. In Proc. of the VLDB Conference, pages 155--158, Brighton, England, 1987.</raw><contexts><context>ver the data as the number of combinations of items in the antecedent, which is exponentially large. Related work in the database literature is the work on inferring functional dependencies from data [16]. Functional dependencies are rules requiring strict satisfaction. Consequently, having determined a dependency X ! A, the algorithms in [16] consider any other dependency of the form X + Y ! A redund</context></contexts></citation><citation id="7306077"><authors>H Mannila,H Toivonen,A I Verkamo</authors><title>Efficient algorithms for discovering association rules</title><venue>In KDD-94: AAAI Workshop on Knowledge Discovery in Databases</venue><venType>CONFERENCE</venType><year>1994</year><raw>H. Mannila, H. Toivonen, and A. I. Verkamo. Efficient algorithms for discovering association rules. In KDD-94: AAAI Workshop on Knowledge Discovery in Databases, July 1994.</raw><contexts><context>tep, we delete all itemsets c 2 C k such that some (k \Gamma 1)-subset of c is not in L k\Gamma1 : 1 Concurrent to our work, the following two-step candidate generation procedure has been proposed in [17]: C 0 k = fX [ X 0 jX; X 0 2 L k\Gamma1 ; jX &amp;quot; X 0 j = k \Gamma 2g C k = fX 2 C 0 k jX contains k members of L k\Gamma1 g These two steps are similar to our join and prune steps respectively. However,</context></contexts></citation><citation id="7306078"><authors>S Muggleton,C Feng</authors><title>Efficient induction of logic programs</title><venue>Inductive Logic Programming</venue><venType>CONFERENCE</venType><year>1992</year><editors>In S. Muggleton, editor</editors><publisher>Academic Press</publisher><raw>S. Muggleton and C. Feng. Efficient induction of logic programs. In S. Muggleton, editor, Inductive Logic Programming. Academic Press, 1992.</raw><contexts><context>e discovery in databases [21]. Related, but not directly applicable, work includes the induction of classification rules [8] [11] [22], discovery of causal rules [19], learning of logical definitions [18], fitting of functions to data [15], and clustering [9] [10]. The closest work in the machine learning literature is the KID3 algorithm presented in [20]. If used for finding all association rules, th</context></contexts></citation><citation id="7306079"><authors>J Pearl</authors><title>Probabilistic reasoning in intelligent systems: Networks of plausible inference</title><year>1992</year><raw>J. Pearl. Probabilistic reasoning in intelligent systems: Networks of plausible inference, 1992.</raw><contexts><context> mining [3] [12], also called knowledge discovery in databases [21]. Related, but not directly applicable, work includes the induction of classification rules [8] [11] [22], discovery of causal rules [19], learning of logical definitions [18], fitting of functions to data [15], and clustering [9] [10]. The closest work in the machine learning literature is the KID3 algorithm presented in [20]. If used</context></contexts></citation><citation id="7306080"><authors>G Piatestsky-Shapiro</authors><title>Discovery, analysis, and presentation of strong rules</title><venue>Knowledge Discovery in Databases</venue><venType>CONFERENCE</venType><year>1991</year><editors>In G. Piatestsky-Shapiro, editor</editors><publisher>AAAI/MIT Press</publisher><raw>G. Piatestsky-Shapiro. Discovery, analysis, and presentation of strong rules. In G. Piatestsky-Shapiro, editor, Knowledge Discovery in Databases. AAAI/MIT Press, 1991.</raw><contexts><context>al rules [19], learning of logical definitions [18], fitting of functions to data [15], and clustering [9] [10]. The closest work in the machine learning literature is the KID3 algorithm presented in [20]. If used for finding all association rules, this algorithm will make as many passes over the data as the number of combinations of items in the antecedent, which is exponentially large. Related work </context><context>ules X ! Y and Y ! Z does not necessarily mean that X ! Z holds because the latter may not have minimum confidence. There has been work on quantifying the &amp;quot;usefulness &amp;quot; or &amp;quot;interestingness&amp;quot; of a rule [20]. What is useful or interesting is often application-dependent. The need for a human in the loop and providing tools to allow human guidance of the rule discovery process has been articulated, for exa</context></contexts></citation><citation id="7306081"><authors>G Piatestsky-Shapiro,editor</authors><title>Knowledge Discovery in Databases</title><year>1991</year><publisher>AAAI/MIT Press</publisher><raw>G. Piatestsky-Shapiro, editor. Knowledge Discovery in Databases. AAAI/MIT Press, 1991.</raw><contexts><context>lity of mining association rules over very large databases. The problem of finding association rules falls within the purview of database mining [3] [12], also called knowledge discovery in databases [21]. Related, but not directly applicable, work includes the induction of classification rules [8] [11] [22], discovery of causal rules [19], learning of logical definitions [18], fitting of functions to</context></contexts></citation><citation id="7306082"><authors>J R Quinlan</authors><title>C4.5: Programs for Machine Learning</title><year>1993</year><publisher>Morgan</publisher><pubAddress>Kaufman</pubAddress><raw>J. R. Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufman, 1993.</raw><contexts><context>s within the purview of database mining [3] [12], also called knowledge discovery in databases [21]. Related, but not directly applicable, work includes the induction of classification rules [8] [11] [22], discovery of causal rules [19], learning of logical definitions [18], fitting of functions to data [15], and clustering [9] [10]. The closest work in the machine learning literature is the KID3 algo</context></contexts></citation></citations><fileInfo><url>http://140.115.82.191/old/Agr0801/vldb94.ps</url><repID>rep1</repID><conversionTrace>*LEGACY*</conversionTrace><checkSums><checkSum><fileType>ps</fileType><sha1>ded225ad10f660a159fde9906516a2ec1261e48b</sha1></checkSum><checkSum><fileType>pdf</fileType><sha1>29946f1724104a47d73ab9c88cde129505b66ee5</sha1></checkSum></checkSums></fileInfo></document>