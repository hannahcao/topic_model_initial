Evaluation sheet		v0.1		
		Abstract	Authors	Title
citingPub	115382	In order to select a good hypothesis language (or model) from a collection of possible models,  one has to assess the generalization performance  of the hypothesis which is returned  by a learner that is bound to use that model.  This paper deals with a new and very efficient  way of assessing this generalization performance.  We present a new analysis which  characterizes the expected generalization error  of the hypothesis with least training error  in terms of the distribution of error rates of  the hypotheses in the model. This distribution  can be estimated very efficiently from  the data which immediately leads to an efficient  model selection algorithm. The analysis  predicts learning curves with a very high  precision and thus contributes to a better  understanding of why and when over-fitting  occurs. We present empirical studies (controlled  experiments on Boolean decision trees  and a large-scale text categorization problem)  which show that the model selection  algorithm leads to error rates wh...	Thorsten Joachims, Tobias Scheffer	Expected Error Analysis for Model Selection
evalAuthor	2788		Tobias Scheffer	

Evaluation Legend				
xx	strongly influenced			
x	influenced			
o	not really influenced			
oo	unrelated			
?	can't judge			

Evaluation				
======	id	Abstract	Authors	Title
oo	384105	"We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural  network. By removing unimportant weights from a network, several  improvements can be expected: better generalization, fewer  training examples required, and improved speed of learning and/or  classification. The basic idea is to use second-derivative information  to make a tradeoff between network complexity and training  set error. Experiments confirm the usefulness of the methods on a  real-world application.  1 INTRODUCTION  Most successful applications of neural network learning to real-world problems have  been achieved using highly structured networks of rather large size [for example  (Waibel, 1989; LeCun et al., 1990)]. As applications become more complex, the  networks will presumably become even larger and more structured. Design tools  and techniques for comparing different architectures and minimizing the network  size will be needed. More impor..."	John S. Denker, Sara A. Solla, Le Cun	Optimal Brain Damage
xx	26484	"Current methods to avoid overfitting are either data-oriented (using separate data for  validation) or representation-oriented (penalizing  complexity in the model). This paper  proposes process-oriented evaluation, where  a model's expected generalization error is  computed as a function of the search process  that led to it. The paper develops  the necessary theoretical framework, and applies  it to one type of learning: rule induction.  A process-oriented version of the CN2  rule learner is empirically compared with  the default CN2. The process-oriented version  is more accurate in a large majority  of the datasets, with high significance, and  also produces simpler models. Experiments  in artificial domains suggest that processoriented  evaluation is particularly useful in  high-dimensional domains.  1 INTRODUCTION  Overfitting avoidance is often considered the central  problem of machine learning (e.g., (Cheeseman &amp;  Oldford, 1994)). If a learner is sufficiently powerful,  it must guard against selec..."	Pedro Domingos	A Process-Oriented Heuristic for Model Selection
oo	553162	This paper explores the use of Support Vector Machines (SVMs) for learning text classifiers from examples. It analyzes the particular properties of learning with text data and identifies, why SVMs are appropriate for this task. Empirical results support the theoretical findings. SVMs achieve substantial improvements over the currently best performing methods and they behave robustly over a variety of different learning tasks. Furthermore, they are fully automatic, eliminating the need for manual parameter tuning.<		Text Categorization with Support Vector Machines: Learning with Many Relevant Features
o	23660	this paper is to provide such a comparison, and more importantly, to describe the general conclusions to which it has led. Relying on evidence that is divided between controlled experimental results and related formal analysis, we compare three well-known model selection algorithms. We attempt to identify their relative and absolute strengths and weaknesses, and we provide some general methods for analyzing the behavior and performance of model selection algorithms. Our hope is that these results may aid the informed practitioner in making an educated choice of model selection algorithm (perhaps based in part on some known properties of the model selection problem being confronted). The summary of the paper follows. In Section 2, we provide a formalization of the model selection problem. In this formalization, we isolate the problem of choosing the appropriate<	Andrew Y. Ng, Dana Ron, Michael Kearns, Yishay Mansour	An Experimental and Theoretical Comparison of Model Selection Methods
o	356583	In the feature subset selection problem, a learning algorithm is faced with the problem of selecting a relevant subset of features upon which to focus its attention, while ignoring the rest. To achieve the best possible performance with a particular learning algorithm on a particular domain, a feature subset selection method should consider how the algorithm and the training data interact. We explore the relation between optimal feature subset selection and relevance. Our wrapper method searches for an optimal feature subset tailored to a particular algorithm and a domain. We study the strengths and weaknesses of the wrapper approach and show improvements over the original design. We compare the wrapper approach to induction without feature subset selection and to Relief, a filter-based approach to feature subset selection. Significant improvement in accuracy on real problems is achieved for the two families of induction algorithms used: decision trees and Naive-Bayes. 1 Intr...<	H.John, Ron Kohavi	Wrappers for Feature Subset Selection
oo	441895	This paper introduces a new representation for Boolean functions, called decision lists,<	Ronald L. Rivest	Learning Decision Lists
o	187991	Strategies for increasing predictive accuracy through selective pruning have been widely adopted by researchers in decision tree induction. It  is easy to get the impression from research reports that there are statistical  reasons for believing that these overfitting avoidance strategies  do increase accuracy and that, as a research community, we are making  progress toward developing powerful, general methods for guarding  against overfitting in inducing decision trees. In fact, any overfitting  avoidance strategy amounts to a form of bias and, as such, may degrade  performance instead of improving it. If pruning methods have  often proven successful in empirical tests, this is due, not to the methods,  but to the choice of test problems. As examples in this article  illustrate, overfitting avoidance strategies are not better or worse, but  only more or less appropriate to specific application domains. We are  not---and cannot be---making progress toward methods both powerful  and general.  The ...	Cullen Schaffer	Overfitting Avoidance as Bias
oo	40099	. One of the surprising recurring phenomena observed in experiments with boosting is that the test error  of the generated hypothesis usually does not increase as its  size becomes very large, and often is observed to decrease  even after the training error reaches zero. In this paper, we  show that this phenomenon is related to the distribution of  margins of the training examples with respect to the generated  voting classification rule, where the margin of an  example is simply the difference between the number of  correct votes and the maximum number of votes received  by any incorrect label. We show that techniques used in the  analysis of Vapnik's support vector classifiers and of neural  networks with small weights can be applied to voting methods  to relate the margin distribution to the test error. We  also show theoretically and experimentally that boosting is  especially effective at increasing the margins of the training  examples. Finally, we compare our explanation to those  based on th...	Peter Bartlett, Robert Schapire, Wee Sun Lee, Yoav Freund	Boosting the Margin: A New Explanation for the Effectiveness of Voting Methods
xx	119453	". Model selection is considered the problem of choosing a ""good"" hypothesis language from a given ensemble of models. Here, a ""good"" model is one for which the true  (or generalization) error of the hypothesis returned by a learner which takes the model as  hypothesis language is low. The crucial part of model selection is to somehow assess the true  error of the apparently best hypothesis (the empirical minimizer) of a model. In this paper,  we discuss a new, very efficient approach to model selection. Our approach is inherently  Bayesian, but instead of using priors on target functions or hypotheses, we talk about priors  on error values -- which leads us to a new insightful characterization of the expected true error.  Consequently, our solution is based on the prior of error values for the given problem which is,  of course, unknown. But we show next that this prior can be estimated efficiently for a given  learning problem by recording the empirical errors of a constant number of randomly ..."	Thorsten Joachims, Tobias Scheffer	Estimating the Expected Error of Empirical Minimizers for Model Selection
xx	367009	In order to select a good hypothesis language (or model) from a collection of possible models, one has to assess the generalization performance of the hypothesis which is returned  by a learner that is bound to use some particular model. This paper deals with  a new and very efficient way of assessing this generalization performance. We present  a new analysis which characterizes the expected generalization error of the hypothesis  with least training error in terms of the distribution of error rates of the hypotheses in  the model. This distribution can be estimated very efficiently from the data which immediately  leads to an efficient model selection algorithm. The analysis predicts learning  curves with a very high precision and thus contributes to a better understanding of why  and when over-fitting occurs. We present empirical studies (controlled experiments on  Boolean decision trees and a large-scale text categorization problem) which show that the  model selection algorithm l...	Thorsten Joachims, Tobias Scheffer	Expected Error Analysis for Model Selection
x	16898	We investigate the structure of model selection problems via the bias/variance decomposition. In  particular, we characterize the essential aspects  of a model selection task by the bias and variance  profiles it generates over the sequence of hypothesis  classes. With this view, we develop a new  understanding of complexity-penalization methods:  First, the penalty terms can be interpreted as  postulating a particular profile for the variances  as a function of model complexity---if the postulated  and true profiles do not match, then systematic  under-fitting or over-fitting results, depending  on whether the penalty terms are too large or  too small. Second, we observe that it is generally  best to penalize according to the true variances  of the task, and therefore no fixed penalization  strategy is optimal across all problems. We then  use this characterization to introduce the notion  of easy versus hard model selection problems.  Here we show that if the variance profile grows  too rapidly in rela...	Dale Schuurmans, Dean Foster, Lyle H. Ungar	Characterizing the Generalization Performance of Model Selection Strategies
======				
