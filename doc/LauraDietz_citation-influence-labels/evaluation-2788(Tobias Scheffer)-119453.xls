Evaluation sheet		v0.1		
		Abstract	Authors	Title
citingPub	119453	". Model selection is considered the problem of choosing a ""good"" hypothesis language from a given ensemble of models. Here, a ""good"" model is one for which the true  (or generalization) error of the hypothesis returned by a learner which takes the model as  hypothesis language is low. The crucial part of model selection is to somehow assess the true  error of the apparently best hypothesis (the empirical minimizer) of a model. In this paper,  we discuss a new, very efficient approach to model selection. Our approach is inherently  Bayesian, but instead of using priors on target functions or hypotheses, we talk about priors  on error values -- which leads us to a new insightful characterization of the expected true error.  Consequently, our solution is based on the prior of error values for the given problem which is,  of course, unknown. But we show next that this prior can be estimated efficiently for a given  learning problem by recording the empirical errors of a constant number of randomly ..."	Thorsten Joachims, Tobias Scheffer	Estimating the Expected Error of Empirical Minimizers for Model Selection
evalAuthor	2788		Tobias Scheffer	

Evaluation Legend				
xx	strongly influenced			
x	influenced			
o	not really influenced			
oo	unrelated			
?	can't judge			

Evaluation				
======	id	Abstract	Authors	Title
o	147542	"We exhibit a theoretically founded algorithm T2 for agnostic PAC-learning of decision trees of at  most 2 levels, whose computation time is almost  linear in the size of the training set. We evaluate  the performance of this learning algorithm T2  on 15 common ""real-world"" datasets, and show  that for most of these datasets T2 provides simple  decision trees with little or no loss in predictive  power (compared with C4.5). In fact, for datasets  with continuous attributes its error rate tends to be  lower than that of C4.5. To the best of our knowledge  this is the first time that a PAC-learning algorithm  is shown to be applicable to ""real-world""  classification problems.  Since one can prove that T2 is an agnostic PAClearning  algorithm, T2 is guaranteed to produce  close to optimal 2-level decision trees from sufficiently  large training sets for any (!) distribution  of data. In this regard T2 differs strongly from all  other learning algorithms that are considered in  applied machine learning, for w..."	Peter Auer, Robert C. Holte, Wolfgang Maass	Theory and Applications of Agnostic PAC-Learning with Small Decision Trees
o	500489	The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input  vectors are non-linearly mapped to a very high-dimension feature space. In this feature  space a linear decision surface is constructed. Special properties of the decision surface  ensures high generalization ability of the learning machine. The idea behind the supportvector  network was previously implemented for the restricted case where the training  data can be separated without errors. We here extend this result to non-separable  training data.	V. Vapnik, Corinna Cortes	Support-Vector Networks
oo	384105	"We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural  network. By removing unimportant weights from a network, several  improvements can be expected: better generalization, fewer  training examples required, and improved speed of learning and/or  classification. The basic idea is to use second-derivative information  to make a tradeoff between network complexity and training  set error. Experiments confirm the usefulness of the methods on a  real-world application.  1 INTRODUCTION  Most successful applications of neural network learning to real-world problems have  been achieved using highly structured networks of rather large size [for example  (Waibel, 1989; LeCun et al., 1990)]. As applications become more complex, the  networks will presumably become even larger and more structured. Design tools  and techniques for comparing different architectures and minimizing the network  size will be needed. More impor..."	John S. Denker, Sara A. Solla, Le Cun	Optimal Brain Damage
o	177279	This paper reviews five statistical tests for determining whether one learning algorithm outperforms another on a particular learning task. These tests are compared experimentally to  determine their probability of incorrectly detecting a difference when no difference exists (type  1 error). Two widely-used statistical tests are shown to have high probability of Type I error in  certain situations and should never be used. These tests are (a) a test for the difference of two  proportions and (b) a paired-differences t test based on taking several random train/test splits.  A third test, a paired-differences t test based on 10-fold cross-validation, exhibits somewhat  elevated probability of Type I error. A fourth test, McNemar's test, is shown to have low Type  I error. The fifth test is a new test, 5x2cv, based on 5 iterations of 2-fold cross-validation.  Experiments show that this test also has good Type I error. The paper also measures the  power (ability to detect algorithm differences when...	Thomas G Dietterich	Statistical Tests for Comparing Supervised Classification Learning Algorithms
oo	123646	"In an earlier paper [9], we introduced a new ""boosting"" algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that  consistently generates classifiers whose performance is a little better than random guessing.  We also introduced the related notion of a ""pseudo-loss"" which is a method for forcing a learning  algorithm of multi-label concepts to concentrate on the labels that are hardest to discriminate.  In this paper, we describe experiments we carried out to assess how well AdaBoost with and  without pseudo-loss, performs on real learning problems.  We performed two sets of experiments. The first set compared boosting to Breiman's [1]  ""bagging"" method when used to aggregate various classifiers (including decision trees and  single attribute-value tests). We compared the performance of the two methods on a collection  of machine-learning benchmarks. In the second set of experiments, we studied in more detail  the performance of ..."	Robert Schapire, Yoav Freund	Experiments with a New Boosting Algorithm
o	553162	This paper explores the use of Support Vector Machines (SVMs) for learning text classifiers from examples. It analyzes the particular properties of learning with text data and identifies, why SVMs are appropriate for this task. Empirical results support the theoretical findings. SVMs achieve substantial improvements over the currently best performing methods and they behave robustly over a variety of different learning tasks. Furthermore, they are fully automatic, eliminating the need for manual parameter tuning.<		Text Categorization with Support Vector Machines: Learning with Many Relevant Features
x	23660	this paper is to provide such a comparison, and more importantly, to describe the general conclusions to which it has led. Relying on evidence that is divided between controlled experimental results and related formal analysis, we compare three well-known model selection algorithms. We attempt to identify their relative and absolute strengths and weaknesses, and we provide some general methods for analyzing the behavior and performance of model selection algorithms. Our hope is that these results may aid the informed practitioner in making an educated choice of model selection algorithm (perhaps based in part on some known properties of the model selection problem being confronted). The summary of the paper follows. In Section 2, we provide a formalization of the model selection problem. In this formalization, we isolate the problem of choosing the appropriate<	Andrew Y. Ng, Dana Ron, Michael Kearns, Yishay Mansour	An Experimental and Theoretical Comparison of Model Selection Methods
x	99782	. In this paper we initiate an investigation of generalizations of the Probably Approximately Correct (PAC) learning model that attempt to significantly weaken the target function assumptions. The ultimate goal in this direction is informally termed agnostic learning, in which we make virtually no assumptions on the target function. The name derives from the fact that as designers of learning algorithms, we give up the belief that Nature (as represented by the target function) has a simple or succinct explanation. We give a number of positive and negative results that provide an initial outline of the possibilities for agnostic learning. Our results include hardness results for the most obvious generalization of the PAC model to an agnostic setting, an efficient and general agnostic learning method based on dynamic programming, relationships between loss functions for agnostic learning, and an algorithm for a learning problem that involves hidden variables. Keywords: machine learning,...<	Michael Kearns, Robert Schapire	Toward Efficient Agnostic Learning
x	356583	In the feature subset selection problem, a learning algorithm is faced with the problem of selecting a relevant subset of features upon which to focus its attention, while ignoring the rest. To achieve the best possible performance with a particular learning algorithm on a particular domain, a feature subset selection method should consider how the algorithm and the training data interact. We explore the relation between optimal feature subset selection and relevance. Our wrapper method searches for an optimal feature subset tailored to a particular algorithm and a domain. We study the strengths and weaknesses of the wrapper approach and show improvements over the original design. We compare the wrapper approach to induction without feature subset selection and to Relief, a filter-based approach to feature subset selection. Significant improvement in accuracy on real problems is achieved for the two families of induction algorithms used: decision trees and Naive-Bayes. 1 Intr...<	H.John, Ron Kohavi	Wrappers for Feature Subset Selection
o	222642	A new research area, Inductive Logic Programming, is presently emerging. While inheriting various positive characteristics of the parent subjects  of Logic Programming and Machine Learning, it is hoped that the new area will  overcome many of the limitations of its forebears. The background to present  developments within this area is discussed and various goals and aspirations for  the increasing body of researchers are identified. Inductive Logic Programming  needs to be based on sound principles from both Logic and Statistics. On the  side of statistical justification of hypotheses we discuss the possible relationship between  Algorithmic Complexity theory and Probably-Approximately-Correct (PAC)  Learning. In terms of logic we provide a unifying framework for Muggleton and  Buntine's Inverse Resolution (IR) and Plotkin's Relative Least General Generalisation  (RLGG) by rederiving RLGG in terms of IR. This leads to a discussion of  the feasibility of extending the RLGG framework to allow for t...	Glasgow G Ad, S. H. Muggleton	Inductive Logic Programming
o	316526	This paper is an extended version of [12]. Generic author design sample pages 2000/07/31 03:05	Alex Smola, Jens Kohlmorgen, K. Muller, V. Vapnik	Using Support Vector Machines for Time Series Prediction
o	90428	"Suppose that, for a learning task, we have to select one hypothesis out of a set of hypotheses  (that may, for example, have been generated  by multiple applications of a randomized  learning algorithm). A common approach is  to evaluate each hypothesis in the set on some  previously unseen cross-validation data, and  then to select the hypothesis that had the  lowest cross-validation error. But when the  cross-validation data is partially corrupted  such as by noise, and if the set of hypotheses  we are selecting from is large, then ""folklore""  also warns about ""overfitting"" the crossvalidation  data [Klockars and Sax, 1986,  Tukey, 1949, Tukey, 1953]. In this paper, we  explain how this ""overfitting"" really occurs,  and show the surprising result that it can  be overcome by selecting a hypothesis with a  higher cross-validation error, over others with  lower cross-validation errors. We give reasons  for not selecting the hypothesis with the lowest  cross-validation error, and propose a new  algorithm, L..."		"Preventing ""Overfitting"" of Cross-Validation Data"
o	102410	: This paper continues the introduction to minimum encoding inductive inference given by Oliver and Hand. This series of papers was written with the objective of providing an introduction to this area for  statisticians. We describe the message length estimates used in Wallace's Minimum Message Length (MML)  inference and Rissanen's Minimum Description Length (MDL) inference. The differences in the message  length estimates of the two approaches are explained. The implications of these differences for applications  are discussed.  Contents  1 Introduction 3  2 Differences 3  2.1 Stochastic Complexity: One Part Messages : : : : : : : : : : : : : : : : : : : : : : : : : : : : 6  2.2 Interpretations of the Probability Distribution on Model Parameters : : : : : : : : : : : : : : 6  2.3 A Specific Objection to Priors : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 7  2.4 Prediction with MDL and MML : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 7  3 Two...		MDL and MML: Similarities and Differences
o	40099	. One of the surprising recurring phenomena observed in experiments with boosting is that the test error  of the generated hypothesis usually does not increase as its  size becomes very large, and often is observed to decrease  even after the training error reaches zero. In this paper, we  show that this phenomenon is related to the distribution of  margins of the training examples with respect to the generated  voting classification rule, where the margin of an  example is simply the difference between the number of  correct votes and the maximum number of votes received  by any incorrect label. We show that techniques used in the  analysis of Vapnik's support vector classifiers and of neural  networks with small weights can be applied to voting methods  to relate the margin distribution to the test error. We  also show theoretically and experimentally that boosting is  especially effective at increasing the margins of the training  examples. Finally, we compare our explanation to those  based on th...	Peter Bartlett, Robert Schapire, Wee Sun Lee, Yoav Freund	Boosting the Margin: A New Explanation for the Effectiveness of Voting Methods
o	30391	We introduce a new approach to model selection that performs better than the standard complexitypenalization  and hold-out error estimation techniques  in many cases. The basic idea is to exploit the intrinsic  metric structure of a hypothesis space, as determined  by the natural distribution of unlabeled training  patterns, and use this metric as a reference to detect  whether the empirical error estimates derived from a  small (labeled) training sample can be trusted in the  region around an empirically optimal hypothesis. Using  simple metric intuitions we develop new geometric  strategies for detecting overfitting and performing robust  yet responsive model selection in spaces of candidate  functions. These new metric-based strategies  dramatically outperform previous approaches in experimental  studies of classical polynomial curve fitting.  Moreover, the technique is simple, efficient, and  can be applied to most function learning tasks. The  only requirement is access to an auxiliary collection ...	Dale Schuurmans	A New Metric-Based Approach to Model Selection
x	16898	We investigate the structure of model selection problems via the bias/variance decomposition. In  particular, we characterize the essential aspects  of a model selection task by the bias and variance  profiles it generates over the sequence of hypothesis  classes. With this view, we develop a new  understanding of complexity-penalization methods:  First, the penalty terms can be interpreted as  postulating a particular profile for the variances  as a function of model complexity---if the postulated  and true profiles do not match, then systematic  under-fitting or over-fitting results, depending  on whether the penalty terms are too large or  too small. Second, we observe that it is generally  best to penalize according to the true variances  of the task, and therefore no fixed penalization  strategy is optimal across all problems. We then  use this characterization to introduce the notion  of easy versus hard model selection problems.  Here we show that if the variance profile grows  too rapidly in rela...	Dale Schuurmans, Dean Foster, Lyle H. Ungar	Characterizing the Generalization Performance of Model Selection Strategies
======				
