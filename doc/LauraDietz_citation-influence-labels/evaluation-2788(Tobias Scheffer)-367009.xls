Evaluation sheet		v0.1		
		Abstract	Authors	Title
citingPub	367009	In order to select a good hypothesis language (or model) from a collection of possible models, one has to assess the generalization performance of the hypothesis which is returned  by a learner that is bound to use some particular model. This paper deals with  a new and very efficient way of assessing this generalization performance. We present  a new analysis which characterizes the expected generalization error of the hypothesis  with least training error in terms of the distribution of error rates of the hypotheses in  the model. This distribution can be estimated very efficiently from the data which immediately  leads to an efficient model selection algorithm. The analysis predicts learning  curves with a very high precision and thus contributes to a better understanding of why  and when over-fitting occurs. We present empirical studies (controlled experiments on  Boolean decision trees and a large-scale text categorization problem) which show that the  model selection algorithm l...	Thorsten Joachims, Tobias Scheffer	Expected Error Analysis for Model Selection
evalAuthor	2788		Tobias Scheffer	

Evaluation Legend				
xx	strongly influenced			
x	influenced			
o	not really influenced			
oo	unrelated			
?	can't judge			

Evaluation				
======	id	Abstract	Authors	Title
oo	500489	The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input  vectors are non-linearly mapped to a very high-dimension feature space. In this feature  space a linear decision surface is constructed. Special properties of the decision surface  ensures high generalization ability of the learning machine. The idea behind the supportvector  network was previously implemented for the restricted case where the training  data can be separated without errors. We here extend this result to non-separable  training data.	V. Vapnik, Corinna Cortes	Support-Vector Networks
oo	384105	"We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural  network. By removing unimportant weights from a network, several  improvements can be expected: better generalization, fewer  training examples required, and improved speed of learning and/or  classification. The basic idea is to use second-derivative information  to make a tradeoff between network complexity and training  set error. Experiments confirm the usefulness of the methods on a  real-world application.  1 INTRODUCTION  Most successful applications of neural network learning to real-world problems have  been achieved using highly structured networks of rather large size [for example  (Waibel, 1989; LeCun et al., 1990)]. As applications become more complex, the  networks will presumably become even larger and more structured. Design tools  and techniques for comparing different architectures and minimizing the network  size will be needed. More impor..."	John S. Denker, Sara A. Solla, Le Cun	Optimal Brain Damage
oo	123646	"In an earlier paper [9], we introduced a new ""boosting"" algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that  consistently generates classifiers whose performance is a little better than random guessing.  We also introduced the related notion of a ""pseudo-loss"" which is a method for forcing a learning  algorithm of multi-label concepts to concentrate on the labels that are hardest to discriminate.  In this paper, we describe experiments we carried out to assess how well AdaBoost with and  without pseudo-loss, performs on real learning problems.  We performed two sets of experiments. The first set compared boosting to Breiman's [1]  ""bagging"" method when used to aggregate various classifiers (including decision trees and  single attribute-value tests). We compared the performance of the two methods on a collection  of machine-learning benchmarks. In the second set of experiments, we studied in more detail  the performance of ..."	Robert Schapire, Yoav Freund	Experiments with a New Boosting Algorithm
o 	553162	This paper explores the use of Support Vector Machines (SVMs) for learning text classifiers from examples. It analyzes the particular properties of learning with text data and identifies, why SVMs are appropriate for this task. Empirical results support the theoretical findings. SVMs achieve substantial improvements over the currently best performing methods and they behave robustly over a variety of different learning tasks. Furthermore, they are fully automatic, eliminating the need for manual parameter tuning.<		Text Categorization with Support Vector Machines: Learning with Many Relevant Features
x	23660	this paper is to provide such a comparison, and more importantly, to describe the general conclusions to which it has led. Relying on evidence that is divided between controlled experimental results and related formal analysis, we compare three well-known model selection algorithms. We attempt to identify their relative and absolute strengths and weaknesses, and we provide some general methods for analyzing the behavior and performance of model selection algorithms. Our hope is that these results may aid the informed practitioner in making an educated choice of model selection algorithm (perhaps based in part on some known properties of the model selection problem being confronted). The summary of the paper follows. In Section 2, we provide a formalization of the model selection problem. In this formalization, we isolate the problem of choosing the appropriate<	Andrew Y. Ng, Dana Ron, Michael Kearns, Yishay Mansour	An Experimental and Theoretical Comparison of Model Selection Methods
x	99782	. In this paper we initiate an investigation of generalizations of the Probably Approximately Correct (PAC) learning model that attempt to significantly weaken the target function assumptions. The ultimate goal in this direction is informally termed agnostic learning, in which we make virtually no assumptions on the target function. The name derives from the fact that as designers of learning algorithms, we give up the belief that Nature (as represented by the target function) has a simple or succinct explanation. We give a number of positive and negative results that provide an initial outline of the possibilities for agnostic learning. Our results include hardness results for the most obvious generalization of the PAC model to an agnostic setting, an efficient and general agnostic learning method based on dynamic programming, relationships between loss functions for agnostic learning, and an algorithm for a learning problem that involves hidden variables. Keywords: machine learning,...<	Michael Kearns, Robert Schapire	Toward Efficient Agnostic Learning
x	263013	: We give an analysis of the generalization error of cross validation in terms of two natural measures of the difficulty of the problem under consideration: the approximation rate (the accuracy to which the target function  can be ideally approximated as a function of the number of hypothesis parameters), and the estimation rate (the  deviation between the training and generalization errors as a function of the number of hypothesis parameters). The  approximation rate captures the complexity of the target function with respect to the hypothesis model, and the  estimation rate captures the extent to which the hypothesis model suffers from overfitting. Using these two measures,  we give a rigorous and general bound on the error of cross validation. The bound clearly shows the tradeoffs involved  with making fl --- the fraction of data saved for testing --- too large or too small. By optimizing the bound with  respect to fl, we then argue (through a combination of formal analysis, plotting, and ...	Michael Kearns	A Bound on the Error of Cross Validation Using the Approximation and Estimation Rates, with Consequences for the Training-Test Split
x	356583	In the feature subset selection problem, a learning algorithm is faced with the problem of selecting a relevant subset of features upon which to focus its attention, while ignoring the rest. To achieve the best possible performance with a particular learning algorithm on a particular domain, a feature subset selection method should consider how the algorithm and the training data interact. We explore the relation between optimal feature subset selection and relevance. Our wrapper method searches for an optimal feature subset tailored to a particular algorithm and a domain. We study the strengths and weaknesses of the wrapper approach and show improvements over the original design. We compare the wrapper approach to induction without feature subset selection and to Relief, a filter-based approach to feature subset selection. Significant improvement in accuracy on real problems is achieved for the two families of induction algorithms used: decision trees and Naive-Bayes. 1 Intr...<	H.John, Ron Kohavi	Wrappers for Feature Subset Selection
x	14473	"We present a bias-variance decomposition of expected misclassification rate, the most commonly used loss function in supervised classification learning. The bias-variance decomposition for quadratic loss functions is well known and serves as an important tool for analyzing learning algorithms, yet no decomposition was offered for the more commonly used zero-one (misclassification) loss functions until the recent work of Kong &amp; Dietterich (1995) and Breiman (1996). Their decomposition suffers from some major shortcomings though (e.g., potentially negative variance), which our decomposition avoids. We show that, in practice, the naive frequency-based estimation of the decomposition terms is by itself biased and show how to correct for this bias. We illustrate the decomposition on various algorithms and datasets from the UCI repository. 1 Introduction The bias plus variance decomposition (Geman, Bienenstock &amp; Doursat 1992) is a powerful tool from sampling theory statistics for analyzing ...<"	David H. Wolpert, Ron Kohavi	Bias Plus Variance Decomposition for Zero-One Loss Functions
o	24575	Previous research has shown that a technique called error-correcting output coding  (ECOC) can dramatically improve the  classification accuracy of supervised learning  algorithms that learn to classify data  points into one of k AE 2 classes. This  paper presents an investigation of why the  ECOC technique works, particularly when  employed with decision-tree learning algorithms.  It shows that the ECOC method---  like any form of voting or committee---can  reduce the variance of the learning algorithm.  Furthermore---unlike methods that  simply combine multiple runs of the same  learning algorithm---ECOC can correct for  errors caused by the bias of the learning algorithm.  Experiments show that this bias  correction ability relies on the non-local behavior  of C4.5.  1 Introduction  Error-correcting output coding (ECOC) is a method  for applying binary (two-class) learning algorithms  to solve k-class supervised learning problems. It  works by converting the k-class supervised learning  problem into a la...	Eun Bae Kong, Thomas G Dietterich	Error-Correcting Output Coding Corrects Bias and Variance
o	90428	"Suppose that, for a learning task, we have to select one hypothesis out of a set of hypotheses  (that may, for example, have been generated  by multiple applications of a randomized  learning algorithm). A common approach is  to evaluate each hypothesis in the set on some  previously unseen cross-validation data, and  then to select the hypothesis that had the  lowest cross-validation error. But when the  cross-validation data is partially corrupted  such as by noise, and if the set of hypotheses  we are selecting from is large, then ""folklore""  also warns about ""overfitting"" the crossvalidation  data [Klockars and Sax, 1986,  Tukey, 1949, Tukey, 1953]. In this paper, we  explain how this ""overfitting"" really occurs,  and show the surprising result that it can  be overcome by selecting a hypothesis with a  higher cross-validation error, over others with  lower cross-validation errors. We give reasons  for not selecting the hypothesis with the lowest  cross-validation error, and propose a new  algorithm, L..."		"Preventing ""Overfitting"" of Cross-Validation Data"
o	441895	This paper introduces a new representation for Boolean functions, called decision lists,<	Ronald L. Rivest	Learning Decision Lists
o	187991	Strategies for increasing predictive accuracy through selective pruning have been widely adopted by researchers in decision tree induction. It  is easy to get the impression from research reports that there are statistical  reasons for believing that these overfitting avoidance strategies  do increase accuracy and that, as a research community, we are making  progress toward developing powerful, general methods for guarding  against overfitting in inducing decision trees. In fact, any overfitting  avoidance strategy amounts to a form of bias and, as such, may degrade  performance instead of improving it. If pruning methods have  often proven successful in empirical tests, this is due, not to the methods,  but to the choice of test problems. As examples in this article  illustrate, overfitting avoidance strategies are not better or worse, but  only more or less appropriate to specific application domains. We are  not---and cannot be---making progress toward methods both powerful  and general.  The ...	Cullen Schaffer	Overfitting Avoidance as Bias
o	40099	. One of the surprising recurring phenomena observed in experiments with boosting is that the test error  of the generated hypothesis usually does not increase as its  size becomes very large, and often is observed to decrease  even after the training error reaches zero. In this paper, we  show that this phenomenon is related to the distribution of  margins of the training examples with respect to the generated  voting classification rule, where the margin of an  example is simply the difference between the number of  correct votes and the maximum number of votes received  by any incorrect label. We show that techniques used in the  analysis of Vapnik's support vector classifiers and of neural  networks with small weights can be applied to voting methods  to relate the margin distribution to the test error. We  also show theoretically and experimentally that boosting is  especially effective at increasing the margins of the training  examples. Finally, we compare our explanation to those  based on th...	Peter Bartlett, Robert Schapire, Wee Sun Lee, Yoav Freund	Boosting the Margin: A New Explanation for the Effectiveness of Voting Methods
x	35995	In order to rank the performance of machine learning algorithms, many researchers conduct  experiments on benchmark data sets. Since  most learning algorithms have domain-specific  parameters, it is a popular custom to adapt  these parameters to obtain a minimal error rate  on the test set. The same rate is then used  to rank the algorithm, which causes an optimistic  bias. We quantify this bias, showing,  in particular, that an algorithm with more parameters  will probably be ranked higher than  an equally good algorithm with fewer parameters.  We demonstrate this result, showing the  number of parameters and trials required in order  to pretend to outperform C4.5 or FOIL,  respectively, for various benchmark problems.  We then describe out how unbiased ranking experiments  should be conducted.  1 Introduction  Estimating the accuracy of a classifier is a topic that has  experienced much attention in the ML community. One  of the main results is that N -fold cross validation provides  a bias-free [ Sto74...	Ralf Herbrich, Tobias Scheffer	Unbiased Assessment of Learning Algorithms
xx	119453	". Model selection is considered the problem of choosing a ""good"" hypothesis language from a given ensemble of models. Here, a ""good"" model is one for which the true  (or generalization) error of the hypothesis returned by a learner which takes the model as  hypothesis language is low. The crucial part of model selection is to somehow assess the true  error of the apparently best hypothesis (the empirical minimizer) of a model. In this paper,  we discuss a new, very efficient approach to model selection. Our approach is inherently  Bayesian, but instead of using priors on target functions or hypotheses, we talk about priors  on error values -- which leads us to a new insightful characterization of the expected true error.  Consequently, our solution is based on the prior of error values for the given problem which is,  of course, unknown. But we show next that this prior can be estimated efficiently for a given  learning problem by recording the empirical errors of a constant number of randomly ..."	Thorsten Joachims, Tobias Scheffer	Estimating the Expected Error of Empirical Minimizers for Model Selection
o	30391	We introduce a new approach to model selection that performs better than the standard complexitypenalization  and hold-out error estimation techniques  in many cases. The basic idea is to exploit the intrinsic  metric structure of a hypothesis space, as determined  by the natural distribution of unlabeled training  patterns, and use this metric as a reference to detect  whether the empirical error estimates derived from a  small (labeled) training sample can be trusted in the  region around an empirically optimal hypothesis. Using  simple metric intuitions we develop new geometric  strategies for detecting overfitting and performing robust  yet responsive model selection in spaces of candidate  functions. These new metric-based strategies  dramatically outperform previous approaches in experimental  studies of classical polynomial curve fitting.  Moreover, the technique is simple, efficient, and  can be applied to most function learning tasks. The  only requirement is access to an auxiliary collection ...	Dale Schuurmans	A New Metric-Based Approach to Model Selection
x	16898	We investigate the structure of model selection problems via the bias/variance decomposition. In  particular, we characterize the essential aspects  of a model selection task by the bias and variance  profiles it generates over the sequence of hypothesis  classes. With this view, we develop a new  understanding of complexity-penalization methods:  First, the penalty terms can be interpreted as  postulating a particular profile for the variances  as a function of model complexity---if the postulated  and true profiles do not match, then systematic  under-fitting or over-fitting results, depending  on whether the penalty terms are too large or  too small. Second, we observe that it is generally  best to penalize according to the true variances  of the task, and therefore no fixed penalization  strategy is optimal across all problems. We then  use this characterization to introduce the notion  of easy versus hard model selection problems.  Here we show that if the variance profile grows  too rapidly in rela...	Dale Schuurmans, Dean Foster, Lyle H. Ungar	Characterizing the Generalization Performance of Model Selection Strategies
o	88085	"This paper is a comparative study of feature selection methods in statistical learning of  text categorization. The focus is on aggressive  dimensionality reduction. Five methods  were evaluated, including term selection  based on document frequency (DF), information  gain (IG), mutual information (MI), a  &#216;  2  -test (CHI), and term strength (TS). We  found IG and CHI most effective in our experiments.  Using IG thresholding with a knearest  neighbor classifier on the Reuters corpus,  removal of up to 98% removal of unique  terms actually yielded an improved classification  accuracy (measured by average precision)  . DF thresholding performed similarly.  Indeed we found strong correlations between  the DF, IG and CHI values of a term. This  suggests that DF thresholding, the simplest  method with the lowest cost in computation,  can be reliably used instead of IG or CHI  when the computation of these measures are  too expensive. TS compares favorably with  the other methods with up to 50% vocabulary  redu..."	Jan Pedersen, Yibing Yang	A Comparative Study on Feature Selection in Text Categorization
======				
