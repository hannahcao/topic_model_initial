177279

#abstract
This paper reviews five statistical tests for determining whether one learning algorithm outperforms another on a particular learning task. These tests are compared experimentally to  determine their probability of incorrectly detecting a difference when no difference exists (type  1 error). Two widely-used statistical tests are shown to have high probability of Type I error in  certain situations and should never be used. These tests are (a) a test for the difference of two  proportions and (b) a paired-differences t test based on taking several random train/test splits.  A third test, a paired-differences t test based on 10-fold cross-validation, exhibits somewhat  elevated probability of Type I error. A fourth test, McNemar's test, is shown to have low Type  I error. The fifth test is a new test, 5x2cv, based on 5 iterations of 2-fold cross-validation.  Experiments show that this test also has good Type I error. The paper also measures the  power (ability to detect algorithm differences when...
