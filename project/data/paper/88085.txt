88085

#abstract
This paper is a comparative study of feature selection methods in statistical learning of  text categorization. The focus is on aggressive  dimensionality reduction. Five methods  were evaluated, including term selection  based on document frequency (DF), information  gain (IG), mutual information (MI), a  &#216;  2  -test (CHI), and term strength (TS). We  found IG and CHI most effective in our experiments.  Using IG thresholding with a knearest  neighbor classifier on the Reuters corpus,  removal of up to 98% removal of unique  terms actually yielded an improved classification  accuracy (measured by average precision)  . DF thresholding performed similarly.  Indeed we found strong correlations between  the DF, IG and CHI values of a term. This  suggests that DF thresholding, the simplest  method with the lowest cost in computation,  can be reliably used instead of IG or CHI  when the computation of these measures are  too expensive. TS compares favorably with  the other methods with up to 50% vocabulary  redu...
