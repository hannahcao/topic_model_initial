{"user_name":" Fast Algorithms for Projected Clustering ","user_timeline":[{"aspect":"abstract","tweet":" The clustering problem is well known in the database literature for its numerous applications in problems such as customer segmentation, classification and trend analysis. Unfortunately, all known algorithms tend to break down in high dimensional spaces because of the inherent sparsity of the points. In such high dimensional spaces not all dimensions may be relevant to a given cluster. One way of handling this is to pick the closely correlated dimensions and find clusters in the corresponding subspace. Traditional feature selection algorithms attempt to achieve this. The weakness of this approach is that in typical high dimensional data mining applications different sets of points may cluster better for different subsets of dimensions. The number of dimensions in each such cluster-specific subspace may also vary. Hence, it may be impossible to find a single small subset of dimensions for all the clusters. We therefore discuss a generalization of the clustering problem, referred to as the projected clustering problem, in which the subsets of dimensions selected are specific to the clusters themselves. We develop an algorithmic framework for solving the projected clustering problem, and test its performance on synthetic data. "},{"aspect":"expanalysis","tweet":" 4.2 Accuracy Results To test how accurately the algorithm performs we compute the Confusion Mat& defined as follows: entry (i, j) is equal to the number of data points assigned to output cluster i, that were generated as part of input cluster j. The last row and column of the matrix represent output outliers, respectively input outliers, and their entries are similarly defined. Obviously, we want each row to have one entry that is much larger than the others, which indicates a clear correspondence between the input and output clusters. In the tables below, the input clusters are denoted by letters, while the output clusters are denoted by numbers. Another significant result is the set of dimensions computed for each output cluster, as compared to the set of dimensions of the corresponding input cluster. We divided the experiments in two classes. First, we used input files :for which all clusters had been generated in the same number of dimensions, but in different subspaces (Case 1). Then, we used input files containing clusters generated in different number of dimensions (Case 2). We report below the results for one experiment in each class. We obtained similar quality in all the other experiments we performed. Both files had iV = 100,000 data points in a 20dimensional space, with k: = 5. The first input file had 1 = 7 (\u201aÄò. . all input clust,ers were generated in some 7- diizensional subspace:), while the second file had 1 = 4, and the clusters were generated as follows: two clusters were generated in different 2-dimensional subspaces, one in a J-dimensional subspace, one in a 6-dimensional subspace, and one in a 7-dimensional subspace. In both cases PROCLUS discovers output clusters 68 in which the majority of points comes from o:ne input cluster, as shown in Tables 3 and 4. In other words, it recognizes the natural clustering of the points. VVe note that for both files the output clusters pick some of the original outliers and report them as cluste:: points. This is not necessarily an error, since the outliers were randomly placed throughout the entire space, and it is probable that some of them have actually been placed inside clusters. The output clusters j.n Table 4 also have some small number of points that should have been assigned to other clusters. For example, the 267 points in row 1 and column C should have been assigned to cluster 4, because they were glmerated as part of input cluster C, and output cluster 4 has a clear correspondence to cluster C. However, the percentage of misplaced points is very small so that it does not influence the correspondence between input and output clusters, nor would it significantly Jter the result of any data mining application based on thLis clustering. Moreover, there is a perfect correspondence between the sets of dimensions of the output clusters and their corresponding input clusters, as illustrated by Tables 1 and 2. This is important for applicatio:ns that require not only a good partitioning of the data, but also additional information as to what dirnensio:ns (or attributes) are relevant for each partition. As we mentioned before, CLIQUE does not guarant\u201aÄôee that the result it returns represents a partitioning of the points. To quantify how different its output is from an actual partitioning, we compute the average ol.erlap as follows: Table 3: PROCLUS: Confusion Matrix (same number of dimensions) for Case 1 Table 4: PROCLUS: Confusion Matrix (different number of dimensions) for Case 2 overlap = f$l \u201aÄò,\u201aÄúI, rl + where 4 is the number of output clusters. Thus, an overlap of 1 means that on the average each point that is not an outlier is assigned to only one cluster, and so the result can be considered a partitioning. On the other hand, a large overlap means that many of the points are assigned to more than one output cluster, so the result cannot be considered a reasonable approximation for a partitioning. In the experiments below we try to determine the cases in which CLIQUE is likely to generate an output with small overlap. For those cases we compare the results of CLIQUE and PROCLUS in terms of quality and running time, to decide which method is preferable. One problem we have encountered during these experiments is that on the average half of the cluster points are considered outliers by CLIQUE. This is a consequence of the density-based approach of the algorithm, since lower- density areas in a cluster can cause some of its points to be thrown away. Another reason is the fact that clusters are considered to be axis-parallel regions. Such a region generally offers a low coverage of the corresponding input cluster, especially as the dimensionality of the cluster increases. Hence, a significant percentage of relevant data points are erroneously considered outliers by CLIQUE. Of course, this percentage can be lowered 69 by tuning the input parameters  and T appropriately. This leads to a tradeoff between quality of output and running time. Moreover, the density threshold of a unit must take into account both the number of intervals on a dimension and the dimensionality of the space. Hence, variation of one input parameter must be correlated with the variation of the other parameter. No obvious method is indicated in [l] for how to choose the two parameters. For files in which clusters exist in different number of dimensions CLIQUE reported a large number of output clusters, most of which were projections of a higher dimensional cluster. As a result, the average overlap was also large. It is unclear how one can differentiate between, for example, a 2-dimensional output cluster corresponding to a 2-dimensional input cluster, and the 2-dimensional projection of a 6-dimensional cluster. In this case, CLIQUE cannot be used to obtain a good approximation for a partitioning. Below, we discuss the results we obtained with CLIQUE for input files in which all clusters exist in the same number of dimensions. As in [l], we set [ (the number of intervals on each dimension) to 10, and we try various values for the density threshold r. We present the results obtained on an input file with 1 = 7, the same for which we reported the PROCLUS results above. However, the issues we discuss were noted on other input files and for different values of 2, as well. For m B C D E Out. n 2 II 11128 0 0 0 0 0 Table 5: CLIQUE: Matching between Input and Output Clusters (small snapshot) r = 0.5 and r = 0.8, the average overlap was 1, but the percentage of cluster points discovered by CLIQUE was low (42.7%, respectively 30.7%). We then experimented with lower values for 7, more exactly 7 = 0.2 and T = 0.1, expecting the percentage of cluster points to increase. However, because of the low density, CLIQUE reported output clusters in 8 dimensions (one dimension more than they were generated), and the percentage of cluster points decreased to 21.2% for T = 0.1. Two of the original input clusters were entirely missed, and all their points declared outliers. Of the remaining three input clusters, at least 50% of the points in each one were thrown away as outliers, and two of these input clusters were split into four output clusters. We finally ran CLIQUE with T = 0.1 and set it to find clusters only in 7 dimensions, using an option provided by the program. It reported 48 output clusters, with a percentage of cluster points equal to 74.6%. The average overlap was 3.63, which means that on the average, an input point had been assigned to at least 3 output clusters. We present the results of this last experiment in Table 5. Due to lack of space, we do not provide the entire Confusion Matrix, but only a small snapshot that reflects both \u201aÄúgood\u201aÄù and \u201aÄúbad\u201aÄù output clusters discovered by CLIQUE. We conclude that, while there are cases in which CLIQUE returns a partitioning of the points, PROCLUS is still useful because of its better accuracy. 4.3 Scalability IResults In what follows we will say that two input files are similar if the following parameters are identical for both files: number of points N, dimensionality of the space d, number of clusters Ic, and average dimensionality of a cluster 1. As noticed in the previous subsection, the output of CLIQUE could only be interpreted as an (approximate) partitioning of the points when all clusters exist in the same number of dimensions. Hence, we compare the running times of CLIQUE and PRCCLUS only on such files. However, we also tested PROCLUS on 70 Figure 7: Scalability with number of points similar files in which clusters exist in different number of dimensions, and found no significant difference between the respective running times. Because of the random nature of PROCLUS, each running time reported in this section is averaged over three similar input files. We want to mention that in each run the quality of the results returned by PROCLUS was similar to that presented in the previous subsection. Number of points: All data files on which we tested contained 5 clusters, each existing in some 5-dimensional subspace. The data space \u201aÄòwas 20- dimensional. We ran CLIQUE with  = 10 and T = 0.5. Figure 7 shows that PROCLUS scales linearly with the number of input points, while outperforming CLIQUE by a factor of approximately 10. The graph has logarithmic scale along the y coordinate. Average dimensionality of the clusters: All files on which we tested had N = 100,000 points and contained 5 clusters. The data space was 20- dimensional. We ran CLIQUE with  = 10 and T = 0.5 for files in which the dimensionality of clusters was 4,5 or 6, and with 7 = 0.1 for dimensionality of clusters equal to 7 and 8. We selected a lower r for the highler dimensional clusters because, as the volume of the clusters increases, the cluster density decreases. This corresponds to the approach used for the experiments in [l]. Figure 8 shows that the two algorithms have a different type of dependency on the average cluster dimensionality 1. The results we obtained for CLIQUE are consistent with those reported in [l], where an exponential dependency on 1 is proven. On the other hand, the running time of PROCLUS is only slightly influenced by 1. This happens because the main contribution of 1 to the running time is during the computation of segmental distances, which takes O(N . Figure 8: Scalability with average dimensionality Figure 9: Scalability with dimensionality of the space k . I) for each iteration. Since we are also computing distances in the full dimensional space in time O(N . km d), the running time of an iteration is dominated by this second term and only slightly influenced by a change in 1. This very good behavior of PROCLUS with respect to I is important for the situations in which it is not clear what value should be chosen for parameter 1. Because the running time is so small (about 150 seconds for each point shown in the graph), it is easy to simply run the algorithm a few times and try different values for 1. Dimensionality of the space: We tested on files with N = 100,000 points that contained 5 clusters, each existing in a 5-dimensional space. The sensitivity with respect to the dimensionality of the space is illustrated in Figure 9. As expected, PROCLUS scales linearly with the dimensionality of the entire space. 71 "},{"aspect":"expdata","tweet":" 4.1 Synthetic Data Generation In order to generate the data we used a method similar to that discussed by Zhang et. al. [26]. However, we added generalizations to the data generation process in order to take into account the possibility of different clusters occurring in different subspaces. The points 67 have coordinates in the range [0, 1001 and are either cluster points or outliers. The maximum percentage of outliers is a simulation parameter and was chosen to be F ,,,,trier = 5%. Outliers were distributed uniformly at random throughout the entire space. In order to generate cluster points the program takes as input parameters the number of clusters k and a Poisson parameter k that determines the number of dimensions in each cluster, as we explain below. The algorithm proceeds by defining so-called anchor points around which the clusters will be distributed, as well as the dimensions associated with each such anchor point. Then, it determines how many points will be associated with each cluster and finally it generates the cluster points. We explain these steps in more detail below. The anchor points of clusters are obtained by generat- ing k uniformly distributed points in the d-dimensional space. We shall denote the anchor point for the ith cluster by ci. The number of dimensions associated with a cluster is given by the realization of a Poisson random variable with mean CL, with the additional restriction that this number must be at least 2 and at most d. Once the number of dimensions di for the cluster i is generated, the dimensions for each cluster are chosen using the following technique: The dimensions in the first cluster are chosen randomly. The dimensions for the ith cluster are then generated inductively by choosing min{di-1, di/2} d\u201aÄô lmensions from the (i - l)st cluster and generating the other dimensions randomly. This iterative technique is intended to model the fact that different clusters frequently dimensions. share subsets of correlated To decide the number of points in each cluster, we generate k exponential random variables with mean 1 and then assign to each cluster a number of points proportional to these realizations. More exactly, let 9, ~2,. . .rk be the realizations of the k random variables, and let N, = N. (1 - Foutlic,.) be the number of cluster points. Then, the number of points in cluster i is given by N, . =FL, ri. Finally, the points for a given cluster i are generated as follows: The coordinates of the points on the non- cluster dimensions are generated uniformly at random. For a cluster dimension j, the coordinates of the points projected onto dimension j follow a normal distribution with mean at the respective coordinate of the anchor point, and variance determined randomly in the following manner: Fix a spread parameter r and choose a scale factor sij E [l, s] uniformly at random, where s is user defined. Then the variance of the normal distribution on dimension j is (sij . T)\u201aÄú. For our data generation we chose r = s = 2. Dimensions Found Dimensions 1 Points n 1 11 4. 6. 11. 13. 14. 17. 19 1 18701 11 Y R A n Input I( Dimensions I Poinla B 11 2, 3, 4, 2, 9, 3, 11, 7 14, 18 1 Outliers 11 Dimensions Table 1: PROCLUS: Dimensions of the Input Clusters Table 2: PROCLUS: Dimensions of the Input Clusters (Top) and Output Clusters (Bottom) for Case 1 (Top) and Output Clusters (Bottom) for Case 2 "},{"aspect":"background","tweet":" 1 Introduction The clustering problem has been discussed extensively in the database literature as a tool for similarity search, customer segmentation, pattern recognition, trend analysis and classification. Various methods have been studied in considerable detail by both the statistics and database communities [3, 4, 7, 8, 9, 13, 21, 261. Detailed surveys on clustering methods can be found in [6, 17, 18, 20, 251. The problem of clustering data points is defined as follows: Given a set of points in multidimensional Permission to make digital or hard copies of all or part of this work fit personal or classroom use is granted without fee provided that copies are not made or distributed liar profit or commercial advantage anti that copies hear this notice and the full citation on the lirst page. TO COPY othcrwisc, to republish, to post on scrvcrs or to redistribute to lists, requires prior specific permission andior a fee. SIGMOD \u201aÄò99 Philadelphia PA Copyright ACM 1999 l-581 13-084-8/99/05...$5.00 61 Jong Soo Park Sungshin Women\u201aÄôs University Seoul, Korea jpark@cs.sungshin.ac.kr space, find a partition of the points into clusters so that the points within each cluster are close to one another. (There may also be a group of outlier points.) Some algorithms assume that the number of clusters is prespecified as a user parameter. Various objective functions may be used in order to make a quantitative determination as to how well the points are clustered. Alternatively, distribution based methods [15, 241 may be used in order to find clusters of arbitrary shape. Most clustering algorithms do not work efficiently in higher dimensional spaces because of the inherent sparsity of the data [l, 221. In high dimensional applications, it is likely that for any given pair of points there exist at least a few dimensions on which the points are far apart from one another. So a clustering algorithm is often preceded by feature selection (see for example [19]). The goal is to find the particular dimensions on which the points in the data are correlated. Pruning away the remaining dimensions reduces the noise in the data. The problem of using traditional feature selection algorithms is that picking certain dimensions in advance can lead to a loss of information. Furthermore, in many real data examples, some points are correlated with respect to a given set of dimensions and others are correlated with respect to different dimensions. Thus it may not always be feasible to prune off too many dimensions without at the same time incurring a substantial loss of information. We demonstrate this with the help of an example. In Figure 1 we have illustrated two different projected cross sections for a set of points in 3-dimensional space. There are two patterns in the data. The first pattern corresponds to a set of points that are close to one another in the z-y plane, while the second pattern corresponds to a set of points that are close to one another in the c-z plane. We would like to have some way of discovering such patterns. Note that traditional feature selection does not work in this case, as each dimension is relevant to at least one of the clusters. At the same time, clustering in the full dimensional space will not discover the two patterns, since each of them is spread out along one of the dimensions. I xX x x X X Y axis XX X XX x x x x xx $2 Z axis x axllS (a) Cross Section on X-Y axis t I XX xx X X F xx x x X XX I e X axis @) Cross Section on X-Z axis Figure 1: Difficulties Associated with Feature Preselec- tion In this context .we now define what we call a projected cluster. Consider a set of data points in some multidimensional space. A projected cluster is a subset C of data points together with a subset p of dimensions such that the points in C are closely clustered in the subspace of dimensi,ons V. In Figure 1, two clusters exist in two different projected subpaces. Cluster 1 exists in projected s-y space, while cluster 2 exists in projected z-z space. In this paper we focus on a method to find clusters in small projected subspaces for data of high dimension- ality. We call our algorithm PROCLUS to denote the fact that it is a PRO$ected CLUStering algorithm. We assume that the number k: of clusters to be found is an input para.meter. The output of the algorithm will be twofold: a (k: + 1)-way partition {Cl, ,.., Ck, 0) of the data, so that the points in each partition element except the last form a cluste:r. (The points in the last partition element are the outliers, which by definition do not cluster well.) a possibly different subset Vi of dimensions for each cluster Ci, 1 _ i 5 Ic, so that the points in Ci are correlated with respect to these dimensions. (The dimensions for the outlier set 0 can be assumed 62 to be the empty set.) For different clusters, the cardinality of the corresponding set \u201aÄòDi can be different. In addition to the number of clusters Jz the algorithm takes as input the average number of dimensions 1 in a cluster. The two parameters can be varied independently of one another. (The only restriction is that the total number of dimensions Ic . 1 must be integral.) 1.1 Contributions of this paper The contributions of this paper are as follows: (1) We discuss the concept of projected clustering for finding clusters in multidimensional spaces. Thus, we compute clusters based not only on poi.nts but also on dimensions. For data ir. a la.rge number of dimensions this can result in a f,ignificant improvement in the quality of the clustering. (2) We propose an algorithm for the projected cluster- ing problem which uses the so-called metEoid tech- nique described in [21] to find the appropriate sets of clusters and dimensions. The algorithm uses a lo- cality analysis in order to find the set of di.mensions associated with each medoid. The fact that different points may cluster bet,ter for different subsets of dimensions has been observed for the first time by Agrawal et. al. in [l]. This paper presents an effective method for finding regions of greater density in high dimensional data in a way which has good scalability and usability. The work in [l] illustrates the merits of looking at different !bubspaces for different clusters as opposed to a full dimensional clustering. The algorithm, called CLIQUE, works from lower to higher dimensionality subspaces and discovers \u201aÄúdense\u201aÄù regions in each subspace. More precisely, each dimension is divided into a number of intervals t. For a given set of dimensions, a cross- product of such intervals (one on each dimension in the set) is called a unit in the respective subspace. Units are dense if the number of points the:{ contain is above a certain threshold 7. Both  and 7 are user parameters. The algorithm discovers all dense units in each b-dimensional subspace by building from the dense units in (h - 1)-dimensional subspaces, and then \u201aÄúconnects\u201aÄù these axis-parallel units to form the reported rectangular regions. Although such an approach can discover interesting characteristics of the data, it does not produce a clustering in the accepted definition of the word, since the points are not partitioned int,, disjoint groups. Rather, there is a large overlap among the reported dense regions, due to the fact that fc\u201aÄôr a given dense region all its projections on lower dimensionahty subspaces are also dense and get reported. While both CLIQUE and PROCLUS aim to discover interesting correlations among data in various subspaces of the original high dimensional space, their output is significantly different. CLIQUE is successful in exploring dense regions in all subspaces of some desired dimensionality. For many applications in customer segmentation and trend analysis, a partition of the points is required. Furthermore, partitions provide clearer interpretability of the results, as compared to reporting dense regions with very high overlap. In such cases, PROCLUS is preferable to CLIQUE. The remainder of this paper is organized as follows. Section 2 describes our clustering algorithm in detail. In Section 3 we provide a theoretical analysis of the robustness of PROCLUS. Empirical results based on synthetic data are presented in Section 4. Section 5 contains conclusions and areas of future work. 1.2 Definitions and Notations In order to describe our algorithm we introduce a few notations and definitions. Let iV denote the total number of data points, and d denote the dimensionality of the data space. Let C = (~1, ~2,. . . , ~1) be the set of points in a cluster. The cenfroid of a cluster is the algebraic average of all the points in the cluster. Thus, the centroid of the cluster C is given by 5~ = c,\u201aÄú,, rci/t. Given a specific distance function d(., e), we define the radius of a cluster to be the average distance of a point from the centroid of the cluster: rc = C,\u201aÄú=, d&, q)/t. Various distance functions have been used in full dimensional clustering algorithms, depending on the particular problem being solved. Two such well known functions are the Manhattan distance and the euclidean distance. The Manhattan distance between two points Zl = (q1, * * .I ZI,~) and 52 = (az,~, . . .,zz,~) is given 1c1,i - ~2~1, and the euclidean by d:(xl,xz) = cfz, I distance is given by da(al, 82) = d\u201aÄô&(zl,i - a~+)~. Both distance functions are derived from norms. In general, the distance corresponding to the so-called Lp norm is given by dk(al, 22) = (\u201aÄò& Izl,i - ~2,il~)~lP. Thus the Manhattan distance corresponds to the Ll norm and the euclidean distance to the La norm. In this paper we will use a variant of the Manhattan distance, called Manhattan segmental distance, that is defined relative to some set of dimensions 2). Specif- ically, for any two points ~1 = (al,l, . . . , ZI,~) and 3J2 = (CZ,I,- - *, XZ,~), and for any set of dimensions V, IDI 5 d, the Manhattan segmental distance be- tween xl and x2 relative to 2, is given by dp(xl, ~2) = (xi,=., Ixl,i-az,i1)/12)1. Employing the Manhattan seg- mental distance as opposed to the traditional Manhat- tan distance is useful when comparing points in two dif- ferent clusters that have varying number of dimensions, because the number of dimensions has been normalized away. There is no comparably easy normalized vari- 63 ant for the euclidean metric. For many applications, the Manhattan segmental distance often has physical significance. One potential application is collaborative filtering [lo], where customers need to be partitioned into groups with similar interests for target marketing. Here one needs to be able to handle a large number of di- mensions (for different products or product categories) with an objective function representing the average dif- ference of preferences on the different objects. "},{"aspect":"expintro","tweet":" The simulations were performed on a 233-MHz IBM RS/SOOO computer with 128M of memory, running AIX 4.1.4. The data was stored on a 2GB SCSI drive. We report results obtained for synthetic data. We evaluate the accuracy of PROCLUS on synthetic data and determine how the running time scales with: - size of database. - dimensionality of the data space. - average dimensionality of clusters. We also investigate the cases in which CLIQUE can be used to return a partition of the data set. For those cases, we compare its accuracy and running time to those of PROCLUS. "},{"aspect":"problemdef","tweet":""},{"aspect":"solution","tweet":" 2 The Clustering Algorithm The problem of finding projected clusters is two-fold: we must locate the cluster centers and find the appro- priate set of dimensions in which each cluster exists. In the full dimensionality setting, the problem of find- ing cluster centers has been extensively investigated, both in the database and in the computational geome- try communities. A well known general approach is the so-called K-Medoids method (see, for example, [18] for a detailed discussion), which uses points in the original data set to serve as surrogate centers for clusters during their creation. Such points are referred to as medoids. One method which uses the K-Medoids approach, called CLARANS, was proposed by Ng and Han [21] for clus- tering in full dimensional space. We combine the greedy method of [14] with the local search approach of the CLARANS algorithm [21] to generate possible sets of medoids, and use some original ideas in order to find the appropriate dimensions for the associated clusters. The overall pseudocode for the algorithm is given in Figure 2. The algorithm proceeds in three phases: an initial- ization phase, an iterative phase, and a cluster refine- ment phase. The general approach is to find the best set of medoids by a hill climbing process similar to the one used in CLARANS, but generalized to deal with projected clustering. By \u201aÄúhill climbing\u201aÄù we refer to the process of successively improving a set of medoids, which serve as the anchor points for the different clus- ters. The purpose of the initialization phase is to re- duce the set of points on which we do the hill climbing, while at the same time trying to select representative points from each cluster in this set. The second phase represents the hill climbing process that we use in or- der to find a good set of medoids. We also compute a set of dimensions corresponding to each medoid so that the points assigned to the medoid best form a cluster in the subspace determined by those dimensions. The assignment of points to medoids is based on Manhat- tan segmental distances relative to these sets of dimen- sions. Thus, we search not just in the space of possible medoids but also in the space of possible dimensions associated with each medoid. Finally, we do a cluster refinement phase in which we use one pass over the data in order to improve the quality of the clustering. We detail each phase in the following. 2.1 Initialization Phase We call a set of k medoids piercing if each point is drawn from a different cluster. Clearly, finding such a set together with appropriate sets of dimensions is the key objective of our algorithm. The initialization phase is geared towards finding a small enough superset of a piercing set, so that it is possible to efficiently perform hill-climbing on it, as opposed to the entire database of points. In full dimensional algorithms, one of the techniques for finding a piercing set of medoids is based on a greedy method. In this process medoids are picked iteratively, so that the current :medoid is well separated from the medoids that have been chosen so far. The greedy technique has been :proposed in [14] and is illustrated in Figure 3. In full dimensionality, if there are no outliers and if the clusters are well enough separated, this method always returns a piercing set of medoids. However, it does not guarantee a piercing set for the projected clustering problem. In our algorithm we will use the greedy technique in order to find a good superset of a piercing set of medoids. In other words, if we wish to find k clusters in the data, we will pick a set of points of cardinality a few times larger than. k. We will perform two successive steps of subset selection in order to choose our superset. In the first step, we choose a random sample of data points of size proportional to the number of clusters we wish to generate. (In Figure 2, we denote this size by A . k, where A is a constant.) We shall denote this sample by S. In the second step, we apply the above- mentioned greedy technique to S in order to obtain an even smaller final set of points on which to apply the hill climbing technique during the next phase. In our algorithm, the final iset of points has size B . k, where B is a small constant. We shall denote this set by M. The reasons for choosing this two-step method are as follows: (1) The greedy technique tends to pick many outliers due to its distance based approach. On the other hand, the set S probably contains only a very small number of outliers, and the greedy algorithm is likely to pick some representatives from each cluster. (2) The reduction to the sample set S significantly reduces the running time of the initialization phase. 2.2 Iterative P\u201aÄôhase We start by choosing a random set of k medoids from M and progressively improve the quality of medoids by iteratively replacing ithe bad medoids in the current set with new points from M. The hill climbing technique can be viewed as a restricted search on the complete 64 Algorithm PROCLUS(No. of Clusters: k, Avg. Dinmwiona: I) { C; is the ith cluster ) { Vi is the set of dimensions associated with cluster I!; } is the set of medoids in current iteration 1, : tFnt best 1s the best set of medoids found so far } { N is the fmal set of medoids with associated dimensions } { A, B are constant begin integers } { 1. Initialization Phase} S = random sample of siz;e A . k M = GRBEDY(S, B.k) { 2. Iterative Phase} BestObjective = 00 M current = Random set of medoids {ml, m2, . . . mk} C M repeat { Approximate the optimal set of dimensions } for each medoid mi E Mcvr+ent do begin Let 6; be distance to nearest medoid from rni L; = Points in sphere centered at m; with ratis 6; end; L={Ll,...,Lk} Pl,%l... Z)k) = FindDimensions( k,l, 13) { Form the clusters } (Cl,..., Ck) = AssignPoints(D1,. . .2)k) ObjectiveFunction = EvaluateClustets(C~, . . Ck, D1 . . . Z)k) if ObjectiveFunction  Be&Objective then begin BestObjective = ObjectiveFunction M test = Mcuvrent Compute the bad medoids in i&*t end Compute Current by replacing the bad medoids in M beat with random points from M until (termination-criterion) { 3. Refinement L = {C, r...tCk) Phase} 2,. . . Dk) = FindDimensions(k,l, L) V\u201aÄù 1,. . . , Ck) = AssignPoints(&, N = (Mbcstr VI, D2,. . . Dk) return(N) end . . . Z)k) Figure 2: The Clustering Algorithm Algorithm Greedy(Set of points: S, Number of medoids: k) { d(., .) is the distance function } begin M = {ml} { ml is a random point of S } { compute distance between each point and medoid ml } for each t E S \\ M d&(r) = d(z,ml) for i=2 to k begin { choose medoid m; to be far from previous medoids } let m; E S \\ M be s.t. dist(mi) = max{dist(z) 1 t E S \\ M} M=Mu{mi} { compute distance of each point to closest medoid } for eachxES\\M d&(z) = min{dist(z), d(z, m;)} end return M end Figure 3: The Greedy Algorithm graph with vertex set defined by all sets of medoids of cardinality k. The current node in the graph, denoted Mt,,,t in Figure 2, represents the best set of medoids found so far. The algorithm tries to advance to another node in the graph as follows: It first determines the bad medoids in it!&,t using a criterion we discuss at the end of this subsection. Then it replaces the bad medoids with random points from M to obtain another vertex in the graph, denoted Mcurrent. If the clustering determined by Mcurrent is better than the one determined by Mbest, the algorithm sets Mbest = M eur,.ent. Otherwise, it sets Mcurre,,t to another vertex of the graph, obtained by again replacing the bad medoids in &tb,,t with random points of M. If it!ft,,,t does not change after a certain number of vertices have been tried, the hill climbing phase terminates and i&,t is reported. In what follows, we detail how we evaluate the clustering determined by a given set of k medoids. This implies solving two problems: finding the appropriate set of dimensions for each medoid in the set, and forming the cluster corresponding to each medoid. Finding Dimensions: Given a set of k medoids M = Cm, -*-I mk}, PROCLUS evaluates the locality of the space near them in order to find the dimensions that matter most. More exactly, for each medoid rn+ let Si be the minimum distance from any other medoid to rn+, i.e. & = minjgid(m+,mj). For each i, we define the locality Li to be the set of points that are within distance & from m+. (Note that the sets Cr, . . . . lk need not necessarily be disjoint, nor cover the entire set of data points). We then compute the average distance along each dimension from the points in .Ci to m. Let Xi,j denote this average distance along dimension j. To each medoid m we wish to associate those dimensions j for which the values Xi,j are as small as possible relative 65 to statistical expectation, subject to the restriction that the total number of dimensions associated to medoids must be equal to Ice 1. We add the additional constraint that the number of dimensions associated with a medoid must be at least 2. Corresponding to each medoid i we compute the mean Yi = (Cf=,Xi,j)/d and the standard deviation ui = /F of the values Xi,j. Note that Yi represents in fact the average of the Manhattan segmental distances between the points in J$ andx?,T;&ive to the entire space. Thus the value Zi,j = *\u201aÄòi, \u201aÄô indicates how the j-dimensional average distance associated with the medoid m is related to the average Manhattan segmental distance associated with the same medoid. A negative value of Zi,j indicates that along dimension j the points in Ci are more closely correlated to the medoid w. We want to pick the smallest values Zi,j so that a total of k - 2 values are chosen, and at least 2 values are chosen for any fixed i. This problem is equivalent to a so-called separable convex resource allocation problem, and can be solved exactly by a greedy algorithm [16]. Specifically, we sort all the Zi,j values in increasing order, preallocate the 2 smallest for each i (giving a total of 21c values), and then greedily pick the remaining lowest k + (l- 2) values. (There are other algorithms for solving this problem exactly that are even faster from a complexity point of view. We employ a greedy algorithm here because it is sufficiently fast in light of the typically expected values of h and 1. However, see [16] for further details.) With each medoid i we associate those dimensions j whose corresponding Zi,j value was chosen by the above algorithm. We denote the sets of dimensions thus found by %,D,,... 2)k. This is illustrated in Figure 4. Forming Clusters: Given the medoids and their associated sets of dimensions, we assign the points to the medoids using a single pass over the database. For each i, we compute the Manhattan segmental distance relative to Vi between the point and the medoid w, and assign the point to the closest medoid. See Figure 5. We evaluate the quality of a set of medoids as the average Manhattan segmental distance from the points to the centroids of the clusters to which they belong (see Figure 6). Note that the centroid of a cluster will typically differ from the medoid. We also determine the bad medoids as follows: The medoid of the cluster with the least number of points is bad. In addition, the medoid of any cluster with less than (N/k) - minDeviation points is bad, where minDeviation is a constant smaller than 1 (in most experiments, we choose minDeviation = 0.1). We make the assumption that if a medoid forms a cluster with less than (N/k) . minDeviation points (where minDewiation is usually O.l), it is likely that the medoid is either an outlier, or it belongs to a cluster that is pierced by at least one other medoid in the set. Conversely, we assume that an outlier is likely to form a cluster with very few points. We also assume that if the current set of medoids contains two or more medoids from the same natural cluster, one of these medoids (which is the most \u201aÄúcentral\u201aÄù) is likely to form a cluster containing most of the points in the natural cluster, while the remaining medoids that pierce that cluster will form clusters with few points. 2.3 Refinement Phase After the best set of medoids is found, we do one more pass over the data to improve the quality of the cluster- ing. Let {Cl,. . . , Ck]. be the clusters corresponding to these medoids, formed during the Iterative Phase. We discard the dimensions associated with each medoid and compute new ones by a procedure similar to that in the previous subsection. The only difference is that in order to analyze the dimen.sions associated with each medoid, we use the distribution of the points in the clusters at the end of the iterative phase, as opposed to the local- ities of the medoids. In other words, we use Ci instead of .lZi. Once the new dimensions have been computed, we reassign the points to the medoids relative to these new sets of dimensions. The process is illustrated in Figure 2. Outliers are also handled during this last pass over the data. For each medoid RC and new set of dimensions Vi, we find the smallest Manhattan segmental distance Ai to any of the other (k - 1) medoids with respect to the set of dimensions \u201aÄòDi: Ai = minj#i dvi(nt+, mj) We also refer to ,Ai as the sphere of influence of the medoid m. A point is an outlier if its segmental distance to each medoid w, relative to the set of dimensions Vi, exceelds Ai. 3 Analyzing the Robustness of PROCLUS To ensure good accuracy of the output, PROCLUS must be able to ac:hieve two essential results: find a piercing set of medoids, and associate the correct set of dimensions to each medoid. In our discussion of the Initialization .Phase, we gave some insight into why we expect the set M to contain a piercing set of medoids. In the following, we will discuss some issues related to the robustSness of the procedure for finding dimensions. It is important to note that since the locality of a medoid is used in order to determine the set of dimensions corresponding to it, a sufficient number of points must exists in the locality in order to have a robust algorithm. The total number of points in the localities of all the medoids is also useful in order to estimate the number of dimensions for a given cluster. To give some insight into how the localities of medoids 66 Algorithm FindDimensions(k,l, L) begin { d is the total number of dimensions } { X;J is the average distance from the points in I$ tc medoid m; , along dimension j} for each medoid i do begin for e&h dimension j do Zi,j = (Xi,j - Y~)/u; end Pick the k. 1 numbers with the least (most negative) ~~alues of Zi,j subject to the constraint that there are at least 2 dimensions for each cluster if Z;#j is picked then add dimension j to Vi return(VL,Vz,. . . Vk) end Figure 4: Finding the Dimensions Algorithm AssignPoints(V1, V2, . . . Vk) begin foreachiE{l,...,k}doC;=+ for each data point p do begin Let dpi (p, m;) be Manhattan segmental distance of point p from medoid m; relative to dimensions V;; Find i with lowest value of dp,(p,m;) and add p to C;; end; return (Cl,. . . ,Ck) end; Figure 5: Assigning Points to the Various Clusters Algorithm EvaluateClusters(C1,. . . ,Ck, VI,. . . Vk) begin for each Ci do begin for each dimension j E 2); do begin Yi8j = Average distance of points in C; to centroid of C; along dimension j end ret;rn( C-f\u201aÄô- ) end Figure 6: Evaluating the Clusters look like, suppose randomly from the the more elaborate the following. first that the medoids are chosen entire set of points, rather than by procedure in PROCLUS. We prove Theorem 3.1 Let k be the number of medoids and N be the total number of data points. Then, for a random set of k me&ids (ml,. . . ,mk}, the ezpected number of points in J!I~ for the medoid m+ is N/k. Proof: Let d\u201aÄù,, d$, . . . d$ denote the distances of the N points from medoid mi. The problem is equivalent to the following standard result in order statistics (see [2] for details): Given a set of N values (dii, d$, . . ., d&}, suppose we choose k - 1 of them randomly. Then, the expected number of values in the set that are smaller than the k - 1 chosen values is equal to N/k. The k - 1 randomly chosen values correspond to the distances from the k - 1 other medoids to medoid rn+. I The above result shows that, if the medoids were chosen at random from the entire data set, the expected number of points in each locality would be sufficient to ensure the robustness of the FindDimensions procedure. Since our method for choosing the medoids is not random, but rather biased towards ensuring that the medoids are as far away from each other as possible ( i.e. their localities have large radii), we expect the localities of the medoids to contain at least N/k points each. "},{"aspect":"expcomparison","tweet":""}]}