{"user_name":" Graph Indexing: Tree + Delta >= Graph ","user_timeline":[{"aspect":"abstract","tweet":" ABSTRACT Recent scientific and technological advances have witnessed an abundance of structural patterns modeled as graphs. As a result, it is of special interest to process graph containment queries effectively on large graph databases. Given a graph database G, and a query graph q, the graph containment query is to retrieve all graphs in G which contain q as subgraph(s). Due to the vast number of graphs in G and the nature of complexity for subgraph isomorphism testing, it is desirable to make use of high-quality graph indexing mechanisms to reduce the overall query processing cost. In this paper, we propose a new cost-effective graph indexing method based on frequent tree-features of the graph database. We analyze the effectiveness and efficiency of tree as indexing feature from three critical aspects: feature size, feature selection cost, and pruning power. In order to achieve better pruning ability than existing graph-based indexing methods, we select, in addition to frequent tree-features (Tree), a small number of discriminative graphs (\u201aàÜ) on demand, without a costly graph mining process beforehand. Our study verifies that (Tree+\u201aàÜ) is a better choice than graph for indexing purpose, denoted (Tree+\u201aàÜ \u201aâ\u2022Graph), to address the graph containment query problem. It has two implications: (1) the index construction by (Tree+\u201aàÜ) is efficient, and (2) the graph containment query processing by (Tree+\u201aàÜ) is efficient. Our experimental studies demonstrate that (Tree+\u201aàÜ) has a compact index structure, achieves an order of magnitude better performance in index construction, and most importantly, outperforms up-to-date graphbased indexing methods: gIndex and C-Tree, in graph containment query processing. "},{"aspect":"expanalysis","tweet":" 8. CONCLUSIONS Graph indexing plays a critical role in graph containment query processing on large graph databases which have gained increasing popularity in bioinformatics, Web analysis, pattern recognition and other applications involving graph structures. Previous graph indexing mechanisms take 25 paths and graphs as indexing features and suffer from overly large index size, substantial index construction overhead and expensive query processing cost. In this paper, we have proposed a cost-effective graph indexing mechanism for addressing the graph containment query problem: index based on frequent tree-features. We analyze the effectiveness and efficiency of tree as indexing feature, w.r.t. path and graph from three critical perspectives: feature size |F|, feature selection cost CFS and feature pruning power |Cq|. In order to achieve better pruning ability than path-based and graphbased indexing mechanisms, we deliberately select a small portion of discriminative graph-features on demand, which consist of our additional index structure \u201aàÜ related only to query graphs. Our analysis and performance studies confirm that the proposed graph indexing algorithm, (Tree+\u201aàÜ), is a better choice than up-to-date path-based or graph-based indexing algorithms. (Tree+\u201aàÜ) holds a compact index structure, achieves good performance in index construction and most importantly, provides satisfactory query performance for answering graph containment queries over large graph databases. "},{"aspect":"expdata","tweet":""},{"aspect":"background","tweet":" 1. INTRODUCTION Recent scientific and technological advances have resulted in an abundance of data modeled as graphs. As a general data structure representing relations among entities, graph has been used extensively in modeling complicated structures and schemaless data, such as proteins [3], images [4], Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, to post on servers or to redistribute to lists, requires a fee and/or special permission from the publisher, ACM. VLDB \u201aÄò07, September 23¬≠28, 2007, Vienna, Austria. Copyright 2007 VLDB Endowment, ACM 978¬≠1¬≠59593¬≠649¬≠3/07/09. Jeffrey Xu Yu The Chinese University of Hong Kong yu@se.cuhk.edu.hk 938 Philip S. Yu IBM T. J. Watson Research Center psyu@us.ibm.com visions [7], program flows [13], XML documents [15], the Internet and the Web [6], etc. The dominance of graphs in real-world applications asks for effective graph data management so that users can organize, access, and analyze graph data in a way one might have not yet imagined. Among myriad graph-related problems of interest, a common and critical one shared in many applications in science and engineering is the graph containment query problem: given a massive dataset with each transaction modeled as graphs, and a query, represented as graph too, find all graphs in the database which contain the query as a subgraph(s) [1, 4, 9, 20, 21, 23]. To answer graph containment queries effectively is challenging for a number of reasons. First, to determine whether a graph in the database contains the query graph is a subgraph isomorphism problem, which has been proven to be NP-complete [8]; Second, the graph database can be large, which makes a sequential scan over the database impracticable. As to graph databases, existing database infrastructures might not answer graph containment queries in an efficient manner. For example, the indices built on the labels of vertices or edges are usually not selective enough to distinguish complicated, interconnected structures. Therefore, high performance graph indexing mechanisms need to be devised to prune graphs that obviously violate the query requirement. In this way, the number of costly subgraph isomorphism testings is reduced, which is the primary motivation of our study. The strategy of graph indexing is to shift high online query processing cost to the off-line index construction phase. So index construction is always computationally expensive because it requests deliberately selecting high quality indexing features with great pruning power from the graph database. The process of selection and evaluation of indexing features is critical because features with higher pruning power are superior to be selected as index entries. At the same time, the number of indexing features should be as small as possible to keep the whole index structure compact, better to be held in main memory for efficient access and retrieval. In sum, a high quality graph indexing mechanism should be time-efficient in index construction, and indexing features should be compact and powerful for pruning purpose. In this paper we present a new tree-based indexing approach to address the graph containment query problem. Our work is motivated by an evidence that a large number of frequent graphs in the graph database are trees in nature. In fact, for many real-world datasets, over 95% of frequent graphs are trees. It leads us to reconsider an alternative Figure 1: A Graph Database with Three Graphs Figure 2: A Query Graph solution: \u201aÄúCan we use tree instead of graph as the basic indexing feature?\u201aÄù Tree, which is also denoted as free tree, is a special connected, acyclic and undirected graph. As a generalization of linear sequential patterns, tree preserves plenty of structural information of graph. Meanwhile, tree is also a specialization of general graph, which avoids undesirable theoretical properties and algorithmic complexity incurred by graph. As the middle ground between these two extremes, tree becomes an ideal candidate of indexing features over the graph database. The main contributions of this paper are summarized below. \u201aÄ¢ We analyze the effectiveness and efficiency of trees as indexing features by comparing them with paths and graphs from three critical aspects, namely, feature size, feature selection cost, and pruning power. We show that tree-features can be effectively and efficiently used as indexing features for graph databases. Our main results show: (1) in many applications the majority of graph-features (usually more than 95%) are tree-features indeed; (2) frequent tree-features and graph-features share similar distributions and frequent tree-features have similar pruning power like graphfeatures; and (3) tree mining can be done much more efficiently than graph mining (it is not cost-effective to mine frequent graph features in which more than 95% are trees). \u201aÄ¢ We propose a new graph indexing mechanism, called (Tree+\u201aàÜ), that first selects frequent tree-features as the basis of a graph index, and then on-demand selects a small number of discriminative graph-features that can prune graphs more effectively than the selected tree-features, without conducting costly graph mining beforehand. A key issue here is how to achieve the similar pruning power of graph-features without graph mining. We propose a new approach by which we can approximate the pruning power of a graph-feature by its subtree-features with upper/lower bounds. \u201aÄ¢ We conducted extensive experimental studies using a real dataset and a series of synthetic datasets. We compared our (Tree+\u201aàÜ) with two up-to-date graphbased indexing methods: gIndex [23] and C-Tree [9]. Our study confirms that (tree+\u201aàÜ) outperformsgIndex and C-Tree in terms of index construction cost and query processing cost. The rest of the paper is organized as follows. In Section 2, we give the problem statement for the graph containment query processing, and discuss an algorithmic framework with a cost model. In Section 3, we analyze the indexability of frequent features (path, tree and graph) from 939 three perspectives: feature size, feature selection cost, and pruning power. Section 4 discusses our new approach to add discriminative graph-features on demand. Section 5 presents the implementation details of our indexing algorithm (Tree+\u201aàÜ) with an emphasis on index construction and query processing. Section 6 shows the related work concerning the graph containment query problem over large graph databases. Our experimental study is reported in Section 7. Section 8 concludes this paper. "},{"aspect":"expintro","tweet":" 7. EXPERIMENTAL STUDY In this section, we report our experimental studies that validate the effectiveness and efficiency of our (Tree+\u201aàÜ) algorithm. (Tree+\u201aàÜ) is compared with gIndex and C-Tree, two up-to-date graph-based indexing algorithms. We use two kinds of datasets in our experiments: one real dataset and a series of synthetic datasets. Most of our experiments have been performed on the real dataset since it is the source of real demand. All our experiments are performed on a 3.4GHz Intel PC with 2GB memory, running MS Windows XP and Redhat Fedora Core 4. All algorithms of (Tree+\u201aàÜ) are implemented in C++ using the MS Visual Studio compiler. "},{"aspect":"problemdef","tweet":" 2. PRELIMINARIES In this section, we introduce preliminary concepts and outline an algorithmic framework to address the graph containment query problem. A cost evaluation model is also presented on which our analysis of graph indexing solutions are based. 2.1 Problem Statement A graph G = (V, E, Œ£, Œª) is defined as a undirected labeled graph where V is a set of vertices, E is a set of edges (unordered pairs of vertices), Œ£ is a set of labels, and Œª is a labeling function, Œª : V \u201aà™ E \u201aÜí Œ£, that assigns labels to vertices and edges. Let g and g \u201aÄ≤ be two graphs. g is a subgraph of g \u201aÄ≤ , or g \u201aÄ≤ is a supergraph of g, denoted g \u201aäÜ g \u201aÄ≤ , if there exists a subgraph isomorphism from g to g \u201aÄ≤ . We also call g \u201aÄ≤ contains g or g is contained by g \u201aÄ≤ . The concept of subgraph isomorphism from g to g \u201aÄ≤ is defined as a injective function from Vg to Vg \u201aÄ≤ that preserves vertex labels, edge labels and adjacency. The concept of graph isomorphism can be defined analogously by using a bijective function instead of an injective function. The size of g is denoted size(g) = |Vg|. A tree, also known as free tree, is a special undirected labeled graph that is connected and acyclic. For tree, the concept of subtree, supertree, subtree isomorphism, tree isomorphism can be defined accordingly. A path is the simplest tree whose vertex degrees are no more than 2. Given a graph database G = {g1, g2, ¬∑ ¬∑ ¬∑ , gn} and an arbitrary graph g, let sup(g) = {gi|g \u201aäÜ gi, gi \u201aàà G}. |sup(g)| is the support, or frequency of g in G. g is frequent if its support is no less than œÉ ¬∑ |G|, where œÉ is a minimum support threshold provided by users. Graph Containment Query Problem: Given a graph database, G = {g1, g2, ¬∑ ¬∑ ¬∑ , gn}, and a query graph q, a graph containment query is to find the set, sup(q), from G. The graph containment query problem is NP-complete. It is infeasible to find sup(q) by sequentially checking subgraph isomorphism between q and every gi \u201aàà G, for 1 \u201aâ§ i \u201aâ§ n. And it is especially challenging when graphs in G are large, and |G| is also large in size and diverse. Graph indexing provides an alternative to tackle the graph containment query problem effectively. Example 2.1: A sample query graph, shown in Figure 2, is posed to a sample graph database with three graphs, shown in Figure 1. The graph in Figure 1 (c) is the answer. \u201aú∑ "},{"aspect":"solution","tweet":" 2.2 An Algorithmic Framework Given a graph database G, and a query, q, the graph containment query can be processed in two steps. First, a preprocessing step called index construction generates indexing features from G. The feature set, denoted F, constructs the index, and for each feature f \u201aàà F, sup(f) is maintained. Second, a query processing step is performed in a filtering- Table 1: Frequent Features as Indices |F| CFS |Cq| Comments FP small low large simple structure, easy to be discovered, limited indexing ability FT large intermediate small proper structure, easy to be discovered, good indexing ability FG large high small complex structure, hard to be discovered, good indexing ability verification fashion. The filtering phase uses indexing features contained in q to compute the candidate answer set, defined as Cq = \\ sup(f) (1) f\u201aäÜq\u201aàßf\u201aààF Every graph in Cq contains all q\u201aÄôs indexing features. Therefore, the query answer set, sup(q), is a subset of Cq. The verification phase checks subgraph isomorphism for every graph in Cq. False positives are pruned from Cq and the true answer set sup(q) is returned. Eq. (1) suggests indexing structural patterns, that have great pruning power, to reduce false positives included in the candidate answer set, Cq. Furthermore, a small-sized index including high-frequency features is preferred due to its compact storage. However, it is often unknown beforehand which patterns are valuable for indexing. 2.3 Query Cost Model The cost of processing a graph containment query q upon G, denoted C, can be modeled below C = Cf + |Cq| √ó Cv Here, Cf is the total cost for filtering based on Eq. (1) in the filtering phase. Every graph in Cq needs to be fetched from the disk in the verification phase to verify subgraph isomorphism, where Cv is such an average cost. The key issue to improve query performance is to minimize |Cq| for a graph containment query, q. Intuitively, |Cq| will be minimized if we index all possible features of G. However, it is infeasible because the feature set, F, can be very large, which makes the space complexity prohibitive, and the filtering cost Cf becomes large accordingly. In other words, enlarging F will increase the cost of Cf, but probably reduce |Cq|; On the other hand, reducing F will decease Cf but probably increase |Cq|. There is a subtle trade-off between time and space in the graph containment query problem. As seen above, the graph containment query problem is challenging. An effective graph indexing mechanism with high quality indexing features is required to minimize the query cost C as much as possible. On the other hand, the feature selection process itself introduces another nonnegligible cost for index construction, i.e., the cost to discover F from G, denoted CFS. A graph indexing mechanism is also cost-effective if it results in a small feature set size (or index size), |F|, to reduce Cf in the query processing. In next section, we discuss the three major factors: |F|, CFS, and |Cq|, that affect C. 3. GRAPH VS. TREE VS. PATH Frequent features (paths, trees, graphs) expose intrinsic characteristics of a graph database. They are representatives to discriminate between different groups of graphs in a graph database. We denote the frequent path-feature set (2) 940 as FP, the frequent tree-feature set as FT and the frequent graph-feature set as FG. In the following, when we discuss different frequent feature sets, we assume that all the frequent feature sets are mined with the minimum support threshold œÉ. Note: FP \u201aäÜ FT \u201aäÜ FG. Because path is a special tree, and tree is a spatial graph, we have the following. FT \u201aÄ≤ = FT \u201aàí FP (3) FG \u201aÄ≤ = FG \u201aàí FT (4) FG = FP \u201aà™ FT \u201aÄ≤ \u201aà™ FG \u201aÄ≤ (5) Here, FT \u201aÄ≤ and FG \u201aÄ≤ denote the tree-feature set without any path-features (nonlinear tree-features, Eq. (3)) and the graphfeature set without any tree-features (non-tree graph-features, Eq. (4)). The relationships among them are given in Eq. (5). Note: FP \u201aà© FT \u201aÄ≤ = \u201aàÖ, FP \u201aà© FG \u201aÄ≤ = \u201aàÖ, and FT \u201aÄ≤ \u201aà© FG \u201aÄ≤ = \u201aàÖ. We explore the indexability of frequent features for the purpose of addressing the graph containment query problem, and focus on the three major factors, namely, the size of a frequent feature set (|F|), the feature selection cost (CFS), and the candidate answer set size (|Cq|), that affect the query processing cost C (Eq. (1)). Some observations are summarized in Table 1 for path-feature set (FP), treefeature set (FT) and graph-feature set (FG). We discuss each of them in detail below. 3.1 Feature Set Size: |F| The size of a frequent feature set (|F|) is of special interest because a space-efficient index is critical to query performance, as mentioned in Section 2.3. Below, we discuss different frequent feature sets: FG, FT, and FP w.r.t. feature set size. To our surprise, among all frequent graph-features in FG (Eq. (5)), the majority (usually more than 95%) are trees. The non-tree frequent graph-feature set, FG \u201aÄ≤, can be very small in size, and the significant portion of FG is FT \u201aÄ≤, i.e., non-linear tree-features. FP shares a very small portion in FG, because a path-feature has a simple linear structure, which has little variety in structural complexity. We explain the reasons below. First, for a non-tree frequent graphfeature, g \u201aàà FG \u201aÄ≤, based on the Apriori principle, all g\u201aÄôs subtrees, t1, t2, ¬∑ ¬∑ ¬∑ , tn are frequent, in other words, ti \u201aàà FT, for 1 \u201aâ§ i \u201aâ§ n. Second, given two arbitrary frequent non-tree graph-features, g and g \u201aÄ≤ , in FG \u201aÄ≤, because of the structural diversity and (vertex/edge) label variety, there is little chance that subtrees of g coincide with those of g \u201aÄ≤ . It is true especially when graphs of G are large and labels of graph are diverse. Consider a complete graph K10 with 10 vertices, and each vertex has a distinct label. If K10 is frequent, all its 10 1vertex subtrees, 45 2-vertex subtrees, ¬∑ ¬∑ ¬∑ , and 100, 000, 000 10-vertex subtrees are frequent. There are 162, 191, 420 frequent subtrees in total. If the linear trees (paths) are excluded, there are still 156, 654, 565 frequent trees involved. Next, we investigate the frequent feature distributions. In other words, we consider the distributions of the total Number of Frequent Features Number of Frequent Features 7000 6000 5000 4000 3000 2000 1000 0 |F P | |F T | |F G | 0 5 10 15 20 Frequent Feature Size Number of Frequent Features 1000 100 10 1 |F P | |F T \u201aÄô| |F G \u201aÄô| 1 2 3 4 5 6 7 8 9 10 Frequent Feature Size (a) |FP | vs. |FT | vs. |FG| (b) |FP | vs. |FT \u201aÄ≤| vs. |FG \u201aÄ≤| (c) |FP | vs. |FT \u201aÄ≤| vs. |FG \u201aÄ≤| Figure 3: The Real Dataset N = 1000, œÉ = 0.1 200000 150000 100000 50000 |F P | |F T | |F G | 0 0 5 10 15 20 25 Frequent Feature Size Number of Frequent Features 100000 10000 1000 100 10 1 |F P | |F T \u201aÄô| |F G \u201aÄô| 1 2 3 4 5 6 7 8 9 10 Frequent Feature Size Number of Frequent Features 7000 6000 5000 4000 3000 2000 1000 0 100000 90000 80000 70000 60000 50000 40000 30000 20000 10000 0 Number |F P | |F T \u201aÄô| Frequent Features (a) |FP | vs. |FT | vs. |FG| (b) |FP | vs. |FT \u201aÄ≤| vs. |FG \u201aÄ≤| (c) |FP | vs. |FT \u201aÄ≤| vs. |FG \u201aÄ≤| Figure 4: The Synthetic Dataset N = 1000, œÉ = 0.1 number of frequent features that have the same size, n. Figure 3 (a) illustrates the numbers of frequent features (paths, trees and graphs) w.r.t. the feature size (modeled as vertex number), in a sample AIDS antivirus screen dataset of 1, 000 graphs and œÉ = 0.1. When n increases, both |FT | and |FG| share similar distributions. They first grow exponentially, and then drop exponentially. Similar phenomena appear in the sample synthetic dataset of 1, 000 graphs and œÉ = 0.1, as illustrated in Figure 4 (a). It is worth noting that in terms of distribution, the frequent tree-features and the frequent graph-features behave in a similar way. Note: |FP | is almost unchanged while n increases, because of simple linear structure. We also compare among the frequent path-feature set, FP, the frequent nonlinear tree-feature set, FT \u201aÄ≤ = FT \u201aàí FP, and the frequent non-tree graph-feature set, FG \u201aÄ≤ = FG \u201aàí FT. In the sample AIDS antivirus screen dataset, when the feature size varies from n = 1 to 10, the number of frequent nonlinear tree-features grows exponentially and is much larger than frequent non-tree graph-features, as shown in Figure 3 (b). In total, for all frequent fea- tures with size up to 10, FT \u201aÄ≤ dominates, as shown in Figure 3 (c). In the sample synthetic dataset, non-tree graphfeatures more frequently appear in G, so many frequent nontree graph-features are discovered, in comparison with the sample AIDS antivirus dataset. However, the number of frequent non-path tree-features still grows much faster than non-tree graphs, as shown in Figure 4 (b). FT \u201aÄ≤ dominates FG, as shown in Figure 4 (c). Based on the analysis mentioned above, some conclusions can be made w.r.t. the feature size |F|: First, in terms of feature distributions, tree-features and graph-features share a very similar distribution; Second, the size of frequent treefeature set dominates the total feature set, F; Third, the number of non-tree graph-features is very small in FG. 3.2 Feature Selection Cost: CFS In the previous section, we indicated that, in the frequent graph-feature set FG, the number of non-tree graph-features can be very small, i.e., the majority of graph-features in FG are trees (including paths) in nature. It motivates us to 941 Number of Frequent Features Number |F P | |F T \u201aÄô| |F G \u201aÄô| Frequent Features reconsider if there is a need to conduct costly graph mining to discover frequent graph-features in which most of them (usually more than 95%) are trees indeed. Below, we briefly review the cost for mining frequent paths/trees/graphs from graph databases. Given a graph database, G, and a minimum support threshold, œÉ, we sketch an algorithmic framework, called FFS (Frequent Feature Selection), shown in Algorithm 1, to discover the frequent feature set F (FP/FT/FG) from G. The algorithm initiates from the smallest frequent features in G (size(f) = 1 at line 2) and expands current frequent features by growing one vertex each time. This pattern-growth process, denoted FFM (Frequent Feature Mining), is recursively called until all frequent features are discovered from G , shown in Algorithm 2. The candidate frequent feature set of a frequent feature f, C(f) = {f \u201aÄ≤ |size(f \u201aÄ≤ ) = size(f)+1}, is determined as a projected database, upon which frequency checking is performed to discover larger-size frequent features (line 5 \u201aàí 6). If the frequent graph-feature set FG needs to be selected from G by the FFS algorithm, two prohibitive operations are unavoidable. In Algorithm 2, line 1, graph isomorphism has to be checked to determine whether a frequent graphfeature, f, has already been selected in F or not. If f has been discovered before, there is no need to mine f and all its supergraphs again. The graph isomorphism problem is not known to be either P or NP-complete, so it is unlikely to check f \u201aàà F in polynomial time. In Algorithm 2, line 5, subgraph isomorphism testing needs to be performed to determine whether a candidate graph f \u201aÄ≤ \u201aàà C(f) is frequent or not. The subgraph isomorphism testing has been proven to be NP-complete. Therefore, the feature selection cost, CFS, for discovering FG from G is expensive. If the frequent tree-feature set FT needs to be selected, although a costly tree-in-graph testing is still unavoidable in Algorithm 2, line 5, tree isomorphism in Algorithm 2, line 1, can be performed efficiently in O(n), where n is tree size [2]. Compared with FG, FT can be discovered much more efficiently from G. If the frequent path feature set FP needs to be selected, the costly (sub)graph isomorphism checking boils down to |F G \u201aÄô| Algorithm 1 FFS (G, œÉ) Input: A graph database G, the minimum support threshold œÉ Output: The frequent feature set F 1: F \u201aÜê \u201aàÖ; 2: for all frequent features, f, with size(f) = 1 do 3: FFM(f, F, G, œÉ); 4: return F; Algorithm 2 FFM (f, F, G, œÉ) Input: A frequent feature f, the frequent feature set F, G and œÉ Output: The frequent feature set F 1: if (f \u201aàà F) then return; 2: F \u201aÜê F \u201aà™ {f}; 3: Scan G to determine the candidate frequent feature set of f, C(f) = {f \u201aÄ≤ |size(f \u201aÄ≤ ) = size(f) + 1}; 4: for all f \u201aÄ≤ \u201aàà C(f) do 5: if |sup(f \u201aÄ≤ )| \u201aâ\u2022 œÉ √ó |G| then 6: FFM(f \u201aÄ≤ , F, G, œÉ); the (sub)string matching operation, which can be done efficiently in polynomial time. Compared with FG and FT, the discovery of FP from G is the most efficient. Based on the analysis mentioned above, we draw the conclusion w.r.t. the feature selection cost, CFS: It is not costeffective to make use of an expensive graph mining process to select frequent graph-features, most of which are treefeatures indeed. 3.3 Candidate Answer Set Size: |Cq| The key to boost graph containment query performance is to minimize candidate answer set size, |Cq|, given a query q. It requests selecting frequent features that have great pruning power (Eq. (1)). We define the pruning power power(f) of a frequent feature, f \u201aàà F, in Eq. (6), power(f) = |G| \u201aàí |sup(f)| |G| The pruning power, power(f), gives a real number between 0 and 1, such as 0 \u201aâ§ P(f) < 1. When power(f) = 0, f is contained in every graph of G, so f has no pruning ability if it is included in the index. When power(f) approaches to 1, the feature f has great pruning power to reduce |Cq|, if it appears in q. Note power(f) cannot be 1, because f is a feature that appears at least once in G. Since q may contain multiple frequent features, we consider the pruning power bestowed by a set of such features together. Let S = {f1, f2, ¬∑ ¬∑ ¬∑ , fn} \u201aäÜ F, for 1 \u201aâ§ i \u201aâ§ n. The pruning power of S can be similarly defined as power(S) = |G| \u201aàí | T n i=1 sup(fi)| |G| Lemma 3.1: Given a frequent feature f \u201aàà F. Let its frequent sub-feature set be S(f) = {f1, f2, ¬∑ ¬∑ ¬∑ , fn} \u201aäÜ F, for fi \u201aäÜ f and 1 \u201aâ§ i \u201aâ§ n. Then, power(f) \u201aâ\u2022 power(S(f)). \u201aú∑ Proof Sketch: for all gi \u201aàà sup(f), we have f \u201aäÜ gi. For every fi \u201aàà S(f), because fi \u201aäÜ f, fi \u201aäÜ gi is true. Hence, gi \u201aàà sup(fi). If gi \u201aàà sup(fi), for 1 \u201aâ§ i \u201aâ§ n, then gi T \u201aàà n i=1 sup(fi). Therefore, sup(f) \u201aäÜ Tn i=1 sup(fi). It implies (6) (7) 942 Pruning Power 1 0.8 0.6 0.4 0.2 power(g) power(T(g)) 0 0 20 40 60 80 100 120 Frequent graphs 0 0 20 40 60 80 100 120 Frequent graphs (a) power(g) vs. power(T (g)) (b) power(g) vs. power(P(g)) Pruning Power 1 0.8 0.6 0.4 0.2 Figure 5: Pruning Power power(g) power(P(g)) |sup(f)| \u201aâ§ | Tn i=1 sup(fi)|, so power(f) \u201aâ\u2022 power(S(f)). \u201aú∑ Based on Lemma 3.1, we can directly get the following two important results: Theorem 3.1: Given a frequent graph-feature g \u201aàà F, and let its frequent sub-tree set be T (g) = {t1, t2, ¬∑ ¬∑ ¬∑ , tn} \u201aäÜ F. Then, power(g) \u201aâ\u2022 power(T (g)). \u201aú∑ Theorem 3.2: Given a frequent tree-feature t \u201aàà F, and let its frequent sub-path set be P(t) = {p1, p2, ¬∑ ¬∑ ¬∑ , pn} \u201aäÜ F. Then, power(t) \u201aâ\u2022 power(P(t)). \u201aú∑ Theorem 3.1 demonstrates that the pruning power of a frequent graph-feature is no less than that of all its frequent subtree-features. Similarly, the pruning power of a frequent tree-feature is no less than that of all its frequent sub-path features, as presented in Theorem 3.2. Therefore, among all frequent features in F, graph-feature has the greatest pruning power; path-feature has the least pruning power; while tree-feature stands in the middle sharing the pruning power less than graph-feature, but more than path-feature. It is interesting to note that the pruning power of all frequent subtree-features, T (g), of a frequent graph-feature g can be similar to the pruning power of g. It is because T (g) may well preserve structural information provided by g. However, in general, there is a big gap between the pruning power of a graph-feature g and that of all its frequent sub-path features, P(g). It is because, when g is replaced by P(g), the structural information of g is almost lost and it becomes difficult to identify g in P(g). Therefore, frequent path-features (P(g)) can not be effectively used as a candidate to substitute g, in terms of pruning power. Figure 5 illustrates the pruning power distributions of frequent graph-features with regard to their subtrees and subpaths in the sample real dataset mentioned in Section 3. Among 122 frequent non-tree graph-features found in G, for each frequent graph-feature, g, its pruning power power(g) is firstly compared with power(T (g)), (Figure 5 (a)), and then compared with power(P(g)) (Figure 5 (b)). We observe that power(T (g)) is very close to power(g) for almost all frequent graph-features. However, P(g), the set of frequent subpaths of g, has quite limited pruning power. Remark 3.1: The frequent tree-feature set, FT, dominates FG in quantity, and FT can be discovered much more efficiently than FG from G. In addition, FT can contribute similar pruning power like that provided by FG. It is feasible and effective to select FT, instead of FG, as indexing features for the graph containment query problem. \u201aú∑ Consider Example 2.1. We explain the disadvantages of the path-based indexing approach that uses frequent pathfeatures to prune. As shown in Example 2.1, when the query graph, q (Figure 2) is issued against the graph database Figure 1. Only the graph in Figure 1 (c) is the answer. But, Figure 6: Frequent Graphs of G all the path-features appearing in the query graph q are c, c\u201aÄìc, c\u201aÄìc\u201aÄìc, c\u201aÄìc\u201aÄìc\u201aÄìc and c\u201aÄìc\u201aÄìc\u201aÄìc\u201aÄìc. They cannot be used to prune the two graphs in Figure 1 (a) and (b), even if all these path-features are frequent in the graph database. Reconsider Example 2.1 for the graph-based indexing approach that uses frequent graph-features as index entries. This approach needs to mine frequent graph-features beforehand, which incurs high computation cost. In this example, some frequent graph-features discovered are shown in Figure 6, with œÉ = 2/3. In order to answer the query graph (Figure 2), only the graph-feature (Figure 6 (a)) can be used, which is a tree-feature in nature, while other frequent graph-features (Figure 6 (b) and Figure 6 (c)) are mined wastefully. 4. GRAPH FEATURE ON DEMAND Based on the discussions in Section 3, a tree-based indexing mechanism can be efficiently deployed. It is compact and can be easily maintained in main memory, as shown in our performance studies. We have also shown that a treebased index can have similar pruning power like that provided by the costly graph-based index on average in general. However, based on Theorem 3.1, it is still necessary to use effective graph-features to reduce the candidate answer set size, |Cq|, while tree-features cannot. In this section, we discuss how to select additional non-tree graph-features from q on demand that have greater pruning power than their subtree-features, based on the tree-feature set discovered. Consider a query graph q, which contains a non-tree subgraph g \u201aàà FG \u201aÄ≤. If power(g) \u201aâà power(T (g)) w.r.t. pruning power, there is no need to index the graph-feature g, because its subtrees jointly have the similar pruning power. However, if power(g) \u201aâ´ power(T (g)), it will be necessary to select g as an indexing feature because g is more discriminative than T (g) for pruning purpose. Note the concept we use here in this paper is different from the discriminative graph concept used in gIndex, which is based on two frequent graph-features instead. In this paper, we select discriminative graph-features from queries on-demand, without mining the whole set of frequent graph-features from G beforehand. These selected discriminative graph-features are therefore used as additional indexing features, denoted \u201aàÜ, which can also be reused further to answer subsequent queries. In order to measure the similarity of pruning power between a graph-feature g and its subtrees, T (g), we define a discriminative ratio, denoted Œµ(g), for a non-tree graph, g \u201aàà FG \u201aÄ≤ w.r.t. T (g) as 8 < power(g) \u201aàí power(T (g)) if power(g) ÔøΩ= 0 Œµ(g) = : power(g) (8) 0 if power(g) = 0 Here, 0 \u201aâ§ Œµ(g) \u201aâ§ 1. When Œµ(g) = 0, g has the same pruning power as the set of all its frequent subtrees, T (g). The larger Œµ(g) is, the greater pruning power g has than T (g). When Œµ(g) = 1, the frequent subtree set T (g) has no prun- 943 Figure 7: Discriminative Graphs ing power, while g is the most discriminative graph-feature and definitely needed to be reclaimed and indexed from the graph database, G. Based on Eq. (8), we define a discriminative graph in Definition 4.1: Definition 4.1: A non-tree graph g \u201aàà FG \u201aÄ≤ is discriminative if Œµ(g) \u201aâ\u2022 Œµ0, where Œµ0 is a user-specified minimum discriminative threshold (0 < Œµ0 < 1). \u201aú∑ If a frequent non-tree graph g is not discriminative, we consider that there is no need to select g as an indexing feature, because it can not contribute more for pruning than its frequent subtrees that have already been used as indexing features. Otherwise, there is a good reason to reclaim g from G into the index, because g has greater pruning power than all its frequent subtrees (T (g)). Suppose we set œÉ = 2/3 and Œµ0 = 0.5 for the sample database in Figure 1. Figure 7 illustrates two discriminative frequent graph-features. The pruning power of Figure 7 (a) is (1 \u201aàí 2/3) = 1/3 and the pruning power of Figure 7 (b) is (1 \u201aàí 1/3) = 2/3. Note: all frequent subtrees in Figure 7 (a) are subtrees of c\u201aÄìc\u201aÄìc\u201aÄìc\u201aÄìc, whose pruning power is 0. So the discriminative ratio, Œµ, of Figure 7 (a) is 1. The discriminative ratio Œµ of Figure 7 (b) can be computed similarly as 1/2. 4.1 Discriminative Graph Selection Given a query q, let\u201aÄôs denote its discriminative subgraph set as D(q) = {g1, g2, ¬∑ ¬∑ ¬∑ , gn}, where every non-tree graph gi \u201aäÜ q (1 \u201aâ§ i \u201aâ§ n) is frequent and discriminative w.r.t. its subtree set, T (gi). For D(q), it is not necessary to reclaim every gi from G as indexing features, because to reclaim gi from G means to compute sup(gi) from scratch, which incurs costly subgraph isomorphism testings over the whole database. Given two graphs g, g \u201aÄ≤ \u201aàà D(q), where g \u201aäÜ g \u201aÄ≤ , intuitively, if the gap between power(g \u201aÄ≤ ) and power(g) is large enough, g \u201aÄ≤ will be reclaimed from G; Otherwise, g is discriminative enough for pruning purpose, and there is no need to reclaim g \u201aÄ≤ in the presence of g. Based on the above analysis, we propose a new strategy to select discriminative graphs from D(q). Recall in [23], a frequent graph-feature, g \u201aÄ≤ , is discriminative, if its support, |sup(g \u201aÄ≤ )|, is significantly greater than |sup(g)|, where g \u201aÄ≤ is a supergraph of g. It is worth noting that a costly graph mining process is needed to compute sup(g \u201aÄ≤ ) and sup(g). Below, we discuss our approach to select discriminative graph-features without graph mining beforehand. In order to do so, we approximate the discriminative computation between g \u201aÄ≤ and g, in the presence of our knowledge on frequent tree-features discovered. sup(g)(?) x ? ? \u201aàí\u201aàí\u201aàí\u201aàí\u201aÜí sup(g \u201aÄ≤ )(?) x ? sup(Tg) \u201aàí\u201aàí\u201aàí\u201aàí\u201aÜí sup(Tg \u201aÄ≤) The diagram above illustrates how to estimate the dis- criminative graph-features based on their frequent subtrees. Suppose g and g \u201aÄ≤ are two graph-features from D(q) such that g \u201aäÇ g \u201aÄ≤ , we define the occurrence probability of g in the graph database, G as Pr(g) = |sup(g)| |G| = œÉg (9) Similarly, the conditional occurrence probability of g \u201aÄ≤ , w.r.t. g, can be measured as Pr(g \u201aÄ≤ |g) = Pr(g \u201aàß g\u201aÄ≤ ) Pr(g) = Pr(g\u201aÄ≤ ) Pr(g) = |sup(g\u201aÄ≤ )| |sup(g)| (10) Pr(g \u201aàß g \u201aÄ≤ ) = Pr(g \u201aÄ≤ ) because g is a subgraph of g \u201aÄ≤ . For each occurrence of g \u201aÄ≤ in G, g must occur simultaneously with g \u201aÄ≤ , but the reverse is not necessarily true. Here, Pr(g \u201aÄ≤ |g) models the probability to select g \u201aÄ≤ from G in the presence of g. According to Eq. (10), if Pr(g \u201aÄ≤ |g) is small, g \u201aÄ≤ has a high probability to be discriminative, w.r.t. g. However, it is still impractical to calculate Pr(g \u201aÄ≤ |g) based on Eq. (10), because the exact values of sup(g) and sup(g \u201aÄ≤ ) are unknown yet. As illustrated above, instead, we estimate Pr(g \u201aÄ≤ |g) by making use of T (g) and T (g \u201aÄ≤ ). Note: all T (g) (\u201aäÜ FT), T (g \u201aÄ≤ ) (\u201aäÜ FT), and the entire frequent tree-feature set (FT) are known already. Below, we give the details, and discuss the tight upper and lower bound of Pr(g \u201aÄ≤ |g), based on T (g) and T (g \u201aÄ≤ ). Since g and g \u201aÄ≤ are both frequent, the following inequalities hold, |sup(g)| \u201aâ\u2022 œÉ|G| and |sup(g \u201aÄ≤ )| \u201aâ\u2022 œÉ|G| (11) Since g and g \u201aÄ≤ are both discriminative (g, g \u201aÄ≤ \u201aàà D(q)), the following inequalities holds, \u2026õ(g) \u201aâ\u2022 \u2026õ0 and \u2026õ(g \u201aÄ≤ ) \u201aâ\u2022 \u2026õ0 (12) Based on Eq. (6), Eq. (7) and Eq. (8), we translate the above inequality Eq. (12) to |sup(g)| (|sup(g \u201aÄ≤ )|) by an expression of |sup(T (g))| (|sup(T (g \u201aÄ≤ ))|) and \u2026õ0, |sup(g)| \u201aâ§ |G| \u201aàí |G| \u201aàí |sup(T (g)|) 1 \u201aàí \u2026õ0 |sup(g \u201aÄ≤ )| \u201aâ§ |G| \u201aàí |G| \u201aàí |sup(T (g\u201aÄ≤ )|) 1 \u201aàí \u2026õ0 (13) (14) Based on Eq. (11) and Eq. (14), we derive the upper bound of Pr(g \u201aÄ≤ |g) which is solely relied on T (g \u201aÄ≤ ), constant factors œÉ and \u2026õ0, as shown in Eq. (15). Here, œÉx = |sup(x)|/|G|, where x is a frequent feature (or a set of frequent features) of G. Pr(g \u201aÄ≤ |g) = |sup(g\u201aÄ≤ )| |sup(g)| \u201aâ§ |G| \u201aàí |G|\u201aàí|sup(T (g\u201aÄ≤ )|) 1\u201aàí\u2026õ0 = œÉ|G| œÉT (g \u201aÄ≤ ) \u201aàí \u2026õ0 (1 \u201aàí \u2026õ0)œÉ (15) Similarly, based on Eq. (12) and Eq. (13), we derive the lower bound of Pr(g \u201aÄ≤ |g) which is solely relied on T (g), constant factors œÉ and \u2026õ0, as shown in Eq (16). Pr(g \u201aÄ≤ |g) = |sup(g\u201aÄ≤ )| |sup(g)| \u201aâ\u2022 |G| \u201aàí œÉ|G| |G|\u201aàí|sup(T (g)|) 1\u201aàí\u2026õ0 = œÉ(1 \u201aàí \u2026õ0) œÉT (g) \u201aàí \u2026õ0 (16) Since Pr(g \u201aÄ≤ |g) is a probability definition, i.e., 0 \u201aâ§ Pr(¬∑) \u201aâ§ 1, we have the following restrictions for T (g) and T (g \u201aÄ≤ ) œÉT (g) \u201aâ\u2022 max{\u2026õ0, œÉ + (1 \u201aàí œÉ)\u2026õ0} (17) 944 Algorithm 3 SelectGraph (G, q) Input: A graph database G, a non-tree query graph q Output: The selected discriminative graph set D \u201aäÜ D(q) 1: D \u201aÜê \u201aàÖ; 2: C \u201aÜê {c1, c2, ¬∑ ¬∑ ¬∑ , cn}, ci \u201aäÜ q, ci is a simple cycle; 3: for all ci \u201aàà C do 4: g \u201aÜê g \u201aÄ≤ \u201aÜê ci; 5: while size(g \u201aÄ≤ ) \u201aâ§ maxL do 6: if g /\u201aàà \u201aàÜ then D \u201aÜê D \u201aà™ {g}; 7: g \u201aÄ≤ \u201aÜê g \u201aÄ≤ \u201aãÑ v; 8: if T (g), T (g \u201aÄ≤ ) satisfy Eq. (17), Eq. (18), Eq. (19) and (œÉT (g \u201aÄ≤ ) < œÉ \u201aàó √ó œÉT (g)) then 9: g \u201aÜê g \u201aÄ≤ ; 10: scan G to compute sup(g) for every g \u201aàà D and add an index entry for g in \u201aàÜ, if needed; 11: return D; and max{\u2026õ0, œÉ} \u201aâ§ œÉT (g \u201aÄ≤ ) \u201aâ§ œÉ + (1 \u201aàí œÉ)\u2026õ0 (œÉT (g) \u201aàí \u2026õ0)(œÉT (g \u201aÄ≤ ) \u201aàí \u2026õ0) \u201aâ\u2022 [œÉ(1 \u201aàí \u2026õ0)] 2 (18) (19) Our discovery is expressed in Eq. (15): the conditional occurrence probability of Pr(g \u201aÄ≤ |g), is solely upper-bounded by T (g \u201aÄ≤ ). Therefore, to select g \u201aÄ≤ from D(q) in the presence of g is equivalent to meet the qualification of œÉT (g \u201aÄ≤ ) which drops under a specific threshold related to g. In real applications, we can just test whether the inequality œÉT (g \u201aÄ≤ ) < œÉ \u201aàó √ó œÉT (g) satisfies or not. So the costly computation of Pr(g \u201aÄ≤ |g) is successfully translated to an approximate estimation toward T (g \u201aÄ≤ ). Note all frequent tree-features are discovered and indexed from G, so T (g) and T (g \u201aÄ≤ ) can be computed efficiently. The diagram below summarizes the whole estimation process. sup(g)(?) x ? \u2026õ(g)\u201aâ\u2022\u2026õ0 sup(Tg) ? \u201aàí\u201aàí\u201aàí\u201aàí\u201aÜí sup(g \u201aÄ≤ )(?) x ? ?\u2026õ(g \u201aÄ≤ )\u201aâ\u2022\u2026õ0 |sup(T (g))|\u201aâ\u2022œÉ|G| \u201aàí\u201aàí\u201aàí\u201aàí\u201aàí\u201aàí\u201aàí\u201aàí\u201aàí\u201aàí\u201aàí\u201aÜí |sup(T (g \u201aÄ≤ ))|\u201aâ\u2022œÉ|G| sup(Tg \u201aÄ≤) 4.2 Graph Selection Algorithm The graph selection algorithm is outlined in Algorithm 3. Let the input query be a non-tree graph q, the algorithm selects discriminative graphs from D(q) based on the selection criteria discussed in Section 4.1. The algorithm initiates from simple cycles of q (line 2), which can be selected from q efficiently. After a simple cycle c is selected, we extend c by growing one vertex (and all its corresponding edges) each time to get a larger-size graph g \u201aÄ≤ (denoted \u201aãÑ in line 7). If the conditions hold (line 8), g \u201aÄ≤ will be selected from D(q). Finally, in line 10, the algorithm compute sup(g) for every g \u201aàà D and add g as an indexing feature into \u201aàÜ. There are several implications in Algorithm 3. First, given a simple cycle, c \u201aäÜ q, all c\u201aÄôs subtrees are paths in nature. According to Theorem 3.1 and Theorem 3.2, the simple cycle c is usually discriminative w.r.t. its subpath feature set. Therefore, it is reasonable to consider all simple cycles of q as the starting point of our discriminative graph selection algorithm. Second, a maximum feature size, maxL, is set Figure 8: A non-tree query graph q and the discriminative graph-feature selection upon q such that only discriminative graph features within a specific size are selected. And the pattern-growth process for each simple cycle c continues at most (maxL\u201aàísize(c)) times (line 7). Third, if T (g \u201aÄ≤ ) is not frequent in the condition test (line 8), both g \u201aÄ≤ and all g \u201aÄ≤ \u201aÄôs subsequent supergraphs must be infrequent, i.e., the while loop can be early terminated. Fourth, when a discriminative graph g has already been selected and indexed into \u201aàÜ by previous queries, there is no need to select g for multiple times (line 6). Actually, all the indexed discriminative graph features can be shared by subsequent queries. Figure 8 (a) presents a non-tree query q submitted to the sample graph database G, shown in Figure 1. The minimum support threshold œÉ is set 2/3. The discriminative ratio \u2026õ0 is set 1/2 and œÉ \u201aàó = œÉ = 2/3. Following Algorithm 3, the simple cycle c in Figure 8 (b) is selected as the starting point for graph-feature selection. Based on Figure 7, c is discriminative w.r.t. all its frequent subtrees. Figure 8 (c) is generated by extending one vertex of q upon c, which is denoted as g. For c and g, œÉT (c) = 1, œÉT (g) = 2/3, which satisfies the constraints expressed in Eq. (17), Eq. (18) and Eq. (19). However, 2/3 √ó œÉT (c) = 2/3 = œÉT (g), i.e., it does not satisfy the constraint: œÉT (g) < œÉ \u201aàó √ó œÉT (c). So g is not selected from D(q) as a discriminative graph feature. Similarly, Figure 8 (d) is generated by extending one vertex of q upon g, which is denoted as g \u201aÄ≤ . Since T (g \u201aÄ≤ ) = 1/3 < œÉ, i.e., g \u201aÄ≤ is infrequent in G, which disobeys the constraint expressed in Eq. (18), g \u201aÄ≤ is not selected from D(q), either. 5. IMPLEMENTATIONS In this section, we give implementation details of our costeffective graph indexing algorithm, (Tree+\u201aàÜ). We present data structures and design principles of (Tree+\u201aàÜ) from the perspectives of index construction and query processing. In order to construct (Tree+\u201aàÜ), we mine frequent treefeatures from the graph database, G. There exist many reported studies on frequent structural pattern mining over large graph databases such asgSpan [22],Gaston [17],Hybrid -TreeMiner [5], etc. We proposed a fast frequent free tree mining algorithm [24] which takes full advantage of characteristics of tree structure. Once the frequent tree feature set FT is selected from G, every tree t \u201aàà FT is sequentialized and maintained in a hash table. Every t \u201aàà FT is associated with its support set sup(t), which contains the ids of graphs in G containing t as subgraph(s). With the aid of the hash table, every frequent tree feature and its support set can be located and retrieved quickly. For index maintenance, the similar strategies discussed in [23] can be adapted. The query processing of (Tree+\u201aàÜ) is outlined in Algorithm 4 with three parameters: a query graph q, a graph database G, and the index built on frequent tree-features of G. Below, we use FT to denote the index composed 945 Algorithm 4 Query Processing (q, FT, G) Input: A query graph q, the frequent tree-feature set FT, and the graph database G Output: Candidate answer set Cq 1: D \u201aÜê \u201aàÖ; 2: T (q) \u201aÜê {t | t \u201aäÜ q, t \u201aàà FT,size(t) \u201aâ§ maxL}; 3: Cq \u201aÜê T t\u201aààT (q) sup(t); 4: if (Cq ÔøΩ= \u201aàÖ) and (q is cyclic) then 5: D \u201aÜê SelectGraph(G, q); 6: for all (g \u201aàà T D) do sup(g); 7: Cq \u201aÜê Cq 8: return Cq; of frequent tree-features. While the selected discriminative graph-features are maintained in the additional index structure, \u201aàÜ, which is handled by SelectGraph (Algorithm 3). In Algorithm 4, (Tree+\u201aàÜ) enumerates all frequent subtrees of q up to the maximum feature size maxL which are located in the index FT (line 2). Based on the obtained frequent subtree feature set of q, T (q), the algorithm computes the candidate answer set, Cq, by intersecting the support set of t, for all t \u201aàà T (q) (line 3). If q is a non-tree cyclic graph, it calls SelectGraph to obtain a set of discriminative graphfeatures, D (line 5). Those discriminative graph-features may be cached in \u201aàÜ already. If not, SelectGraph will reclaim them from the graph database and maintain them in \u201aàÜ. Then the algorithm further reduces the candidate answer set Cq by intersecting the support set of g, for all g \u201aàà D (line 6-7). The pseudo-code in Algorithm 4 is far from optimized. At line 2, If a tree-feature fi \u201aäÜ q does not appear in FT, it implies that fi is infrequent, so there is no need to consider fj \u201aäÜ q if fi \u201aäÜ fj, due to the Apriori principle. At line 3 and line 7, the candidate answer set Cq is obtained by intersecting support sets of frequent trees and discriminative graphs of q. However, it is unnecessary to intersect every frequent feature derived from q. Given a series of frequent features f1, f2, ¬∑ ¬∑ ¬∑ , fn \u201aäÜ q, if f1 \u201aäÇ f2 T T T T \u201aäÇ ¬∑ ¬∑ T ¬∑ \u201aäÇ fn, then Cq sup(f1) sup(f2) ¬∑ ¬∑ ¬∑ sup(fn) = Cq sup(fn). So only the maximum frequent features are considered when computing the candidate answer set. More formally, Let Fm(q) be the set of maximum frequent features of q, i.e., Fm(q) = {f|f \u201aäÜ q, \u201aàÑf \u201aÄ≤ \u201aäÜ q, s.t., f \u201aäÇ f \u201aÄ≤ }. In order to compute Cq, we only need to perform intersection operations on the support sets of maximum frequent features in Fm(q), which substantially facilitates the whole graph containment query processing. For graph isomorphism testing in line 5, every discriminative graph g is encoded to a canonical code, cc(g). Two graphs g and g \u201aÄ≤ are isomorphic to each other, if and only if cc(g) = cc(g \u201aÄ≤ ). There exist several canonical codings for a general graph, such as CAM [11], DFS-code [22] etc. We use CAM as canonical code of discriminative graphs in our implementation. Another issue to be concerned is the selection and tuning of different parameters: the minimum support threshold, œÉ; the minimum discriminative ratio, \u2026õ0, and the discriminative graph selection threshold, œÉ \u201aàó . For œÉ, it has a close correlation with |F|, CFS and |Cq|. When œÉ is set small, the number of frequent tree-features discovered from G grows exponentially, which inevitably enhances the feature selection cost, Number of features 10000 9000 8000 7000 6000 5000 4000 3000 Tree+\u201aàÜ gIndex 2000 0 2000 4000 6000 8000 10000 Database size (a) Feature Size Index size (Kbyte) 25 20 15 10 5 Tree+\u201aàÜ gIndex C-Tree 0 0 2000 4000 6000 8000 10000 Database size (b) Index Size CFS. In the mean time, the feature space, F, is enlarged accordingly while |Cq| might be reduced because of more indexing features considered. So, œÉ should be determined on a deliberate balance between time and space. As to \u2026õ0 and œÉ \u201aàó , they both correlate with the filtering cost, Cf and |Cq|. Setting a loose bound for the discriminative graph selection (small \u2026õ0 and large œÉ \u201aàó ) results in more graph-features to be reclaimed from G, which increases Cf, whereas |Cq| probably decreases for more discriminative graph-features are indexed in \u201aàÜ. Since the number of discriminative graph-features held in \u201aàÜ is fairly small w.r.t. the tree-based index size, |FT |, the space overhead of \u201aàÜ can be negligible. A practical way to determine \u2026õ0 and œÉ \u201aàó is to sample a small portion of G and select discriminative graphs w.r.t their subtrees which well reflect the characteristics of the whole graph database, and \u2026õ0 and œÉ \u201aàó are tuned accordingly. "},{"aspect":"expcomparison","tweet":" 7.1 AIDS Antiviral Screen Dataset The experiments described in this section use the antiviral screen dataset from the Developmental Theroapeutics Program in NCI/NIH 1 . This 2D structure dataset contains 42390 compounds retrieved from DTP\u201aÄôs Drug Information System. There are total 63 kinds of atoms in this dataset, most of which are C, H, O, S, etc. Three kinds of bonds are popular in these compounds: single-bond, double-bond and aromatic-bond. We take atom types as vertex labels and omit edge labels because C-Tree does not support edgelabeled graphs. On average, compounds in the dataset has 43 vertices and 45 edges. The graph of maximum size has 221 vertices and 234 edges. We set the following parameters in (Tree+\u201aàÜ), gIndex and C-Tree for our experimental studies. In (Tree+\u201aàÜ) and gIndex, the maximum feature size maxL is set 10. For (Tree+\u201aàÜ), the minimum discriminative ratio Œµ0 is set 0.1; the minimum support threshold œÉ is set 0.1, and œÉ \u201aàó is set 0.8 for discriminative graph selection during query processing. InC-Tree, we set the minimum number of child vertices m = 20 and the maximum number M = 2m \u201aàí 1. We use the NBM method to compute graph closures. All these ex- 1 http://dtp.nci.nih.gov/docs/aids/aids data.html 25 0 5 10 15 Query size 20 25 30 (c) N=4000 Figure 10: False Positive Ratio Filtering Time (Secs) 200 150 100 50 0 5 Tree+\u201aàÜ gIndex C-Tree 10 15 Query Size 20 (c) N=4000 Figure 11: Filtering Cost 947 25 |C(q)| / |sup(q)| Filtering Time (Secs) 7 6 5 4 3 2 1 Tree+\u201aàÜ gIndex C-Tree Tree 0 5 10 15 Query size 20 25 30 400 350 300 250 200 150 100 50 0 (d) N=8000 5 Tree+\u201aàÜ gIndex C-Tree 10 15 Query Size 20 (d) N=8000 25 |C(q)| / |sup(q)| Filtering Time (Secs) 8 7 6 5 4 3 2 1 Tree+\u201aàÜ gIndex C-Tree Tree 0 5 10 15 Query size 20 25 30 600 500 400 300 200 100 0 (e) N=10000 5 Tree+\u201aàÜ gIndex C-Tree 10 15 Query Size 20 (e) N=10000 perimental settings are identical to author-specified values in [9, 23]. The first test is on index size |F| and index construction cost CFS of three different indexing algorithms: (Tree+\u201aàÜ), gIndex and C-Tree. The test dataset consists of N graphs, which is randomly selected from the antivirus screen database. Figure 9 (a) depicts the number of frequent features indexed in (Tree+\u201aàÜ) and gIndex with the test dataset size N varied from 1, 000 to 10, 000 (C-Tree does not provide the explicit number of indexing features, so we omit C-Tree in this experiment). The curves clearly show that frequent features of (Tree+\u201aàÜ) and gIndex are comparable in quantity. Figure 9 (b) illustrates the index size, i.e. |F| of three different indexing algorithms. Although (Tree+\u201aàÜ) has more indexing features than gIndex, the memory consumption for storing trees are much less than that for storing graphs held in gIndex and C-Tree. The curves illustrate that (Tree+\u201aàÜ) has a compact index structure which can easily be held in main memory. In Figure 9(c), we test the index construction time, i.e., CFS, measured in seconds. Both (Tree+\u201aàÜ) and C-Tree outperforms gIndex by an order of magnitude, while (Tree+\u201aàÜ) and C-Tree have similar index construction costs. Finally, the index construction time is averaged for each frequent feature and shown in Figure 9(d), measured in milliseconds. As the figure illustrates, for each indexing feature, CFS of (Tree+\u201aàÜ) is much smaller than that of gIndex and C-Tree. Having verified the index size |F| and index construction cost CFS of gIndex, C-Tree and (Tree+\u201aàÜ), we now evaluate their query performances. Given a query graph q, the query cost is characterized by the candidate answer set size, |Cq|. Since |sup(q)| is the tight lower bound of |Cq|, an algorithm achieving this lower bound can match the queries in the graph database precisely. We denote |C(q)|/|sup(q)|, the false positive ratio, as a measure for the pruning ability of different graph indexing algorithms. The smaller the false positive ratio, the better pruning ability an indexing algorithm has. We select our dataset by randomly sampling graphs from the antivirus screen database with size varying from 1, 000 to 10, 000. Five query sets are tested, each of which has 1, 000 queries. The query sets are generated from the antivirus screen database by randomly selecting 1, 000 graphs and then extracting connected subgraphs (subtrees, subpaths) from them. We remove vertices with the smallest degree 25 Verification Time (Secs) 200 150 100 50 0 5 Tree+\u201aàÜ gIndex C-Tree 10 15 Query Size 20 (a) N=1000 25 Verification Time (Secs) 400 350 300 250 200 150 100 50 0 5 Tree+\u201aàÜ gIndex C-Tree 10 15 Query Size 20 (b) N=2000 from a graph and this process proceeds until we get a query with a specified size. If all vertices of a graph have the same degree, one of them are removed to break the tie and the vertex-deletion procedure continues. This query generation approach secures that query graphs are connected and diverse enough to include different kind of graph structures(path, nonlinear tree and cyclic graph). We generate Q5, Q10, Q15, Q20 and Q25, with the digits denoting the query size. Figure 10 presents the false positive ratios of three graph indexing algorithms on different databases. Additionally, we use only frequent trees of G, i.e., FT as indexing features and denote this indexing algorithms as Tree. Tree is similar to (Tree+\u201aàÜ) but does not find discriminative graphs back from G during query processing. As shown in the figure, In all experimental settings and for every graph query set, (Tree+\u201aàÜ) outperforms gIndex, C-Tree and Tree. When the query size is small, the filtering power of four algorithms is close to each other and when the graph database becomes large and the query size increases, the advantages of (Tree+\u201aàÜ) become more apparent, i.e., (Tree+\u201aàÜ) performs especially well on large graph databases when query graphs are large and diverse. Interestingly, the gap between false positive ratios of gIndex and Tree is not large. It means that the pruning power of tree-feature is close to that of graph-feature, but tree is still less powerful than graph for pruning purpose. This evidence well justifies our analysis on pruning ability of different frequent features in F, as mentioned in Section 3. Meanwhile, it also proves that discriminative graphs play a key role in false positive pruning, and there is a good reason for us to find them back from the graph database G during query processing. Figure 11 illustrates the filtering cost Cf of three different graph indexing algorithms, measured in seconds. As shown in the figure, in all different databases, Cf of C-Tree is much larger than (Tree+\u201aàÜ) and gIndex. Meanwhile, Cf of (Tree+\u201aàÜ) is larger than gIndex because when queries become large and complex, (Tree+\u201aàÜ) has to select discriminative graphs and reclaim them from the graph database, if the discriminative graph-features are not held in \u201aàÜ. When query is fairly small (Q5), there is little chance for a query graph to contain cycles, so Cf of (Tree+\u201aàÜ) is smaller than that of gIndex. It also demonstrates that (Tree+\u201aàÜ) is quite efficient to answer acyclic graph queries. Figure 12 presents the verification cost of three graph indexing algorithms, i.e., the factor |Cq|√óCv of the query cost model in Section 2.3. As illustrated in the figure, (Tree+\u201aàÜ) needs less time to remove false positives from the candidate answer set because |Cq| is smaller than that obtained by gIndex and C-Tree. Based on Figure 11 and Figure 12, we are confirmed that (Tree+\u201aàÜ) outperforms gIndex and C-Tree because our final query cost (Cf + |Cq| \u201aàó Cv) is minimum, in comparison with gIndex and C-Tree, and our (Tree+\u201aàÜ) shows a good scalability w.r.t. the database size. 25 Verification Time (Secs) 1400 1200 1000 800 600 400 200 0 5 Tree+\u201aàÜ gIndex C-Tree 10 15 Query Size 20 25 Verification Time (Secs) 1800 1600 1400 1200 1000 800 600 400 200 0 5 Tree+\u201aàÜ gIndex C-Tree 10 15 Query Size 20 25 Verification Time (Secs) 2000 1500 1000 500 0 5 Tree+\u201aàÜ gIndex C-Tree 10 15 Query Size 20 (c) N=4000 (d) N=8000 (e) N=10000 Figure 12: Verification Time 7.2 Synthetic Dataset In this section, we conduct our performance studies on a synthetic database. This database is generated by the widely-used graph generator [12] described as follows: first a set of S seed fragments are generated randomly, whose size is determined by a Poisson distribution with mean I. The size of each graph is a Poisson random variable with mean T. Seed fragment are then randomly selected and inserted into a graph one by one until the graph reaches its size. A typical database may have the following experimental settings: it has 10, 000 graphs and 1, 00 seed fragments with 5 distinct labels. On average, each graph has 50 edges and each seed fragment has 10 edges. We denote this dataset with the above settings as D10kI10T50S100L5. 948 |C(q)| / |SUP(q)| 2 1.8 1.6 1.4 1.2 1 Tree+\u201aàÜ gIndex C-Tree 0.8 0 5 10 15 Query size 20 25 30 (a) Varying Query Size |C(q)| / |SUP(q)| 1.8 1.7 1.6 1.5 1.4 1.3 1.2 1.1 1 Tree+\u201aàÜ gIndex C-Tree 0.9 3 4 5 6 7 8 9 10 11 Number of Labels (b) Varying #Labels Figure 13: False Positive Ratio We test the false positive ratio on the synthetic database mentioned above with 5 query sets Q5, Q10, Q15, Q20, Q25, which are generated using the same method described in Section 7.1. As shown in Figure 13(a), the filtering power presented by (Tree+\u201aàÜ) is still better than that provided by gIndex and C-Tree. We then test the influence of vertex labels on the query performance of different indexing algorithms. When the number of distinct labels is large, the synthetic dataset is much different from the real dataset. Although local structural similarity appears in different graphs, there is little similarity existing among each graph. This characteristic results in a simpler index structure. For example, if every vertex in one graph has a unique label, we only use vertex labels as index features. This is similar to the inverted index technique in information retrieval. In order to verify this conclusion, we vary the number of labels from 5 to 10 in the synthetic dataset D10kI10T50S100 and test the false positive ratios of three algorithms using the query set Q10. Figure 13(b) shows that they are close with each other when L is growing large. "}]}