{"user_name":" Bidirectional Expansion For Keyword Search on Graph Databases ","user_timeline":[{"aspect":"abstract","tweet":" Abstract Relational, XML and HTML data can be represented as graphs with entities as nodes and relationships as edges. Text is associated with nodes and possibly edges. Keyword search on such graphs has received much attention lately. A central problem in this scenario is to efficiently extract from the data graph a small number of the \u201aÄúbest\u201aÄù answer trees. A Backward Expanding search, starting at nodes matching keywords and working up toward confluent roots, is commonly used for predominantly text-driven queries. But it can perform poorly if some keywords match many nodes, or some node has very large degree. In this paper we propose a new search algorithm, Bidirectional Search, which improves on Backward Expanding search by allowing forward search from potential roots towards leaves. To exploit this flexibility, we devise a novel search frontier prioritization technique based on spreading activation. We present a performance study on real data, establishing that Bidirectional Search significantly outperforms Backward Expanding search. "},{"aspect":"expanalysis","tweet":" 7 Conclusions and Future Work Keyword search on textual data graphs is an emerging application that unifies Web, text, XML, and RDBMS systems and has important applications in heterogeneous data integration and management. We pointed out limitations of earlier keyword search techniques on graphs. We introduced the Bidirectional search algorithm and the novel frontier prioritization technique (based on spreading activation) that guides it. We presented extensive experiments that clearly demonstrate the benefits of Bidirectional search over earlier algorithms. We have extended our answer ranking model as well as the Bidirectional search algorithm to handle partial specification of schema and structure using tree 516 patterns with approximate matching. Implementing these extensions is part of future work. Other areas of future work include (a) improved \u201aÄúlookahead\u201aÄù techniques for Bidirectional search which can reduce the number of nodes touched, (b) more sophisticated activation spreading techniques that will help reduce the upper bounds on answers not yet computed, allowing faster output of already computed answers, (c) using schema information to reduce search priority on paths that are unlikely to give good answers, and (d) a performance study of alternative activation spreading and prioritization techniques on different data sources, such as Web data with hyperlinks. Acknowledgments: We would like to thank Arvind Hulgeri and Govind Kabra for their help with the code, and the referees for their feedback. "},{"aspect":"expdata","tweet":""},{"aspect":"background","tweet":" 1 Introduction Keyword search over graph-structured textual data has attracted quite some interest in the database community lately. Graphs where nodes (and possibly edges) have associated text are a convenient \u201aÄúcommon denominator\u201aÄù representation for relational data \u201aàó Work done while at IIT Bombay. Current affiliations of first two and last two authors are U. C. Berkeley, C.M.U., Microsoft and Oracle, Bangalore, respectively Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. Proceedings of the 31st VLDB Conference, Trondheim, Norway, 2005 505 (where nodes are tuples and links are induced by foreign keys), semistructured XML data (where nodes are elements and links represent element containment, keyrefs, and IDREFs), and Web data (where nodes can be whole pages or DOM elements and links can represent HREFs or DOM element containment). This common representation enables novel systems for heterogeneous data integration and search. Systems for \u201aÄúschema-agnostic\u201aÄù keyword search on databases, such as DBXplorer [1], BANKS [3] and Discover [9], model a response as a tree connecting nodes (tuples) that contain the different keywords in a query (or more generally, nodes that satisfy specified conditions). Here \u201aÄúschema-agnostic\u201aÄù means that the queries need not use any schema information (although the evaluation system can exploit schema information). For example, the query \u201aÄúGray transaction\u201aÄù on a graph derived from DBLP may find Gray matching an author node, transaction matching a paper node, and an answer would be the connecting path; with more than two keywords, the answer would be a connecting tree. The tree model has also been used to find connected Web pages, that together contain the keywords in a query [10]. As in Web search, graph search systems must define a measure of relevance or merit for each response. Responses must be presented in relevance order. Ideally, the query processor must efficiently generate only a few responses that have the greatest relevance scores. A variety of notions of response, relevance measures and search algorithms have been proposed for graph search. DBXplorer and Discover use the number of edges in a tree as a measure of its quality, preferring trees with fewer edges. However, this measure is coarse grained: for example, it cannot favor a highly cited paper on transactions by Gray over a less-known paper. Both DBXplorer and Discover can be extended to associate weights with edges based only on the database schema, but do not consider node weights or edge weights that are not determined by the schema alone, which are required to fully exploit the generality of graph search. XRank [7] assigns a specific Pagerank-like [4] prestige to individual nodes, but depends on treestructured XML for efficient query execution. There are many scenarios where data forms arbitrary graphs, and cannot be meaningfully modeled by tree structures or even DAG structures; the XRank index structure (and other common XML index structures) cannot be used in such scenarios. ObjectRank [2] also assigns a Pagerank-like [4] score to each node, but does not score answer subgraphs as response units. In contrast, the Backward expanding strategy used in BANKS [3] can deal with the general model. In brief, it does a best-first search from each node matching a keyword; whenever it finds a node that has been reached from each keyword, it outputs an answer tree. However, Backward expanding search may perform poorly w.r.t. both time and space in case a query keyword matches a very large number of nodes (e.g. if it matches a \u201aÄúmetadata node\u201aÄù such as a table or column name in the original relational data), or if it encounters a node with a very large fan-in (e.g. the \u201aÄúpaper appeared in conference\u201aÄù relation in DBLP leads to \u201aÄúconference\u201aÄù nodes with large degree). Our Contributions: We introduce a new search algorithm, which we call Bidirectional Search, for schema-agnostic text search on graphs. Note that the graph on which search occurs may be explicitly represented (as in BANKS) or may be implicitly present in the database, as in DBXplorer and Discover. Our search algorithm can be used in either case. Unlike backward expanding strategies, which can only explore paths backward from keyword nodes toward the roots of answer trees, the Bidirectional algorithm can explore paths forward from nodes that are potential roots of answer trees, toward other keyword nodes. For example, if transaction matches a large number of nodes, while Gray matches fewer nodes, it may be profitable to search backward from nodes matching Gray toward potential answer roots, and then forward to find nodes that match transaction. This is equivalent to a join order (with indexed nested loops join) which favors starting from the relation with fewer tuples and probing the one with more tuples. In information retrieval, it is standard to intersect inverted lists starting with the smallest one [15]. This can be regarded as a special case of our algorithm. Spreading activation: A key innovation needed to contain and exploit the flexibility of simultaneous backward and forward expansion is a novel frontier prioritization scheme based on spreading activation (roughly speaking, Pagerank with decay). The prioritization technique allows preferential expansion of paths that have less branching; our experiments show that this can provide large benefits in many cases. Our prioritization mechanism can be extended to implement other useful features. For example, we can enforce constraints using edge types to restrict search to specified search paths, or to prioritize certain paths 506 over others. If we visualize answer tree generation as computing a join, the Bidirectional search algorithm, in effect, chooses a join order dynamically and incrementally on a per-tuple basis. In contrast, the Backward expanding search algorithm can be visualized as having a fixed join order. Experiments: We have implemented the Bidirectional search algorithm as part of the BANKS system, and present a performance study comparing it with the Backward expanding search [3]. Our prototype, tested on DBLP 1 , IMDB 2 , and a subset of US Patents 3 , shows that Bidirectional search can execute at interactive speeds over large graphs with over 20 million nodes and edges, on an ordinary desktop PC. Our study shows that Bidirectional Search outperforms Backward Expanding Search by a large margin across a variety of scenarios. The rest of this paper is organized as follows. Section 2 outlines our data, query and response models. Section 3 outlines the Backward expanding search algorithm presented in [3]. Section 4 describes our Bidirectional search algorithm. Section 5 describes our performance study. Section 6 describes related work. We make concluding remarks in Section 7. "},{"aspect":"expintro","tweet":" In this section we study the quality of answers and the execution time of Bidirectional search with respect to two versions of Backward expanding search, and the Sparse algorithm from [8]. Differences from other keyword search algorithms, which make them incomparable, are explained in Section 6. We used a single processor 2.4 GHz P4 HT machine with 1GB of RAM, running Linux, for our experiments. We have experimented with three datasets - the complete DBLP database, IMDB (the Internet Movie Database), and a subset of the US Patent database. The complete DBLP database has about 300,000 authors and 500,000 papers resulting in about 2 million nodes and 9 million edges. IMDB database has a similar size while the US Patent databases is even larger and has 4 million nodes and 15 million edges. "},{"aspect":"problemdef","tweet":""},{"aspect":"solution","tweet":" 2 Data, Query and Responses In this section, we briefly outline the graph model of data, and the answer tree model that our graph search algorithms focus on. 2.1 Graph Data Model We model the database as a weighted directed graph in which nodes are entities and edges are relationships. A node may represent a tuple or row in a database, or an XML element. An edge may represent a foreign key/primary key relationship, element containment, or IDREF links in XML. For example, in the graph model used in several systems (such as DBXplorer, BANKS, Discover and ObjectRank) for each row r in a database that we need to represent, the data graph has a corresponding node ur. We will speak interchangeably of a tuple and the corresponding node in the graph. For each pair of tuples r1 and r2 such that there is a foreign key from r1 to r2, the graph contains an edge from ur1 to ur2. In DBXplorer and Discover, the edges are undirected. Directionality is natural in many applications: the strength of connections between two nodes is not necessarily symmetric. In BANKS [3], edge directionality was introduced avoid meaningless short paths through \u201aÄúhubs\u201aÄù. Consider the data graph of DBLP, which has a (metadata) node called conference, connected to a node for each conference, which are then connected 1 http://www.informatik.uni-trier.de/~ley/db/ 2 http://www.imdb.com/ 3 http://www.uspto.gov/main/patents.htm to papers published in those conferences. The path through the conference node is a relatively meaningless (compared to an authored-by edge from paper to author, say) \u201aÄúshortcut\u201aÄù path which can make papers look more similar than they are. To deal with this, edges are treated as directed; the paper has a reference to the conference node, so a directed edge is created from the paper to the conference node. On the other hand, to admit interesting response graphs, the model must allow paths to traverse edges \u201aÄúbackwards\u201aÄù. For example, if paper u co-cites v and w, there is no directed path between v and w, but we often wish to report such subgraphs. To handle this situation, given an edge u \u201aÜí v in the original database graph, BANKS creates a \u201aÄúbackward edge\u201aÄù v \u201aÜí u, with a weight dependent on the number of edges incident on v. Backward edges from \u201aÄúhubs\u201aÄù with many incident edges would have a high weight, thereby resulting in a low relevance score for meaningless shortcuts through such hubs. In the conference example, given that many papers refer to a particular conference, and many conferences refer to the conference node, the backward edges from these nodes would have large weight, and thus a low relevance. 2.2 Query and Response Models In its simplest form, a query is a set of keywords. Let the query consist of n terms t1, t2, . . . , tn. For each search term ti in the query, let Si be the set of nodes that match the search term ti. A node matches a term if the corresponding tuple contains the term; if a term matches a relation name, all tuples in the relation are assumed to match the term. In the directed graph model of [3], a response or answer to a keyword query is a minimal rooted directed tree, embedded in the data graph, and containing at least one node from each Si. In undirected graph models such as DBXplorer and Discover, the answer tree is not directed. Intuitively, the paths connecting the keyword nodes (i.e., nodes with keyword(s)) explain how the keywords are related in the database. 2.3 Response Ranking In addition to edge weights, the ranking of an answer may also depend on a notion of node prestige. As with Pagerank, not all nodes are equal in status; for example, users expect the query recovery on DBLP to rank first the most popular papers about recovery, as judged by their link neighborhood, including citations. A method of computing node prestige based on indegree is defined in [3], while [2] defines global and per-keyword node prestige scores for each node. Node prestige scores can be assumed to be precomputed for our purpose, although they could potentially be computed on-the-fly. We do not address the issue of how to compute node prestige here. The overall score of an answer must then be defined by (a) a specification of the overall edge-score of 507 the tree based on individual edge weights, and (b) a specification of the overall node-prestige-score of the tree, obtained by combining individual node prestige scores, and finally, (c) a specification for combining the tree edge-score with the node-prestige-score to get the overall score for the answer tree. The focus of this paper is on the search algorithms, rather than on the ranking technique. For concreteness, we outline the specification used in our BANKS code; see [3] for details. \u201aÄ¢ The weights of forward edges (in the direction of foreign keys, etc.) are defined by the schema, and default to 1. \u201aÄ¢ If the graph had a forward edge u \u201aÜí v with weight wuv, we create a backward edge v \u201aÜí u with weight wvu = wuv log2 (1+indegree(v)). This discourages spurious shortcuts. \u201aÄ¢ We define a score s(T, ti) for an answer tree T with respect to keyword ti as the sum of the edge weights on the path from the root of T to the leaf containing keyword ti. \u201aÄ¢ We define the ÔøΩ aggregate edge-score E of an answer 4 tree T as i s(T, ti). \u201aÄ¢ The prestige of each node is determined using a biased version of the Pagerank [4] random walk, similar to the computation of global ObjectRank [2], except that, in our case, the probability of following an edge is inversely proportional to its edge weight taken from the data graph instead of the schema graph. \u201aÄ¢ We define the tree node prestige N as the sum of the node prestiges of the leaf nodes and the answer root. \u201aÄ¢ We define the overall tree score as EN Œª where Œª helps adjust the importance of edge and node scores. As a default, we use Œª = 0.2, a choice found to work well [3]. We have chosen a response-ranking specification guided by recent literature, but clearly, other definitions of answer tree scores are possible. Comparison of alternatives must involve large-scale user studies, which is scanty in current literature and would be valuable as future work. Our search algorithm works on arbitrary directed weighted graphs, and is not affected by how the edges and edge weights are defined, or on the exact technique for computing answer scores. However, our prioritization functions do need to take these into account, as do functions that compute upper bounds on the score of future answers (so that answers with a higher score can be output without waiting further). We address this issue later, in Sections 4.3 and 4.5. 4 This step differs from the formula in [3], which adds up the scores of all edges in a tree. The current version is simpler to deal with and we found it gives results of equivalent quality. 3 Backward Expanding Search Before we present our Bidirectional search algorithm, we present the Backward expanding search algorithm of [3] (which we sometimes call Backward search, for brevity). Backward expanding search is based on Dijkstra\u201aÄôs single source shortest path algorithm. Given a set of keywords, it first finds, for each keyword term ti, the set of nodes Si that match ti; the nodes in Si are called keyword nodes. To facilitate this a single index is built on values from selected string-valued attributes from multiple tables. The index maps from keywords to (table-name, tuple-id) pairs. Let the query be given by a set of keywords K = (t1, t2, ...tn) and set Si denote the set of nodes matching keyword ti. Thus S = S1 \u201aà™ S2 \u201aà™ . . . \u201aà™ Sn is the set of relevant nodes for the query. The Backward search creates |S| iterators; each iterator executes a the single source shortest path algorithm starting with one of the nodes in S as source. When a getnext() function is called on an iterator, it restarts from the state saved on the last getnext() call, and runs a step of the Dijkstra algorithm; that is, it finds the minimum distance node m on the frontier, expands the frontier to include all nodes connected to m, saves the fronter in its state, and returns m. Unlike the normal shortest path algorithm, each iterator traverses the graph edges in \u201aÄúreverse direction\u201aÄù, using edges pointing toward the node being expanded rather that edges pointing away from it. The idea is to find a common vertex from which a forward path exists to at least one node in each set Si. Such paths will define a rooted directed tree with the common vertex as the root and containing all the keyword nodes 5 . The tree thus formed is an answer tree. Since the iterators traverse edges in the \u201aÄúbackward\u201aÄù direction, the algorithm is called the Backward expanding search algorithm. At each iteration of the algorithm, one of the iterators is picked for further expansion. The iterator picked is the one whose next vertex to be visited has the shortest path to the source vertex of the iterator (the distance measure can be extended to include node weights of the nodes matching keywords). A list of all the vertices visited is maintained for each iterator. Consider a set of iterators containing one iterator from each set Si. If the intersection of their visited vertex lists is non-empty, then each vertex in the intersection defines a tree rooted at the vertex, with a path to at least one node from each set Si. A resulting tree is an answer tree only if the root of the tree has more than one child. If the root of a tree T has only one child, and all the keywords are present in the non-root nodes, then the tree formed by removing the root node is also present in the result set and has a higher relevance score. Such a non-minimal tree T is 5 Each leaf node must contain at least one keyword and the non-leaf nodes may contain keywords. 508 therefore discarded. 4 Bidirectional Expanding Search In this section we motivate and describe our new algorithm for generating answer responses to keyword queries on graphs, which we call Bidirectional expanding search (or Bidirectional search, for brevity). Note that the problem of finding answer trees is NPhard, since the well known Steiner tree problem for undirected graphs can be easily reduced to the problem of finding answer trees [3]. In fact, even polynomial (approximation) algorithms that require an examination of the entire graph would not be desirable for finding answer trees, since the overall graph may be very large, whereas answer trees can potentially be found by examining a small part of the graph. The goal of Backward expanding search as well as of our Bidirectional search is to generate answers while examining as small a fraction of the graph as possible. 4.1 Motivation for Bidirectional Search The Backward search algorithm would explore an unnecessarily large number of graph nodes in the following scenarios: \u201aÄ¢ The query contains a frequently occurring term: In the Backward algorithm one iterator is associated with every keyword node. The algorithm would generate a large number of iterators if a keyword matches a large number of nodes. This could happen in case of frequently occurring terms (e.g.. database in the DBLP database, John in the IMDB database) or if the keyword matches a relation name (which selects all tuples belonging to the relation). \u201aÄ¢ An iterator reaches a node with large fan-in: An iterator may need to explore a large number of nodes if it hits a node with a very large fan-in (e.g.. a department node in a university database which has a large number of student nodes connected to it). We call the nodes that are already hit by an iterator but whose neighbors are yet to be explored by the iterator as the fringe nodes and the set of fringe nodes of an iterator as the iterator frontier. A node with a large fan-in results in a large increase in the frontier size. In the above scenarios, the Backward search algorithm may explore a large portion of the graph before finding the relevant answers and this may result in a long search time; we give some empirical evidence of this in the experimental evaluation section (Section 5). Our first approach to the problem was to create iterators only for keywords not \u201aÄúfrequently occurring\u201aÄù, and additionally explore forward paths from potential roots to the \u201aÄúfrequent\u201aÄù keywords (see Figure 1). Every node reached by an iterator in the Backward search Backward search Keyword 1 Keyword 2 How about searching in forward direction? Backward search hurts performance as large number of nodes match keyword2 Figure 1: Need for forward search algorithm is a potential root. If we follow forward paths from them, we may hit the frequent keyword nodes faster and hence find answers quickly. However, it is hard to decide on cutoffs that define which keywords are \u201aÄúfrequent.\u201aÄù Moreover, if the iterator reaches a node v with a large indegree, it would still explore a large number of nodes that have an edge to v. Keyword search engines always intersect inverted lists starting with the rarest word [15] and some other pruning strategies based on word rareness are also used. While inverted lists are \u201aÄúflat\u201aÄù sets of document IDs, our situation is more complex because we can transitively expand neighbors to neighbors of neighbors, etc. The two key ideas behind our Bidirectional expanding search algorithm are: \u201aÄ¢ Starting forward searches from potential roots \u201aÄ¢ A spreading activation model to prioritize nodes on the fringe, whereby nodes on an iterator with a small fringe would get a higher priority, and among nodes within a single iterator, those in less bushy subtrees would get a higher priority. Prioritization additionally takes answer relevance into account, and is described in Section 4.3, after describing the search algorithm in Section 4.2. Discouraging backward search from large fringes avoids potentially wasteful expansion, yet we are able to connect up to their corresponding keyword nodes by means of forward search from potential roots with higher activation. Our performance results (Section 5) demonstrate the benefits of this approach. 4.2 The Bidirectional Search Algorithm Let the query be given by a set of keywords K = (t1, t2, ...tn) and set Si denote the set of nodes matching keyword ti. The Bidirectional search algorithm attempts to find best rooted directed trees connecting at least one node from each Si. Before we present the Bidirectional search algorithm, we outline key differences from the Backward search algorithm: \u201aÄ¢ We merge all the single source shortest path iterators from the Backward search algorithm into a single iterator and call it the incoming iterator. 509 K Set of Keywords t1, t2, ... tn Si S Set of nodes matching keyword ti ÔøΩ i Si Qin A priority queue of nodes in backward expanding fringe Qout A priority queue of nodes in forward expanding fringe Xin Set of nodes expanded for incoming paths Xout Set of nodes expanded for outgoing paths Pv Set of nodes u such that edge (u, v) has been explored spu,i Node to follow from u for best path to ti distu,i Length of best known path from u to a node in Si au,i Activation at node u from keyword ti au Overall activation of node u depthu depth of node u from keyword nodes Figure 2: Notation used in Bidirectional Algorithm \u201aÄ¢ We use spreading activation (Section 4.3) to prioritize the search. For the incoming iterator, the next node to be expanded is the one with the highest activation. Activation is a kind of \u201aÄúscent\u201aÄù spread from keyword nodes, and edge weights are taken into account when spreading the activation, so the activation score reflects the edge weight as well as the spreading of the search fringe. \u201aÄ¢ We concurrently run another iterator which we call the outgoing iterator. This iterator follows forward edges starting from all the nodes explored by the incoming iterator. Figure 2 shows the data structures used by the algorithm. Each iterator has two queues, one for the set of nodes to be expanded further and one for the set of already expanded nodes. For the incoming iterator, these are Qin and Xin respectively. For the outgoing iterator, these are Qout and Xout respectively. For every node u explored so far, either in outgoing or in incoming search, we keep track of the best known path from u to any node in Si. Specifically, for every keyword term ti we maintain the child node spu,i that u should follow to reach a node in Si in the best known path. We also maintain distu,i, the length of the best known path from u to a node in Si, and the activation au,i that ti spreads to u (explained later). The term reached-ancestor of a node u refers to ancestors (i.e. nodes that have a path to u) that have been reached earlier (i.e. are in one of Qin, Qout, Xin or Xout). The pseudo-code for the algorithm is shown in Figure 3. The two key data structures used by the algorithm are the incoming iterator Qin and the outgoing iterator Qout; we describe the intuition behind the two iterators below. At each step of the algorithm, among the incoming and outgoing iterators, the one having the node with highest priority is scheduled for exploration. Ex- Bidir-Exp-Search() 1 Qin \u201aÜê S; Qout \u201aÜê œÜ;Xin = œÜ; Xout \u201aÜê œÜ; 2 \u201aàÄu \u201aàà S : Pu \u201aÜê œÜ,depthu = 0; \u201aàÄi, \u201aàÄu \u201aàà S : spu,i \u201aÜê \u201aàû; 3 \u201aàÄi, \u201aàÄu \u201aàà S : if u \u201aàà Si, distu,i \u201aÜê 0 else distu,i \u201aÜê \u201aàû 4 while Qin or Qout are non-empty 5 switch 6 case Qin has node with highest activation : 7 Pop best v from Qin and insert in Xin 8 if is-Complete(v) then EMIT(v) 9 if depthv  dmax then 10 \u201aàÄu \u201aàà incoming[v] 11 ExploreEdge(u, v) 12 if u /\u201aàà Xin insert it into Qin 13 with depth depthv + 1 14 if v /\u201aàà Xout insert it into Qout 15 case Qout has node with highest activation : 16 Pop best u from Qout and 17 insert in Xout 18 if is-Complete(u) then EMIT(u) 19 if depthu  dmax then 20 \u201aàÄv \u201aàà outgoing[u] 21 ExploreEdge(u, v) 22 if v /\u201aàà Xout insert it into Qout 23 with depth depth(u) + 1 ExploreEdge(u, v) 1 for each keyword i 2 if u has a better path to ti via v then 3 spu,i \u201aÜê v; update distu,i with this new dist 4 Attach(u, i) 5 if is-Complete(u) then EMIT(u) 6 if v spreads more activation to u from ti then 7 update au,i with this new activation 8 Activate(u, i) Attach(v, k) 1 update priority of v if it is present in Qin 2 Propagate change in cost distvk 3 to all its reached-ancestors in best first manner Activate(v, k) 1 update priority of v if it is present in Qin 2 Propagate change in activation avk 3 to all its reached-ancestors in best first manner Emit(u) 1 construct result tree rooted at u 2 and add it to result heap Is-Complete(u) 1 for i = 1 to N 2 if distu,i == \u201aàû return false; /* No path to ti*/ 3 return true; Figure 3: Bidirectional expanding search ploring a node v in Qin (resp. Qout) is done as follows: incoming (resp. outgoing) edges are traversed to propagate keyword-distance information from v to adjacent nodes, and the node moved from Qin to Xin (resp. Qout to Xout). Additionally, if the node is found to have been reached from all keywords it is \u201aÄúemitted\u201aÄù to an output queue. Answers are output from the output queue when the algorithm decides that no answers 510 with higher scores will be generated in future. The distance of a node u from the keyword nodes (i.e. the number of edges from the nearest keyword node, as determined when it is first inserted into Qin or Qout) is stored in depthu. A depth value cutoff dmax is used to prevent generation of answers that would be unintuitive due to excessive path lengths, and ensures termination; in our experiments, we used a generous default of dmax = 8. 4.2.1 Incoming Iterator Unlike Backward expanding search, which runs multiple iterators, one from each node matching a keyword, in Bidirectional search we run only one iterator to explore backward paths from the keyword nodes. Note that, unlike the Backward search algorithm, the iterator is not a shortest path iterator since we do not order the nodes to be expanded solely on the basis of the distance from the origin; the nodes are ordered by a prioritization mechanism described later. Each node is present only once in the single backward path iterator, and popped from it only once; in contrast, in Backward expanding search, each node may be present in multiple iterators, since there is an iterator for each node matching each keyword. The benefit of having a single iterator is that the amount of information to be maintained is sharply reduced. However, there are two issues to be addressed. It is possible for a node v to be popped after it is reached from one keyword node, say t1; when a node is popped, its incoming edges from other nodes u are explored. Later on we may find that the node is reached from another keyword node t2. At this stage, we have to traverse its incoming edges again to update distances to t2 from all nodes u that have an incoming edge to v; in fact, this has to be done recursively, to update distances to t2 from already reached ancestors of u. Procedure ATTACH(u, i) carries out this task (in our example, i = 2). In fact, since we prioritize using factors other than distance from the keyword node, it is possible that after finding one path from v to a keyword ti, we may later find a shorter path; the distance update propagation has to be done each time a shorter path is found. Although such repeated propagation could potentially increase execution time, our performance results show that the benefits of prioritization outweigh the costs. The second issue is that, in order to minimize space, we store only the shortest path spu, and distance distu,i from each node u to the closest node among those that match keyword ti. In contrast, Backward expanding search keeps shortest paths to each term that matches ti. This optimization reduces space and time cost, but at the potential cost of changing the answer set slightly, although in practice this effect appears to be negligible \u201aÄì we revisit this issue in Section 4.6. The nodes u in Qin are the nodes on the frontier of the incoming iterator, and Qin is ordered on the activation au of these nodes; the higher the activation of a node, the higher its scheduling priority. The Bidirec- tional search procedure starts by inserting all keyword nodes u \u201aàà S where S = ÔøΩ \u201aàÄi Si into the incoming iterator Qin with an initial activation. For each keyword node, this seed activation is spread to the nodes that are reached in backward direction starting from this keyword node. The exact formulae for calculating the seed activation and for propagating activation can be decided depending on the answer ranking technique, and we discuss them further in Section 4.3. 4.2.2 Outgoing iterator The outgoing iterator expands nodes in the forward direction from potential answer roots. Every node reached by the incoming iterator is treated as a potential answer root. For each root, the outgoing iterator maintains shortest forward paths to each keyword; some of these would have been found earlier by backward search on the incoming iterator, others may be found during forward search on the outgoing iterator. When we explore forward paths from node u, for every adjacent node v such that there is an edge from u to v, we check for each keyword term ti if there is a better path from u to ti through v. If it exists, we update spu,i, distu,i and au,i. A change in these values must be propagated to the ancestors of u. For this purpose, with every node u we maintain a list of its explored parent nodes Pu and update it every time an edge is explored. After exploring all edges, we check if v has been previously expanded in forward search (i.e. is in Qout or was in Qout; \u201aÄúwas\u201aÄù can be checked by seeing if it is in Xout); if not we insert it in Qout. As in the incoming iterator, nodes in Qout are on the frontier of the outgoing iterator and the queue is ordered on the total activation au of each node u. Activation is spread from each node u to all nodes v such that there is a forward edge from u to v. 4.2.3 Generating Results Each time we update the path lengths from a node to a keyword ti, we check if the node has paths to all the other keywords. In that case, we build the corresponding answer tree and insert it into the OutputHeap. The OutputHeap buffers and reorders answers since they may not be generated in relevance score order. In addition to the fact that some answers with a lower edge score may be generated after answers with a higher edge score, the node prestige of the root and leaf nodes also affects the relevance answer score. Results are output from the OutputHeap when we determine that no better result can be generated, as described in Section 4.5. It is also possible for the same tree to appear in more than one result, but with different roots. Such duplicates with lower score are discarded when they are inserted into the OutputHeap. 511 4.3 Activation Initialization and Spreading As mentioned earlier, the Bidirectional search algorithm can work with different ways of defining the initial activation of nodes as well as with different ways of spreading activation. For concreteness we specify here formulas that are tailored to the answer ranking model described in Section 2.3. The overall tree score depends on both an edge score and on the node prestige, and both need to be taken into account when defining activation to prioritize search. Nodes matching keywords, are added to the incoming iterator Qin with initial activation computed as: au,i = nodePrestige(u) , \u201aàÄu \u201aàà Si |Si| (1) where Si is the nest of nodes that match keyword ti. Thus, if the keyword node has high prestige, that node will have a higher priority for expansion. But if a keyword matches a large number of nodes, the nodes will have a lower priority. The activations from different keywords are computed separately to separate the priority contribution from each keyword. When we spread activation from a node, we use an attenuation factor ¬µ; each node v spreads a fraction ¬µ of the received activation to its neighbours, and retains the remaining 1 \u201aàí ¬µ fraction. As a default we set ¬µ = 0.5. The fraction ¬µ of the received activation is divided amongst the neighbors as described below. For the incoming iterator, the activation from keyword ti is spread to nodes uj such that there is an edge uj \u201aÜí v, Amongst these nodes, activation is divided in inverse proportion to the weight of the edge uj \u201aÜí v (respectively, v \u201aÜí uj). This ensures that the activation priority reflects the path length from uj to the keyword node; trees containing nodes that are farther away are likely to have a lower score. For the outgoing iterator, activation from keyword ti is spread to nodes uj such that there is an edge v \u201aÜí uj, again divided in inverse proportion to the edge weights v \u201aÜí uj. This ensures that nodes that are closer to the potential root get higher activation, since tree scores will be worse if they include nodes that are farther away. When a node u receives activation from a keyword ti from multiple edges, we define au,i as the maximum of the received activations. This reflects the fact that trees are scored by the shortest path from the root to each keyword. With scoring models that aggregate scores along multiple paths (as is done in [2]), we could use other ways of combining the activation, such as adding them up 6 . 6 This extension is implemented in the BANKS system and supports a form of queries which we call \u201aÄúnear queries\u201aÄù. For lack of space we cannot describe it further here, but the interested reader can try it out on the BANKS web site http://www.cse. iitb.ac.in/banks. 249 248 151 Paper 1 Database 100 250 101 Writes 150 Author James Writes 106 105 104 103 Author 102 John Figure 4: Bidirectional Search Example The overall of activation a node is then defined as the sum of its activations from each keyword. Specifically, for a node u the overall activation au is defined as: nÔøΩ au = i=0 au,i This reflects the fact that if a node is close to multiple keywords, it is worth prioritizing the node, since connections to fewer keywords are left to be found. 4.4 Bidirectional Search Example A sample graph and query is shown in Figure 4. The user wants to find out \u201aÄúDatabase\u201aÄù papers co-authored by \u201aÄúJames\u201aÄù and \u201aÄúJohn\u201aÄù. \u201aÄúDatabase\u201aÄù is a frequent keyword and has a large origin set while \u201aÄúJames\u201aÄù and \u201aÄúJohn\u201aÄù match singleton nodes. It should be noted that node #102 has a large fan-in as \u201aÄúJohn\u201aÄù has authored many papers. For simplicity lets assume all node prestiges and edge weights to be unity. Backward expanding search would explore at least 151 nodes (and touch 250 nodes) before generating the result rooted at node #100. Bidirectional search would start from nodes #101 and #102 (as nodes #1..100 have a lower activation due to large origin set). Expanding #101 would add #250 (with approximately the same activation) to the incoming queue while #102 would add 48 nodes (#103 .. #150) with a lower activation (\u201aàº ActivationOf(#101)/48). This would result in the exploration of #250 followed by #100. Finally, #100 would be expanded by the outgoing iterator to hit #150 producing the desired result. Hence, Bidirectional search would explore only 4 nodes (and touch about 150 nodes) before generating the result rooted at 100. 4.5 Producing Top-k Result Trees As mentioned in Section 4.2.3, answers are placed in an output buffer when they are generated since they may not be generated exactly in relevance score order. At each iteration of Bidirectional search, we compute an upper bound on relevance of the the next result that can be generated, and use the bound to output all buffered answers with a higher relevance. 512 The upper bound is computed as follows. For each keyword, we maintain the minimum path length mi among all nodes in the backward search trees for keyword i; that is, all nodes whose path length to a node containing keyword ki is less than mi have already been generated. As a coarser approximation, we can use the minimum path length among all nodes in Qin as a lower bound for all the mis. The best possible tree edge score for any answer node not yet seen would be defined by an edge score aggregation function h(m1, m2, . . . , mk) where k is the number of keywords; for the ranking function in Section 2.3, the edge score aggregation function h simply adds up its inputs. 7 However, every node that we have already seen is also a potential answer node; for each such node, we already know the distance to some of the keywords ki; we use these scores along with the bound mi for the remaining keywords to compute their best possible score. Combining these bounds with the maximum node prestige, we can get an upper bound ub on relevance score of any answer that has not yet been generated. (The upper bound computation is similar to that used in the NRA algorithm of [5].) Any answer with relevance score greater than or equal to ub can then be output. As a looser heuristic, we can output any answers whose tree edge score is greater than the score h(m1, m2, . . . , mk) described above; if there are multiple such answers they are sorted by their relevance score and output. This may output some answers out of order, since (a) some nodes seen already (but which have not yet been reached from all keywords) may yet have a score higher than this bound, and (b) the above heuristic ignores node prestige. However, the heuristic is cheaper to implement, and outputs answers faster, and the recall/precision measurements (Section 5.7) show that answers were output in the correct order on almost all queries we tested. 4.6 Single Iterator vs. Multiple Iterators Bidirectional search maintains a single iterator across all keywords, recording for each node n the shortest path from n to a node containing keyword ti, for each i. In contrast, Backward search maintains multiple iterators, recording the shortest path from each node n to each node nj containing keyword ti. Using a single iterator reduces the cost of search significantly. However, because of using a single iterator as above, Bidirectional search does not generate multiple trees with the same root,unlike Backward search. Even if a tree Tk rooted at n cannot be generated as a result, a rotation of Tk would be generated, albeit with a differ- 7 The edge score combination function used in [3] is of a slightly different form, since it adds up the weights of answer tree edges, rather than combining scores with respect to each keyword. For such an edge scoring function, we can still heuristically use a bounding function that adds up the mi\u201aÄôs, although the result would not be an accurate bound. ent score (the rotation would be rooted at one of the other nodes of Tk, with edges pointing from the root toward the leaves). In our experiments, we found that such alternative trees were indeed generated. To separate the effect of using a single iterator from the other effects of Bidirectional search, we created a version of backward search which we call single iterator backward search or SI-backward search. This is identical to Backward search except that it uses only one merged backward iterator, just like Bidirectional search. However, it does not use a forward iterator, and its backward iterator is prioritized only by distance from the keyword, as in the original backward search, without any spreading activation component. To avoid ambiguity, we shall call the original version of Backward search as multiple-iterator Backward search (MI-backward search). "},{"aspect":"expcomparison","tweet":" 5.6 Join order comparison We now describe an experiment to test our claim that Bidirectional search chooses a better join order than the Backward search. We fix the number of keywords Nodes explored ratio 64 32 16 8 4 2 1 Nodes expl. Time ratio Figure 6: Comparison of search algorithms 515 A B C D E F G H Query type (c) SI-Bkwd/Bidirec time & node ratio 25 20 15 10 5 Time ratio A=(T,S,S,S) B=(T,S,S,S) C=(T,S,S,S) D=(T,S,S,S) E=(T,S,S,S) F=(T,S,S,S) G=(T,S,S,S) H=(T,S,S,S) to 4 and size of the most relevant result to 3. Keywords in our queries are divided into four categories: tiny(T) (keyword matches 1 to 500 tuples), small(S) (keyword matches 1000-2000 tuples), medium(M) (keyword matches 2500-5000 tuples) and large(L) (keyword matches over 7000 tuples). We generate a workload of 400 queries from the DBLP database using techniques outlined in Section 5.4. Figure 6(c) shows the ratio of time taken by Bidirectional and Backward along with the nodes explored ratio for selected combinations of keyword categories in the query. We omit the other combinations for lack of space. We observe that Bidirectional outperforms SI-Backward in all the cases and the speedup increases as the difference between the origin sizes of keyword increases. Thus Bidirectional beats SI-Backward by a large margin when the keywords belong to the category (Tiny, Tiny, Tiny, Large), whereas the win is much smaller for (Medium, Medium, Medium, Medium) and (Medium, Large, Large, Large). 5.7 Recall/Precision Experiments We used the queries generated in Section 5.4 and measure the recall and precision ratios for each algorithm. We find the set of relevant results by executing SQL queries on the database while generating the workload, as described in Section 5.4. Our results indicated that both MI-Backward and Bidirectional performed equally well on the recall and precision ratios. The recall was found to be close to 100% for all the cases with an equally high precision at near full recall; in other words, almost all relevant answers were found before any irrelevant answer. Note that such high recall/precision are not unreasonable, given that the relevant answers are well defined, and our weighting schemes discourage irrelevant shortcut answers. "}]}