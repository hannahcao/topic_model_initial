{"user_name":" Discovering Similar Multidimensional Trajectories ","user_timeline":[{"aspect":"abstract","tweet":" Abstract  We investigate techniques for analysis and retrieval of object trajectories in a two or three dimensional space. Such kind of data usually contain a great amount of noise, that makes all previously used metrics fail. Therefore, here we formalize non-metric similarity functions based on the Longest Common Subsequence (LCSS), which are very robust to noise and furthermore provide an intuitive notion of similarity between trajectories by giving more weight to the similar portions of the sequences. Stretching of sequences in time is allowed, as well as global translating of the sequences in space. Efficient approximate algorithms that compute these similarity measures are also provided. We compare these new methods to the widely used Euclidean and Time Warping distance functions (for real and synthetic data) and show the superiority of our approach, especially under the strong presence of noise. We prove a weaker version of the triangle inequality and employ it in an indexing structure to answer nearest neighbor queries. Finally, we present experimental results that validate the accuracy and efficiency of our approach.  "},{"aspect":"expanalysis","tweet":" 7 Conclusion  In this paper we presented efficient techniques to accurately compute the similarity between trajectories of moving objects. Our distance measure is based on the LCSS model and performs very well for noisy signals. Since the exact computation is inefficient, we presented approximate algorithms with provable performance bounds. Moreover, we presented an efficient index structure, which is based on hierarchical clustering, for similarity (nearest neighbor) queries. The distance that we use is not a metric and therefore the triangle inequality does not hold. However, we prove that a similar inequality holds (although a weaker one) that allows to prune parts of the datasets without any false dismissals. Our experimentals indicate that the approximation algorithm can be used to get an accurate and fast estimation of the distance between two trajectories even under noisy conditions. Also, results from the index evaluation show that we can achieve good speed-ups for searching similar trajectories comparing with the brute force linear scan. We plan to investigate biased sampling to improve the running time of the approximation algorithms, especially when full rigid transformations (eg. shifting, scaling and rotation) are necessary. Another approach to index trajectories for similarity retrieval is to use embeddings and map the set of trajectories to points in a low dimensional Euclidean space[14]. The challenge of course is to find an embedding that approximately preserves the original pairwise distances and gives good approximate results to similarity queries.  "},{"aspect":"expdata","tweet":" 5.1 Time and Accuracy Experiments  Here we present the results of some experiments using the approximation algorithm to compute the similarity function  S2. Our dataset here comes from marine mammals' satellite tracking data. "},{"aspect":"background","tweet":" 1 Introduction  In this paper we investigate the problem of discovering similar trajectories of moving objects. The trajectory of a moving object is typically modeled as a sequence of consecutive locations in a multidimensional (generally two or three dimensional) Euclidean space. Such data types arise in many applications where the location of a given object is measured repeatedly over time. Examples include features extracted from video clips, animal mobility experiments, sign language recognition, mobile phone usage, multiple attribute response curves in drug therapy, and so on. Moreover, the recent advances in mobile computing, sensor and GPS technology have made it possible to collect large amounts of spatiotemporal data and there is increasing interest to perform data analysis tasks over this data [4]. For example, in mobile computing, users equipped with mobile devices move in space and register their location at different time instants via wireless links to spatiotemporal databases. In environmental information systems, tracking animals and weather conditions is very common and large datasets can be created by storing locations of observed objects over time. Data analysis in such data include determining and finding objects that moved in a similar way or followed a certain motion pattern. An appropriate and efficient model for defining the similarity for trajectory data will be very important for the quality of the data analysis tasks. 1.1 Robust distance metrics for trajectories  In general these trajectories will be obtained during a tracking procedure, with the aid of various sensors. Here also lies the main obstacle of such data; they may contain a significant amount of outliers or in other words incorrect data measurements (unlike for example, stock data which contain no errors whatsoever). 0 200 400 600 800 1000 1200 0 200 400 600 800 1000 1200 0 100 200 300 400 500 600 time x movement Figure 1. Examples of 2D trajectories. Two instances of video-tracked time-series data representing the word 'athens'. Start & ending contain many outliers.   Athens 1 Athens 2 Boston 1 Boston 2 DTW LCSS Figure 2. Hierarchical clustering of 2D series (displayed as 1D for clariry). Left: The presence of many outliers in the beginning and the end of the sequences leads to incorrect clustering. DTW is not robust under noisy conditions. Right:  The LCSS focusing on the common parts achieves the correct clustering. Our objective is the automatic classification of trajectories using Nearest Neighbor Classification. It has been shown that the one nearest neighbor rule has asymptotic error rate that is at most twice the Bayes error rate[12]. So, the problem is: given a database D of trajectories and a query Q  (not already in the database), we want to find the trajectory  T that is closest to Q. We need to define the following:  1. A realistic distance function, 2. An efficient indexing scheme.  Previous approaches to model the similarity between time-series include the use of the Euclidean and the Dynamic Time Warping (DTW) distance, which however are relatively sensitive to noise. Distance functions that are robust to extremely noisy data will typically violate the triangular inequality. These functions achieve this by not considering the most dissimilar parts of the objects. However, they are useful, because they represent an accurate model of the human perception, since when comparing any kind of data (images, trajectories etc), we mostly focus on the portions that are similar and we are willing to pay less attention to regions of great dissimilarity. For this kind of data we need distance functions that can address the following issues:  \u000f Different Sampling Rates or different speeds. The time-series that we obtain, are not guaranteed to be the outcome of sampling at fixed time intervals. The sensors collecting the data may fail for some period of time, leading to inconsistent sampling rates. Moreover, two time series moving at exactly the similar way, but one moving at twice the speed of the other will result (most probably) to a very large Euclidean distance.  \u000f Similar motions in different space regions. Objects can move similarly, but differ in the space they move. This can easily be observed in sign language recognition, if the camera is centered at different positions. If we work in Euclidean space, usually subtracting the average value of the time-series, will move the similar series closer.  \u000f Outliers. Might be introduced due to anomaly in the sensor collecting the data or can be attributed to human 'failure' (e.g. jerky movement during a tracking process). In this case the Euclidean distance will completely fail and result to very large distance, even though this difference may be found in only a few points.  \u000f Different lengths. Euclidean distance deals with timeseries of equal length. In the case of different lengths we have to decide whether to truncate the longer series, or pad with zeros the shorter etc. In general its use gets complicated and the distance notion more vague.  \u000f Efficiency. It has to be adequately expressive but sufficiently simple, so as to allow efficient computation of the similarity. To cope with these challenges we use the Longest Common Subsequence (LCSS) model. The LCSS is a variation of the edit distance. The basic idea is to match two sequences by allowing them to stretch, without rearranging the sequence of the elements but allowing some elements to be unmatched. The advantages of the LCSS method are twofold: 1) Some elements may be unmatched, where in Euclidean and DTW all elements must be matched, even the outliers.  2) The LCSS model allows a more efficient approximate computation, as will be shown later (whereas in DTW you need to compute some costly L p Norm). In figure 2 we can see the clustering produced by the  DTW distance. The sequences represent data collected through a video tracking process. Originally they represent 2d series, but only one dimension is depicted here for clarity. The DTW fails to distinguish the two classes of words, due to the great amount of outliers, especially in the beginning and in the end of the trajectories. Using the Euclidean distance we obtain even worse results. The LCSS produces the most intuitive clustering as shown in the same figure. Generally, the Euclidean distance is very sensitive to small variations in the time axis, while the major drawback of the  DTW is that it has to pair all elements of the sequences. Therefore, we use the LCSS model to define similarity measures for trajectories. Nevertheless, a simple extension of this model into 2 or more dimensions is not sufficient, because (for example) this model cannot deal with parallel movements. Therefore, we extend it in order to address similar problems. So, in our similarity model we consider a set of translations in 2 or more dimensions and we find the translation that yields the optimal solution to the LCSS  problem. The rest of the paper is organized as follows. In section 2 we formalize the new similarity functions by extending the  LCSS model. Section 3 demonstrates efficient algorithms to compute these functions and section 4 elaborates on the indexing structure. Section 5 provides the experimental validation of the accuracy and efficiency of the proposed approach and section 6 presents the related work. Finally, section 7 concludes the paper.  "},{"aspect":"expintro","tweet":" 5 Experimental Evaluation  We implemented the proposed approximation and indexing techniques as they are described in the previous sections and here we present experimental results evaluating our techniques. We describe the datasets and then we continue by presenting the results. The purpose of our experiments is twofold: first, to evaluate the efficiency and accuracy of the approximation algorithm presented in section 3 and second to evaluate the indexing technique that we discussed in the previous section. Our experiments were run on a PC AMD Athlon at 1 GHz with 1 GB RAM and 60 GB hard disk. "},{"aspect":"problemdef","tweet":" 2 Similarity Measures In this section we define similarity models that match the user perception of similar trajectories. First we give some useful definitions and then we proceed by presenting the similarity functions based on the appropriate models. We assume that objects are points that move on the (x; y)-plane  and time is discrete. Let A and B be two trajectories of moving objects with size n and m respectively, where A = ((a x;1 ; a y;1 ); : : : ;  (a x;n ; a y;n )) and B = ((b x;1 ; b y;1 ); : : : ; (b x;m ; b y;m )). For a trajectory A, let Head(A) be the sequence Head(A) = ((a x;1 ; a y;1 ); : : : ; (a x;n 1 ; a y;n 1 )). Definition 1 Given an integer  and a real number 0   1, we define the LCSS ;\u000f (A; B) as follows:  8 > > > >  > > > > :  0 if A or B is empty;  1 + LCSS ;\u000f (Head(A);Head(B));  if jax;n bx;m j  and jay;n by;m j  and jn mj \u0014   max(LCSS ;\u000f (Head(A);B); LCSS ;\u000f (A; Head(B))); otherwise The constant  controls how far in time we can go in order to match a given point from one trajectory to a point in another trajectory. The constant \u000f is the matching threshold (see figure 3). The first similarity function is based on the LCSS and the idea is to allow time stretching. Then, objects that are close in space at different time instants can be matched if the time instants are also close.  Definition 2 We define the similarity function S1 between two trajectories A and B, given  and \u000f, as follows: S1(; \u000f; A; B) =  LCSS ;\u000f (A; B)  min(n; m)  We use this function to define another similarity measure that is more suitable for trajectories. First, we consider the set of translations. A translation simply shifts a trajectory in space by a different constant in each dimension. Let F be the family of translations. Then a function f c;d belongs to F  if f c;d (A) = ((a x;1 + c; a y;1 +d); : : : ; (a x;n + c; a y;n +d)).  Next, we define a second notion of the similarity based on the above family of functions. 0 200 400 600 800 1000 1200 100 200 300 400 500 600 700 800 900 1000 2e 2d Figure 3. The notion of the LCSS matching within a region of  & \u000f for a trajectory. The points of the 2 trajectories within the gray region can be matched by the extended LCSS function.  Definition 3 Given , \u000f and the family F of translations, we define the similarity function S2 between two trajectories A  and B, as follows:  S2(; \u000f; A; B) = max  f c;d 2F  S1(; \u000f; A; f c;d (B)) So the similarity functions S1 and S2 range from 0 to 1.  Therefore we can define the distance function between two trajectories as follows:  Definition 4 Given , \u000f and two trajectories A and B we define the following distance functions:  D1(; \u000f; A; B) = 1 S1(; \u000f; A; B)  and  D2(; \u000f; A; B) = 1 S2(; \u000f; A; B)  Note that D1 and D2 are symmetric. LCSS ;\u000f (A; B) is equal to LCSS ;\u000f (B; A) and the transformation that we use in D2 is translation which preserves the symmetric property.  By allowing translations, we can detect similarities between movements that are parallel in space, but not identical. In addition, the LCSS model allows stretching and displacement in time, so we can detect similarities in movements that happen with different speeds, or at different times. In figure 4 we show an example where a trajectory B  matches another trajectory A after a translation is applied. Note that the value of parameters c and d are also important since they give the distance of the trajectories in space. This can be useful information when we analyze trajectory data. Time X Y B  c d  f(B) Figure 4. Translation of trajectory B.  The similarity function S 2 is a significant improvement over the S 1 , because: i) now we can detect parallel movements, ii) the use of normalization does not guarantee that we will get the best match between two trajectories. Usually, because of the significant amount of noise, the average value and/or the standard deviation of the time-series, that are being used in the normalization process, can be distorted leading to improper translations.  "},{"aspect":"solution","tweet":" 3 Efficient Algorithms to Compute the Similarity   3.1 Computing the similarity function S1  To compute the similarity functions S1 we have to run a LCSS computation for the two sequences. The LCSS  can be computed by a dynamic programming algorithm in  O(n  2  ) time. However we only allow matchings when the difference in the indices is at most , and this allows the use of a faster algorithm. The following lemma has been shown in [5], [11].  Lemma 1 Given two trajectories A and B, with jAj = n  and jBj = m, we can find the LCSS ;\u000f (A; B) in O((n +  m)) time.  If  is small, the dynamic programming algorithm is very efficient. However, for some applications  may need to be large. For that case, we can speed-up the above computation using random sampling. Given two trajectories A and  B, we compute two subsets RA and RB by sampling each trajectory. Then we use the dynamic programming algorithm to compute the LCSS on RA and RB. We can show that, with high probability, the result of the algorithm over the samples, is a good approximation of the actual value. We describe this technique in detail in [35].  3.2 Computing the similarity function S2  We now consider the more complex similarity function  S2. Here, given two sequences A; B, and constants ; \u000f,  we have to find the translation f c;d that maximizes the length of the longest common subsequence of A; f c;d (B)  (LCSS ;\u000f (A; f c;d (B)) over all possible translations. Let the length of trajectories A and B be n and m respectively. Let us also assume that the translation f c1 ;d1  is the translation that, when applied to B, gives a longest common subsequence LCSS ;\u000f (A; f c1 ;d1 (B)) = a, and it is also the translation that maximizes the length of the longest common subsequence LCSS ;\u000f (A; f c1 ;d1 (B)) =  max c;d2R LCSS ;\u000f (A; f c;d (B)).  The key observation is that, although there is an infinite number of translations that we can apply to B, each translation  f c;d results to a longest common subsequence between  A and f c;d (B), and there is a finite set of possible longest common subsequences. In this section we show that we can efficiently enumerate a finite set of translations, such that this set provably includes a translation that maximizes the length of the longest common subsequence of A and  f c;d (B).  To give a bound on the number of transformations that we have to consider, we look at the projections of the two trajectories on the two axes separately.  We define the x projection of a trajectory B = ((x 1 ; y 1 ); : : : (x m ; ym )) to be the sequence of the values on the x-coordinate: B x = (b x;1 ; : : : ; b x;m ). A one dimensional translation f c is a function that adds a constant to all the elements of a 1-dimensional sequence:  f c (x 1 ; : : : ; xm ) = (x 1 + c; : : : ; xm + c).  Take the x projections of A and B, A x and B x respectively. We can show the following lemma:  Lemma 2  Given trajectories A; B, if LCSS ;\u000f (A; f c1 ;d1 (B)) = a,  then the length of the longest common subsequence of the one dimensional sequences A x and f c1 (B x ) = (b x;1 +  c 1 ; : : : ; b x;m + c 1 ), is at least a: LCSS ;\u000f (A x ; f c1 (B x )) \u0015  a. Also, LCSS ;\u000f (A y ; f d1 (B y )) \u0015 a.  Now, consider A x and B x . A translation by c  0  , applied to B x can be thought of as a linear transformation of the form f(b x;i ) = b x;i + c  0  . Such a transformation will allow  b x;i to be matched to all a x;j for which ji jj  , and  a x;j \u000f \u0014 f(b x;i ) \u0014 a x;j + \u000f.  It is instructive to view this as a stabbing problem: Consider the O((n +m)) vertical line segments ((b x;i ; a x;j  \u000f); (b x;i ; a x;j + \u000f)), where ji jj  (Figure 5). Bx,i Bx,i+1 By,i+2 Bx,i+3 Bx,i+4 Bx,i+5 Ax,i Ax,i+1 Ax,i+2 fc1(x) = x + c1 fc2(x) = x + c2 Ax axis Bx axis Figure 5. An example of two translations . These line segments are on a two dimensional plane, where on the x axis we put elements of B x and on the y  axis we put elements of A x . For every pair of elements  b x;i ; a x;j in A x and B x that are within  positions from each other (and therefore can be matched by the LCSS algorithm if their values are within \u000f), we create a vertical line segment that is centered at the point (b x;i ; a x;j ) and extends  \u000f above and below this point. Since each element in A x can be matched with at most 2 + 1 elements in b x , the total number of such line segments is O(n).  A translation f c 0 in one dimension is a function of the form f c 0 (b x;i ) = b x;i + c  0  . Therefore, in the plane we described above, f c 0 (b x;i ) is a line of slope 1. After translating  B x by f c 0 , an element b x;i of B x can be matched to an element  a x;j of A x if and only if the line f c  0 (x) = x + c  0  intersects the line segment ((b x;i ; a x;j \u000f); (b x;i ; a x;j + \u000f)).  Therefore each line of slope 1 defines a set of possible matchings between the elements of sequences A x and  B x . The number of intersected line segments is actually an upper bound on the length of the longest common subsequence because the ordering of the elements is ignored. However, two different translations can result to different longest common subsequences only if the respective lines intersect a different set of line segments. For example, the translations f c1 (x) = x+c1 and f c2 (x) = x+c2 in figure 5 intersect different sets of line segments and result to longest common subsequences of different length. The following lemma gives a bound on the number of possible different longest common subsequences by bounding the number of possible different sets of line segments that are intersected by lines of slope 1.  Lemma 3 Given two one dimensional sequences A x , B x , there are O((n+m)) lines of slope 1 that intersect different sets of line segments.  Proof: Let f c  0 (x) = x + c  0  be a line of slope 1. If we move this line slightly to the left or to the right, it still intersects the same number of line segments, unless we cross an endpoint of a line segment. In this case, the set of intersected line segments increases or decreases by one. There are O((n+m)) endpoints. A line of slope 1 that sweeps all the endpoints will therefore intersect at most O((n +m))  different sets of line segments during the sweep. 2  In addition, we can enumerate the O((n + m)) translations that produce different sets of potential matchings by finding the lines of slope 1 that pass through the endpoints. Each such translation corresponds to a line f c  0 (x) = x+ c  0  . This set of O((n + m)) translations gives all possible matchings for a longest common subsequence of A x ; B x . By applying the same process on A y ; B y we can also find a set of O((n + m)) translations that give all matchings of  A y ; B y . To find the longest common subsequence of the sequences  A; B we have to consider only the O(  2  (n +m)  2  )  two dimensional translations that are created by taking the Cartesian product of the translations on x and the translations on y. Since running the LCSS algorithm takes  O((n +m)) we have shown the following theorem:  Theorem 1 Given two trajectories A and B, with jAj = n  and jBj = m, we can compute the S2(; \u000f; A; B) in O((n+ m)  3    3  ) time.  3.3 An Efficient Approximate Algorithm  Theorem 1 gives an exact algorithm for computing S2,  but this algorithm runs in cubic time. In this section we present a much more efficient approximate algorithm. The key in our technique is that we can bound the difference between the sets of line segments that different lines of slope 1 intersect, based on how far apart the lines are.  Consider again the one dimensional projections A x ; B x . Lets us consider the O((n +m)) translations that result to different sets of intersected line segments. Each translation is a line of the form f c  0 (x) = x+ c  0  . Let us sort these translations by c  0  . For a given translation f c  0 , let L f c 0 be the set of line segments it intersects. The following lemma shows that neighbor translations in this order intersect similar sets of line segments.  Lemma 4 Let f 1 (x) = x + c  0  1 ; : : : ; fN (x) = x + c  0  N  be the different translations for sequences A x and B x , where  c  0  1 \u0014 : : : \u0014 c  0  N  . Then the symmetric difference L f i \u0001L f j =  ji jj.  We can now prove our main theorem:  Theorem 2 Given two trajectories A and B, with jAj = n  and jBj = m, and a constant 0  1, we can find an approximation  AS2 ; (A; B) of the similarity S2(; \u000f; A; B)  such that S2(; \u000f; A; B) AS2 ; (A; B)  in O((m +  n)  3  =  2  ) time.  Proof: Let a = S2(; \u000f; A; B). We consider the projections of A and B into the x and y axes. There exists a translation  f i on x only such that L f i is a superset of the matches in the optimal LCSS of A and B. In addition, by the previous lemma, there are 2b translations (f i b ; : : : ; f i+b ) that have at most b different matchings from the optimal. Therefore, if we use the translations f ib , for i = 1; : : : ; d  2(n+m)  b  e in the ordering described above, we are within b different matchings from the optimal matching of A and B. We can find these translations in O((n +  m) log(n +m)) time if we find and sort all the translations. Alternatively, we can find these translations in  O(  (n+m) b  (n+m)) time if we run d  2(n+m)  b  e quantile operations. The same is true for A y and B y . So we get a total of (  2(m+n)  b  )  2  pairs of translations in the (x; y) plane. Since there is one that is b away from the optimal in each dimension, there is one that is 2b away from the optimal in 2 dimensions. Setting b =  (n+m)  2 completes the proof. 2  Given trajectories A; B with lengths n; m respectively, and constants , , \u000f, the approximation algorithm works as follows:  1. Using the projections of A; B on the two axes, find the sets of all different translations on the x and y axis. 2. Find the i  (n+m)  2  -th quantiles for each set, 1 \u0014 i \u0014  4    . 3. Run the LCSS ;\u000f algorithm on A and B, for each of the (  4    )  2  pairs of translations. 4. Return the highest result.  4 Indexing Trajectories for Similarity Retrieval   In this section we show how to use the hierarchical tree of a clustering algorithm in order to efficiently answer nearest neighbor queries in a dataset of trajectories. The distance function D2 is not a metric because it does not obey the triangle inequality. Indeed, it is easy to construct examples where we have trajectories A; B and C,  where D2(; \u000f; A; C) > D2(; \u000f; A; B) + D2(; \u000f; B; C).  This makes the use of traditional indexing techniques difficult.  We can however prove a weaker version of the triangle inequality, which can help us avoid examining a large portion of the database objects. First we define:  LCSS ;\u000f;F (A; B) = max f c;d 2F LCSS ;\u000f (A; f c;d (B)) Clearly, D2(; \u000f; A; B) = 1  LCSS ;\u000f;F (A;B)  min(jAj;jBj)  (as before, F  is the set of translations). Now we can show the following lemma:  Lemma 5 Given trajectories A; B; C,  LCSS ;2\u000f;F (A; C) > LCSS ;\u000f;F (A; B)+LCSS ;\u000f;F (B; C)  jBj  where jBj is the length of sequence B.  Proof: Clearly, if an element of A can match an element of B within \u000f, and the same element of B matches an element of C within \u000f, then the element of A can also match the element of C within 2\u000f. Since there are at least  jBj (jBj LCSS ;\u000f;F (A; B)) (jBj LCSS ;\u000f;F (B; C))  elements of B that match with elements of A and with elements of C, it follows that LCSS ;2\u000f;F (A; C) > jBj  (jBj LCSS ;\u000f;F (A; B)) (jBj LCSS ;\u000f;F (B; C)) =  LCSS ;\u000f;F (A; B) + LCSS ;\u000f;F (B; C) jBj 2 4.1 Indexing Structure  We first partition all the trajectories into sets according to length, so that the longest trajectory in each set is at most  a times the shortest (typically we use a = 2.) We apply a hierarchical clustering algorithm on each set, and we use the tree that the algorithm produced as follows: For every node C of the tree we store the medoid (MC ) of each cluster. The medoid is the trajectory that has the minimum distance (or maximum LCSS) from every other trajectory in the cluster:  max v i 2Cmin v j 2CLCSS ;\u000f;F (v i ; v j ; e). So given the tree and a query sequence Q, we want to examine whether to follow the subtree that is rooted at C. However, from the previous lemma we know that for any sequence B in C:  LCSS ;\u000f;F (B; Q)  jBj + LCSS ;2\u000f;F (MC ; Q) LCSS ;\u000f;F (MC ; B)  or in terms of distance:  D2(; \u000f; B; Q) = 1  LCSS ;\u000f;F (B;Q)  min(jBj;jQj)  >  1  jBj  min(jBj;jQj) LCSS ;2\u000f;F (MC;Q)  min(jBj;jQj)  +  LCSS ;\u000f;F (MC ;B) min(jBj;jQj)  In order to provide a lower bound we have to maximize the expression jBj LCSS ;\u000f;F (A; B). Therefore, for every node of the tree along with the medoid we have to keep the trajectory r c that maximizes this expression. If the length of the query is smaller than the shortest length of the trajectories we are currently considering we use that, otherwise we use the minimum and maximum lengths to obtain an approximate result.  4.2 Searching the Index tree for Nearest Trajectories   We assume that we search an index tree that contains trajectories with minimum length minl and maximum length  maxl. For simplicity we discuss the algorithm for the 1Nearest Neighbor query, where given a query trajectory Q  we try to find the trajectory in the set that is the most similar to Q. The search procedure takes as input a node N  in the tree, the query Q and the distance to the closest trajectory found so far. For each of the children C, we check if the child is a trajectory or a cluster. In case that it is a trajectory, we just compare its distance to Q with the current nearest trajectory. If it is a cluster, we check the length of the query and we choose the appropriate value for  min(jBj; jQj). Then we compute a lower bound L to the distance of the query with any trajectory in the cluster and we compare the result with the distance of the current nearest neighbor mindist. We need to examine this cluster only if L is smaller than mindist.  In our scheme we use an approximate algorithm to compute the LCSS ;\u000f;F . Consequently, the value of  LCSS ;\u000f;F (MC ;B) min(jBj;jQj) that we compute can be up to  times higher than the exact value. Therefore, since we use the approximate algorithm of section 3.2 for indexing trajectories, we have to subtract  \u0003min(jMC j;jBj)  min(jBj;jQj)  from the bound we compute for D2(; \u000f; B; Q). Note that we don't need to worry about the other terms since they have a negative sign and the approximation algorithm always underestimates the  LCSS.  "},{"aspect":"expcomparison","tweet":" 1  It consists of sequences of geographic locations of various marine animals (dolphins, sea lions, whales, etc) tracked over different periods of time, that range from one to three months (SEALS dataset). The length of the trajectories is close to 100. Examples have been shown in figure 1. In table 1 we show the computed similarity between a pair of sequences in the SEALS dataset. We run the exact and the approximate algorithm for different values of  and  \u000f and we report here some indicative results. K is the number of times the approximate algorithm invokes the LCSS  procedure (that is, the number of translations (c; d) that we try). As we can see, for K = 25 and 49 we get very good results. We got similar results for synthetic datasets. Also, in table 1 we report the running times to compute the similarity measure between two trajectories of the same dataset. The approximation algorithm uses again from 4 to 49 different runs. The running time of the approximation algorithm is much faster even for K = 49.  As can be observed from the experimental results, the running times of the approximation algorithm is not proportional to the number of runs (K). This is achieved by reusing the results of previous translations and terminating early the execution of the current translation, if it is not going to yield a better result. The main conclusion of the above experiments is that the approximation algorithm can provide a very tractable time vs accuracy trade-off for computing the similarity between two trajectories, when the similarity is defined using the LCSS model.  5.2 Classification using the Approximation Algorithm   We compare the clustering performance of our method to the widely used Euclidean and DTW distance functions. Specifically:  1  http://whale.wheelock.edu/whalenetstuff /stop cover.html  Similarity Running Time (sec)   \u000f Exact Approximate for K tries Exact Approximate for K tries 4 9 25 49 4 9 25 49 2 0.25 0.316 0.1846 0.22 0.253 0.273 17.705 0.0012 0.0014 0.0017 0.0022 2 0.5 0.571 0.410 0.406 0.510 0.521 17.707 0.0012 0.0014 0.00169 0.0022 4 0.25 0.387 0.196 0.258 0.306 0.323 32.327 0.0016 0.0018 0.0022 0.00281 4 0.5 0.612 0.488 0.467 0.563 0.567 32.323 0.0015 0.0018 0.0023 0.00280 6 0.25 0.408 0.250 0.313 0.357 0.367 90.229 0.0017 0.00191 0.00231 0.0031 6 0.5 0.653 0.440 0.4912 0.584 0.591 90.232 0.0017 0.00193 0.0023 0.0031 Table 1. Similarity values and running times between two sequences from our SEALS dataset. 1. The Euclidean distance is only defined for sequences of the same length (and the length of our sequences vary considerably). We tried to offer the best possible comparison between every pair of sequences, by sliding the shorter of the two trajectories across the longer one and recording their minimum distance. 2. For DTW we modified the original algorithm in order to match both x and y coordinates. In both DTW and Euclidean we normalized the data before computing the distances. Our method does not need any normalization, since it computes the necessary translations. 3. For LCSS we used a randomized version with and without sampling, and for various values of . The time and the correct clusterings represent the average values of 15 runs of the experiment. This is necessary due to the randomized nature of our approach.  5.2.1 Determining the values for  & \u000f  The values we used for  and \u000f are clearly dependent on the application and the dataset. For most datasets we had at our disposal we discovered that setting  to more than  20 30% of the trajectories length did not yield significant improvement. Furthermore, after some point the similarity stabilizes to a certain value. The determination of \u000f is application dependent. In our experiments we used a value equal to the smallest standard deviation between the two trajectories that were examined at any time, which yielded good and intuitive results. Nevertheless, when we use the index the value of \u000f has to be the same for all pairs of trajectories.  5.2.2 Experiment 1 - Video tracking data.  The 2D time series obtained represent the X and Y position of a human tracking feature (e.g. tip of finger). In conjuction with a \"spelling program\" the user can \"write\" various words [19]. We used 3 recordings of 5 different words. The data correspond to the following words: 'athens', 'berlin', 'london', 'boston', 'paris'. The average length of the series is around 1100 points. The shortest one is 834 points and the longest one 1719 points. To determine the efficiency of each method we performed hierarchical clustering after computing the N  2  =2  pairwise distances for all three distance functions. We evaluate the total time required by each method, as well as the quality of the clustering, based on our knowledge of which word each trajectory actually represents. We take all possible pairs of words (in this case 5 \u0003 4=2 = 10 pairs) and use the clustering algorithm to partition them into two classes. While at the lower levels of the dendrogram the clustering is subjective, the top level should provide an accurate division into two classes. We clustered using single, complete and average linkage. Since the best results for every distance function are produced using the complete linkage, we report only the results for this approach (table 2). The same experiment is conducted with the rest of the datasets. Experiments have been conducted for different sample sizes and values of  (as a percentage of the original series length). The results with the Euclidean distance have many classification errors and the DTW has some errors, too. For the LCSS the only real variations in the clustering are for sample sizes s \u0014 10%. Still the average incorrect clusterings for these cases were constantly less than one ( 0:7). For 15% sampling or more, there were no errors. 5.2.3 Experiment 2 - Australian Sign Language Dataset (ASL)  2  .  The dataset consists of various parameters (such as the X ,Y, Z hand position, azimuth etc) tracked while different writers sign one the 95 words of the ASL. These series are relatively short (50-100 points). We used only the X and Y parameters and collected 5 recordings of the following 10 words: 'Norway', 'cold', 'crazy', 'eat', 'forget', 'happy', 'innocent', 'later', 'lose', 'spend'. This is the experiment conducted also in [25] (but there only one dimension was used). Examples of this dataset can be seen in figure 6.  2  http://kdd.ics.uci.edu  Distance Function Time (sec) Correct Clusterings (out of 10) Complete Linkage Euclidean 34.96 2 DTW 237.641 8 LCSS :  s = 5%;  = 20% 2.733 9.800  s = 10%;  = 20% 8.041 9.933  s = 15%;  = 20% 16.173 10  s = 20%;  = 20% 28.851 10  s = 25%;  = 20% 45.065 10  s = 30%;  = 20% 65.203 10  s = 40%;  = 20% 113.583 10  s = 60%;  = 20% 266.753 10  s = 100%;  = 20% 728.277 10 Table 2. Results using the video tracking data for various sizes of sample s and . Distance Time (sec) Correct Clusterings (out of 45) ASL Correct Clusterings (out of 45) ASL with noise Euclidean 2.271 15 5 DTW 9.112 20 7 Table 3. Results for ASL data and ASL with added noise for the Euclidean and DTW distance functions. The performance of the LCSS in this experiment is similar to the DTW (DTW recognized correctly 20 clusters and  LCSS recognized 21 clusters). This is expected since this dataset does not contain excessive noise and furthermore the data seem to be already normalized and rescaled within the range [ 1 : : : 1]. Therefore in this experiment we used also the similarity function S1 (no translation), since the translations were not going to achieve any further improvement (see figure 7). Sampling is only performed down to 75% of the series length (these trajectories are already short). As a consequence, even though we don't gain much in accuracy, our execution time is comparable to the Euclidean (without performing any translations). This is easily explained, since the computation of the L 2 Norm is more computationally intensive, than the simple range comparison that is used in our approach.  10 20 30 40 50 60 70 80 90 100-0.5 -0.4 -0.3 -0.2 -0.1 0 0.1 0.2 0.4 0.6 0.8 Time X movement Figure 6. Four recordings of the word 'norway' in the Australian Sign Language. The graph depicts the x &  y position of the writer's hand. 5.2.4 Experiment 3 - ASL with added noise  We added noise at every sequence of the ASL at a random starting point and for duration equal to the 15% of the series length. The noise was added using the function:  h~x noise ; ~y noise i = h~x; ~yi + randn \u0003 rangeV alues,  where randn produces a random number, chosen from a normal distribution with mean zero and variance one, and  rangeV alues is the range of values on X or Y coordinates. In this last experiment we wanted to see how the addition of noise would affect the performance of the three distance functions. Again, the running time is the same as with the original ASL data. The LCSS proves to be more robust than the Euclidean and the DTW under noisy conditions (table 3, figure 7, 8). The Euclidean again performed poorly, recognizing only 5 clusters, the DTW recognized 7 and the LCSS up to 14 clusters (almost as many recognized by the Euclidean without  any noise!).  5.3 Evaluating the quality and efficiency of the indexing technique  In this part of our experiments we evaluated the efficiency and effectiveness of the proposed indexing scheme. We performed tests over datasets of different sizes and different number of clusters. To generate large realistic datasets, we used real trajectories (from the SEALS and  ASL datasets) as \"seeds\" to create larger datasets that follow the same patterns. To perform tests, we used queries that do not have exact matches in the database, but on the other hand are similar to some of the existing trajectories. For each experiment we run 100 different queries and we report the averaged results. We have tested the index performance for different number of clusters in a dataset consisting of a total of 2000 tra-  0.75 0.8 0.85 0.9 0.95 1 0.4 0.8 1 0 2 3 5 10 Delta as percentage of series length  Sample size  Time (sec) Figure 7. ASL data: Time required to compute the pairwise distances of the 45 combinations(same for ASL and ASL with noise)  0.75 0.8 0.85 0.9 0.95 1 0.4 0.8 1 10 14 30 45 Delta as percentage of series length  Sample size  Av. Correct Clusterings (out of 45) Figure 8. Noisy ASL data: The correct clusterings of the LCSS method using complete linkage. 1 5 10 15 20 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Number of Nearest Neighbors  5 Clusters 8 Clusters 10 Clusters Figure 9. Performance for increasing number of Nearest Neighbors.  0 2 4 6 8 10 12 14 16 18 20 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Number of Nearest Neighbors  1000 trajectories 2000 -//- 4000 -//- 8000 -//- 16000 -//- Figure 10. The pruning power increases along with the database size. jectories. We executed a set of K-Nearest Neighbor (KNN) queries for K 1, 5, 10, 15 and 20 and we plot the fraction of the dataset that has to be examined in order to guarantee that we have found the best match for the K-NN query. Note that in this fraction we included the medoids that we check during the search since they are also part of the dataset. In figure 9 we show some results for K-Nearest Neighbor queries. We used datasets with 5, 8 and 10 clusters. As we can see the results indicate that the algorithm has good performance even for queries with large K. We also performed similar experiments where we varied the number of clusters in the datasets. As the number of clusters increased the performance of the algorithm improved considerably. This behavior is expected and it is similar to the behavior of recent proposed index structures for high dimensional data [9, 6, 21]. On the other hand if the dataset has no clusters, the performance of the algorithm degrades, since the majority of the trajectories have almost the same distance to the query. This behavior follows again the same pattern of high dimensional indexing methods [6, 36]. The last experiment evaluates the index performance, over sets of trajectories with increasing cardinality. We indexed from 1000 to 16000 trajectories. The pruning power of the inequality is evident in figure 10. As the size of the database increases, we can avoid examining a larger fraction of the database.  "}]}