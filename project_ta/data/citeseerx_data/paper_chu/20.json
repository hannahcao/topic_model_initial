{"user_name":" A Monte Carlo Algorithm for Fast Projective Clustering ","user_timeline":[{"aspect":"abstract","tweet":" ABSTRACT We propose a mathematical formulation for the notion of optimalprojective cluster, starting from natural requirements on the density  of points in subspaces. This allows us to develop a Monte Carloalgorithm for iteratively computing projective clusters. We prove that the computed clusters are good with high probability. We im-plemented a modified version of the algorithm, using heuristics to speed up computation. Our extensive experiments show that ourmethod is significantly more accurate than previous approaches. In particular, we use our techniques to build a classifier for detectingrotated human faces in cluttered images. "},{"aspect":"expanalysis","tweet":" 5. FACIAL ROTATION ESTIMATOR  In this section we describe an application of projective clusteringin computer vision. Our database consists of gray scale images of  size 16 \\Theta  16 pixels representing human faces. Each face is frontalbut may be rotated in-plane. We repeatedly apply the heuristic described in the previous section, in order to discover projective clus-ters in the data. We allow projective clusters to overlap, since we determined experimentally that this approach is better than com-puting disjoint clusters. A possible explanation is that an image is likely to fit well in more than one pattern, and that each patterncontains a relatively small percentage of images. For example, an image representing a high-cheekboned man with a moustache maybe selected in a cluster of high-cheekboned faces, and in a cluster of faces with a moustache. Although these clusters overlap, we expectthe overlap to be small, and so they should not be merged into just one cluster. On the other hand, a cluster may have small overlapswith a large number of other clusters. If we eliminated the points that are already clustered, the data would quickly become too frag-mented and the computation of additional projective clusters would be meaningless.We propose an algorithm that uses the projective clusters to classify query images into one of 16 rotation classes. This task is veryuseful in enabling face detectors to detect rotated faces. Successful solutions to face detection (see, e.g., [13, 14] and referencestherein) typically work by training a classifier that takes a fixed size image (e.g., 16 \\Theta  16 pixels) as input and outputs 1 if the image is aface and 0 otherwise. To find all faces in a large image, a  16 \\Theta  16window is scanned over all possible positions in the image as well  as at all possible scales. The detector is evaluated on each window.Successful detectors handle frontal upright faces. These detectors can be augmented to handle in-plane rotations as follows: a rotationestimator determines the angle by which an image window should be derotated to make it upright and the derotated image is used asinput to the frontal upright face detector. A previous approach estimated rotation using a neural network [12]. Our rotation estimatorthrows away most non-face windows. However, it also accepts a certain percentage of non-face windows, which it is likely to dero-tate by a random amount; the face detector should still classify the derotated windows as non-faces.To estimate the in-plane rotation angle for a face image, we first discretize the rotation angle into 16 distinct classes, each span-ning  22:5ffi. For each rotation class we use a training set of about 5; 000 faces (see Figure 11). All the original face images belongto rotation class  0, which covers roughly \\Gamma 11:25ffi to 11:25ffi. Wesynthetically generated training examples for the other  15 rotationclasses by rotating the upright images appropriately. For each rotation class, we computed 500 projective clusters, which are enoughto cover most of the training faces in that class. We used the value w = 90 in our experiments (the maximum value for w is 256,i.e. the number of gray values). The average dimensionality of the computed clusters was 45 (almost 20% of the total number ofdimensions).  Since the dimensions of the data space are image pixels, thereis an underlying correlation among some of the dimensions. For example, adjacent pixels are likely to have similar values. How-ever, our projective cluster algorithm assumes that dimensions are independent. This assumption can make the resulting projective  Figure 11: Example of frontal upright face images used fortraining. clusters very sensitive to changes like small translation or scalingof the images. Such sensitivity is undesirable, since small transformations do not affect the overall appearance of an image, and boththe original and the transformed image should be covered by the same cluster. In order to reduce this sensitivity, we apply a trans-formation such that the pixel values in the resulting image reflect a certain correlation among neighboring pixels in the original im-age. Thus, even though the projective cluster method still treats the pixels independently, the data points themselves reflect pixel corre-lations. In addition, our transformation also reduces the effects of different illuminations of the images. The transformation first con-verts each 8-bit gray scale pixel to one of the values 0, 128, or 255 (i.e. black, gray, or white), as follows. A pixel is set to 255 if itsvalue is significantly larger than the average value of its neighbors. Similarly, a pixel is set to 0 if its value is significantly smaller thanthe average value of its neighbors. Otherwise, it is set to 128. The resulting image is then smoothed using standard techniques. Figure12 shows an example of a face image and its transformation.  Figure 12: Original image (left) and resulting transformed im-age (right).  To estimate the rotation class for a query image, we determinethe projective cluster that contains it (if any), and label the query accordingly. If more than one projective cluster contains the query,we label it with the rotation class that has the most clusters containing the query. We tested the accuracy of rotation estimation asfollows. We rotated each face in a test set of  60 frontal uprightfaces in 1ffi increments from 0ffi to 360ffi. We labeled each of these 21600 images with its rotation class and used the projective clustersto estimate the rotation class as above. We computed the accuracy  of the rotation estimator as follows: if a face is in rotation class iand its rotation class is estimated as  j, the error is j \\Gamma  i, exceptfor j = 15 and i = 0, when the error is \\Gamma 1. Figure 13 shows the  resulting histogram of errors, with the x-axis corresponding to theerror. The figure shows that the rotation class for the majority of the examples is correctly classified. About 85% of the exampleshave an error of  0 or \\Sigma 1. Thus, projective clusters can determinethe rotation class of a query with relatively high accuracy. It is interesting to note that, aside from 0 and \\Sigma 1, the only other errorvalues corresponding to a significant percentage of points are  \\Sigma 8.This happens because, due to the low resolution, the mouth and eye  regions look similar, and the algorithm may \"switch\" them, discov-ering the image upside-down.  Figure 13: Histogram of errors for rotation estimation. Figure 14: Output of the face detector on the \"Mir\" image.  Finally, we integrated the rotation estimator with a frontal facedetection method as described earlier. The resulting system used our rotation estimator as the front end and the Viola-Jones frontalface detector [14] as the back end. Figure 14 shows the result of the algorithm on a photograph of cosmonauts inside the Mir spacestation. The black rectangles indicate the windows in which the algorithm discovers human faces (for clarity purposes, if two win-dows overlap significantly, only one of them is drawn). "},{"aspect":"expdata","tweet":" Data generation. We use the data generator described in thePROCLUS paper. The ORCLUS generator is extremely similar, except that it generates arbitrarily oriented clusters. However, sincewe focus on axis-parallel clusters, the two generators are the same.  All the generated points have coordinates in the range [0 : : : 100].Unless otherwise specified, each dataset has 100,000 points that are 200-dimensional, and consists of 5 clusters. We slightly modifythe generator in the following sense: Recall that when generating cluster i+1, about 50% of its bounded dimensions are chosen fromamong the bounded dimensions of cluster  i. This is intended tomodel the fact that different clusters often share some dimensions.  However, the centers of cluster i and i + 1 are chosen completelyindependent of each other. In other words, even if clusters  i and i + 1 share some dimensions, they are likely to be well separatedin that respective subspace. We modify the algorithm for choosing  the cluster centers in the following sense. Suppose that clusters iand  i + 1 both have bounded dimension j. Let cij and ci+1j be the  j-coordinates of centers ci and ci+1. We choose ci+1j ^ cij + 2oeij,where  oeij is the standard deviation of the points in cluster i alongdimension  j. Hence, some of the points in clusters i and i + 1will be close to each other in the subspace spanned by the common  bounded dimensions of the clusters. This introduces an additionalchallenge for distinguishing between the clusters.  Furthermore, we want to investigate the performance of DOC andORCLUS under different distributions of the cluster points in the subspace of bounded dimensions. The original data generator onlyallowed for clusters whose points have a normal distribution along the bounded dimensions. We modify it so that we can generate thefollowing types of projective clusters:  1. U-clusters: Cluster points are uniformly distributed in a smallhyper-cube in the subspace of bounded dimensions. Usually,  the hyper-cube size is 15 or 20. 2. MGq-clusters: Cluster points follow a mixture of q Gaus-sians distribution in the subspace of bounded dimensions.  For a fixed cluster, let c be the center of the cluster, selectedas before. For any bounded dimension  j let oej be the stan-dard deviation generated as in the original algorithm. We define the mean values of the Gaussians along dimension j tobe equally spaced in the interval  [cj \\Gamma  oej; cj + oej]. The stan-dard deviation of each Gaussian along dimension  j is oej=q.  3. N-clusters: These are generated as in the original algorithm,so that cluster points follow a normal distribution in the bounded  subspace. Any N-cluster can be viewed as an MG-clusterwith  q = 1.  We say that a dataset is a U-set, MGq-set or N-set depending on thetype of clusters it contains.  Finally, we wish to verify our intuition that DOC should not misssmall projective clusters. In order to do so, we modify the procedure for generating the number of points in each cluster. Recallthat the original generator assigns  ni points to cluster i, where niis proportional to the realization of an exponential random variable.  This results in all clusters having reasonably close sizes. To forceimbalance in the cluster sizes, we start by computing the values  niin a similar manner, and then do the following. Assume that k is thenumber of generated clusters. For each i ^ k=2, let n0i = ffini, and n0i+k=2 = ni+k=2 + (1 \\Gamma  ffi)ni, where 0 ! ffi ! 1 is a parameter (inour simulations, we used  ffi 2 f0:2; 0:33; 0:5g). We let the values n0i be the number of points that we generate in each cluster. "},{"aspect":"background","tweet":" 1. PROJECTIVE CLUSTERING  Clustering is a widely used technique for data mining, indexing,and classification. Many practical methods proposed in the last few  years, such as CLARANS [11], BIRCH [15], DBSCAN [5, 6], andCURE [7], are \"full-dimensional,\" in the sense that they give equal importance to all the dimensions when computing the distance be-tween two points. While such approaches have proven successful for low-dimensional datasets, their accuracy and/or efficiency de-crease significantly in higher dimensional spaces (see [9] for an excellent analysis and discussion). The reason for this performancedeterioration is the so-called dimensionality curse. Recent research shows that for moderate-to-high dimensional spaces (tens or hun-dreds of dimensions), a full-dimensional distance is often irrelevant, as the farthest neighbor of a point is expected to be almost as  yAuthor did this research when he was associated with the Compaq Research Lab. \\Lambda The work of the first and third authors is supported in part by  National Science Foundation research grants CCR-9732287 andEIA-9870724, by Army Research Office MURI grant DAAH04-  96-1-0013, by an NYI award, by a Sloan fellowship, and by a grantfrom the U.S.-Israeli Binational Science Foundation.  Permission to make digital or hard copies of all or part of this work forpersonal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copiesbear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specificpermission and/or a fee. ACM SIGMOD '2002 June 4-6, Madison, Wisconsin, USACopyright 2002 ACM 1-58113-497-5/02/06 ...$5.00.  close as its nearest neighbor [8].Methods such as Principal Component Analysis (PCA) reduce the dimensionality of the data by projecting all points on a sub-space so that the information loss is minimized. A standard clustering method is then used in this subspace. However, PCA does nothandle well those situations when different subsets of the points lie on different lower-dimensional subspaces. For the example in Fig-ure 1(a), any attempt to reduce the dimensionality of all the points results in significant information loss, and a full-dimensional clus-tering technique like  k-means is unlikely to discover the three pat-terns in the data.  x3  x2  x1  BC;D Bp;D  w  p  2w x2  x3  x1 C3 C1  C2 Figure 1: (a) Three subspace patterns; (b) Boxes corresponding to aprojective cluster  (C; D).  Recognizing the need for increased flexibility in reducing thedata dimensionality, recent database research has proposed computing projective clusters, in which points that are closely corre-lated in some subspace are grouped together. Instead of projecting the entire dataset on a single subspace, these methods project eachcluster on its associated subspace, which is generally different from the subspace associated with another cluster. Projective clusteringalgorithms have been successfully used for indexing [4], as well as pattern discovery in moderately high-dimensional data [1, 2].All these previous approaches are partitioning methods, i.e. they divide the data into k clusters and an outlier set, and iteratively im-prove the quality of the clustering. However, no formal definition is given as to what constitutes an optimal k-clustering, and thereis no guarantee on the quality of the output. As with most partitioning methods, these approaches also suffer from the inherent  necessity of having the user provide the number of clusters k. Thisproblem is less severe in indexing applications, where the choice of k is driven by outside considerations such as the desired treefanout, and where small variations of  k are unlikely to affect theindex performance in a significant way. However, when the goal is  to discover patterns in the data, even increasing or decreasing k by 1 can generate very different outputs. In such cases, the clusteringmethod must be called a few times with different values of  k, untilthe output is deemed \"accurate\" enough (either with respect to a  quality measure, or by a human expert). Our Contributions. In this paper we propose a mathematicaldefinition for the notion of optimal projective cluster, starting from natural requirements on the density of points in subspaces. This al-lows us to develop a Monte Carlo algorithm that computes, with high probability, a good approximation of an optimal projectivecluster. We call our algorithm DOC, from Density-based Optimal projective Clustering.Density-based approaches have been used before, either for fulldimensional clustering [10], or for enumerating all the dense sub-space regions in the data [3]. However, these methods have exponential dependence on the dimensionality, largely due to the factthat they use regular grids in order to find and connect dense areas. The number of relevant grid cells is, in general, exponentialin the number of dimensions. To alleviate this problem, a recent technique called OptiGrid [9] uses irregular grids determined byhyperplanes that pass through areas of small density, in order to compute dense clusters. Since we are interested in projective clus-ters, we take a slightly different view, in that we require each cluster to be dense only in its corresponding subspace. As we will seein the next section, our density conditions refer only to the number of points that project inside an interval of given length, and donot make any assumption on the distribution of points. In our experiments we use synthetic data generated with various subspacedistributions and show that our algorithm maintains high accuracy on all the sets. Iterative clustering versus global partitioning. Once wehave an efficient method for generating a provably good projective cluster, we iterate the algorithm in a greedy manner1, until sometermination criterion is met. During each iteration, we compute an approximation of an optimal cluster over the current set of points.The termination criterion can be defined in more than one way, e.g.: a certain percentage of the points have been clustered; or a user-specified number of clusters have been computed; or the quality measure of the clusters has decreased too much. Hence, by contrastto partitioning methods, the user need not specify the number of clusters k unless he wants to. This allows more flexibility in tuningthe algorithm to the particular application that uses it.  Moreover, we can either require clusters to be disjoint, or allowthem to have common points. In the first case, points that are clustered are eliminated from subsequent computations of clusters. Inthe second case, clustered points are not eliminated, and we simply check that we do not generate the same cluster twice. In the experi-mental section we discuss an application from image processing in which we found that overlapping clusters produce better results.One particularly desirable property of our method is that it is accurate even when the cluster sizes vary significantly (in termsof number of points). Many partitioning methods rely on random sampling for computing an initial partition. Hence, small clusters  1In this paper, we use the terms iterative clustering and greedy clustering interchangeably.  are likely to be missed. As a result, their points are either assignedto other clusters, or declared outliers. We have observed this behavior in the experiments we conducted using the PROCLUS [1] andORCLUS [2] algorithms. By contrast, the DOC algorithm does not miss small clusters. Although it is likely that it discovers onlylarge clusters during the first iterations, once these clusters are eliminated from the current dataset the smaller clusters become largewith respect to the remaining points, and will thus be discovered in subsequent iterations. In addition, our method handles outliers rel-atively easy. Since, during each iteration, we compute a projective cluster that approximates the optimal one, outliers tend to remainunclustered. We note that outlier handling is not an easy task in most partitioning algorithms, which use various heuristics in orderto distinguish between cluster points and outliers.  We conclude that iterative clustering has significant advantagesover partitioning methods. 2 Any greedy clustering method requires two main steps:  1. Define what an optimal cluster is. Since clusters are discov-ered one at a time, the extent to which this definition models  the \"natural\" clusters in the data determines the quality of theresult.  2. Design a fast and accurate method for computing one suchoptimal cluster, or a good approximation for it.  Our main contribution is to propose new solutions for these stepsin the context of projective clustering. Because we provide a rigorous mathematical definition in step 1, we are able to give guaranteeson the quality of the result computed by the method in step 2. Our experimental results, both on real and synthetic data, indicate thatour definition of an optimal projective cluster is a good way of modeling subspace patterns. We restrict our attention to the case whenall projections are along coordinate axes. This restriction is natural in many practical applications, in which coordinate axes have spe-cial meaning. For example, in a database of employees, one axis may represent the salary, another the length of employment withthe company, and a third one the employees' age. Discovering a projective cluster in the subspace spanned by salary and employ-ment length has the following interpretation: there is a correlation between salaries in range A and years of employment in range B,which is independent of employees' age. In fact, we provide experimental evidence that axis-parallel projective clusters are useful foranother interesting dataset which consists of gray scale images (see below). However, it is not difficult to envision applications in whicharbitrarily oriented projective clusters are more effective than axisparallel ones. We intend to study this problem in our future work.We view our current results as a promising step in the direction of developing fast, provably accurate, and stable projective clusteringmethods.  Application to image processing. We apply our clusteringideas to the problem of detecting rotated human faces in cluttered images. Our database consists of low resolution gray scale imagesof  16 \\Theta  16 pixels. All images represent human faces in frontalview, roughly aligned with the image boundaries; see Figure 11.  In addition, each face is rotated in-plane by one of 15 differentrotation angles, resulting in 16 images that contain the same face tilted at various degrees from the vertical. We map each image i toa  256-dimensional point pi, so that each dimension corresponds to  2Another class of methods consists of hierarchical clustering algorithms. However, the cost of such algorithms is usually quadraticin the number of objects, making them too expensive for large  datasets, and for high dimensional spaces.  a pixel. The value of pi on coordinate j is equal to the gray value ofthe  jth pixel in i. A projective cluster in this dataset consists of a setof faces that have similar gray values on a subset of the pixels. For  example, a pixel in the eye region is dark for most of the faces. Ifthe faces in the cluster have eyes at about the same position relative to the image boundary, then eye pixels correspond to some of thedimensions in the cluster. On the other hand, background pixels will in general have a lot of variation, and we do not expect them tobelong to the cluster. We discuss this application in detail in section 5.  Our paper is organized as follows. In Section 2 we formally de-fine the notion of an optimal projective cluster. We then propose  a Monte Carlo algorithm for approximating an optimal projectivecluster in Section 3, and prove our claims on the quality of the cluster it returns. We also discuss practical implementation issues andheuristics for speeding up our method. Our experiments on synthetic data are presented in Section 4, and our results on the imagedatabase are described in Section 5. We conclude in Section 6. "},{"aspect":"expintro","tweet":" We perform our simulations on a Pentium III 800MHz machinewith  256MB of RAM running the Linux operating system. In or-der to evaluate the competitiveness of our method, we implemented  both PROCLUS [1] and ORCLUS [2], the two previous algorithmsthat compute axis-parallel projective clusters. In fact, ORCLUS is a more general algorithm that is able to compute arbitrarily ori-ented clusters. However, it can be restricted to compute only axisparallel clusters, and we use it in this sense for comparison rea-sons. Our intention was to evaluate all three methods on the same datasets. However, PROCLUS proved to be prohibitively expen-sive for datasets of 100,000 points in 200 dimensions. The reason is that PROCLUS is a hill-climbing technique that executes a largenumber of iterations (usually 100 or more) in order to successively improve the current clustering. We attempted to set the numberof iterations to a lower value (first 30, then 60), but it proved to be insufficient and the resulting clusterings were quite inaccurate.We did obtain accurate results with PROCLUS on smaller sets and lower dimensional spaces. However, since it did not scale well, wedecided to drop it from further experiments. "},{"aspect":"problemdef","tweet":" 2. DEFINITION OF AN OPTIMAL PROJEC-TIVE CLUSTER  Let p = (p1; : : : ; pd) be a point in Rd. We use [d] to denotethe set of the  d coordinate axes. As in previous approaches, weview a projective cluster as a pair  (C; D), where C is a subset ofthe data, and D is a subset of the coordinate axes. We require theset C to be dense in the subspace spanned by D in the followingsense:  jCj must be sufficiently large, and the projection of C on thesubspace spanned by  D must be contained in a hyper-cube of givenside length w. However, we do not make any assumption on thedistribution of  C inside the subspace spanned by D. We give theformal definition below.  DEFINITION 1. Let S be a set of points in Rd. For any 0 ^ ff ^ 1 and w * 0, an ff-dense projective cluster of width w in S isa pair  (C; D), C ` S, D ` [d], such that  (1) C is ff-dense, i.e. jCj * ffjSj; (2) 8i 2 D, maxp2C pi \\Gamma  minq2C qi ^ w; (3) 8i 2 [d] n D, maxp2C pi \\Gamma  minq2C qi ? w.  We define the dimensionality of a projective cluster to be jDj.We say that  D is the set of bounded dimensions, and [d] n D is theset of unbounded dimensions. For example,  D = f1; 2g for C1in Figure 1(a). The third condition ensures that  D is the maximalset of bounded dimensions, i.e., D contains all the dimensions i forwhich C projects onto an interval of length at most w. A projectivecluster  (C; D) of width w has a natural geometric interpretation asan axis-aligned box  BC;D = [l1; h1] \\Theta  [l2; h2] \\Theta  \\Delta  \\Delta  \\Delta  \\Theta  [ld; hd],where li = \\Gamma 1 and hi = 1 if i 62 D, and li = minp2C pi and hi = maxp2C pi if i 2 D; see Figure 1(b). By definition, a point p 2 S is in C if and only if p is contained in BC;D. In describingour algorithm, we will also use a slightly different geometric object:  For any p 2 S and D ` [d], let Bp;D = [l1; h1] \\Theta  [l2; h2] \\Theta  \\Delta  \\Delta  \\Delta  \\Theta  [ld; hd], where li = \\Gamma 1 and hi = 1 if i 62 D, and li = pi \\Gamma  wand  hi = pi + w if i 2 D. By definition, for any cluster (C; D)and for any  p 2 C, Bp;D ' BC;D. We say that BC;D has width wand Bp;D has width 2w.Throughout this paper, we assume  w to be fixed. Unless other-wise specified, all the projective clusters we consider have width at  most w. For any 0 ^ ff ^ 1, let Pff denote the set of all ff-denseprojective clusters of width at most  w from S. We want to be able  to compare clusters from the set Pff in order to determine the op-timal one. Clearly, we must define a quality measure on  Pff, andwe must do so by taking into account properties that characterize  the \"natural\" clusters occurring in real data. Intuitively, we wouldlike each cluster to have as many points as possible, in order to be statistically relevant. We would also like each cluster to have aslarge a dimensionality as possible, since more bounded dimensions encode more amount of correlation in the data. However, it is easyto see that the two objectives are at odds. Consider the two extreme cases. If C = S, then jDj is likely to be 0 or a very small integer;such a cluster does not help us mine any useful information from the data. On the other hand, if jDj = d then S probably consistsof just a few points, since data tends to be very sparse in the fulldimensional space; again, such a cluster is useless. The solution isto specify a trade-off between the number of points and the dimensionality of a cluster. Taking these issues into account, we proposethe following definition for the quality of a projective cluster.  DEFINITION 2. Let S be a set of n points in Rd. Let _ : R \\Theta  R ! R be a function such that _(0; 0) = 0 and _ is monotonicallyincreasing in each argument. We define the quality of a projective  cluster (C; D) to be _(jCj; jDj). For any fixed 0 ^ ff ^ 1, aprojective cluster  (C; D) 2 Pff is _-optimal (or optimal for brevity)if it maximizes _ over Pff.For any 0 ^ fi ! 1, we say that a measure _ is fi-balanced if _(a; b) = _(fia; b + 1) for all a ? 0; b * 0.The projective cluster problem over  Pff is to compute an optimalcluster in Pff under a given fi-balanced measure (note that theremay be more than one optimal projective cluster in the data). For  example, one such measure is _(a; b) = a(1=fi)b. The mono-tonicity requirement for  _ models the fact that we want projectiveclusters (C; D) so that both C and D are maximal. The fi-balancedcondition specifies the tradeoff between the number of points and  the number of dimensions in a cluster. Intuitively, for any projec-tive cluster  (C; D), we are willing to throw away at most a (1 \\Gamma  fi)fraction of the points in  C in order to add one more dimensionto D. This condition could be relaxed as follows: _ is (fi1; fi2)-balanced,  fi1 ^ fi2, if _(fi1a; b + 1) ^ _(a; b) ^ _(fi2a; b + 1), 8a ? 0; b * 0. For simplicity, we let fi = fi1 = fi2. "},{"aspect":"solution","tweet":" 3. APPROXIMATING AN OPTIMAL PRO-JECTIVE CLUSTER  In this section we describe our algorithm for approximating anoptimal projective cluster. Intuitively, our approach is as follows. Let (C\\Lambda ; D\\Lambda ) 2 Pff be an optimal projective cluster, and let _\\Lambda  = _(jC\\Lambda j; jD\\Lambda j) denote its quality. We guess (via random sampling)a seed  p 2 C\\Lambda  and then determine the set D\\Lambda  (see below). Let C = S \" Bp;D\\Lambda . Then (C; D\\Lambda ) has at least as many points as (C\\Lambda ; D\\Lambda )and the same dimensionality, which implies that its quality is at  least _\\Lambda . The only problem, however, is that (C; D\\Lambda ) has width 2w instead of w, and is in this sense an infeasible solution (recallthat we formulated our projective cluster problem for clusters of  fixed width w). From a practical point of view, this means that (C; D\\Lambda ) may attract some nearby points that belong to other clustersor are outliers. However, a point is attracted only if it is close to  the seed along each bounded dimension in D\\Lambda . This is unlikely tohappen for any but a very small number of data points that do not belong to the cluster. Hence, it is reasonable to accept (C; D\\Lambda ) asan approximation of the optimal cluster. We say that  (C; D\\Lambda ) is a 2-approximate solution because it has width 2w, instead of w.The only non-trivial step in the above approach is determining  the set of dimensions D\\Lambda . Note that D\\Lambda  depends on C\\Lambda , which wedo not know. However, we show below that  D\\Lambda  can be determined  DOC(S, ff, fi)  r = log(2d)= log(1=2fi); m = (2=ff)r ln 4;  1. for i = 1 to 2=ff2. Choose  p 2 S uniformly at random.3. for j = 1 to m4. Choose  X ` S of size r uniformly at random;5. D = fk j jqk \\Gamma  pkj ^ w; 8q 2 Xg;6. C = S \" Bp;D;7. if (  jCj ! ffjSj) then8. (C; D) = (;; ;);endfor  endfor9. return cluster  (CO; DO) that maximizes _(jCj; jDj)over all computed clusters  (C; D).  Figure 2: Algorithm for approximating an optimal projectivecluster.  if we know only a small subset X ` S, called a discriminating set.The name comes from the fact that we can use this set to discriminate between the bounded and unbounded dimensions of a cluster.More exactly, given a projective cluster  (C; D) and a point p 2 C, adiscriminating set for (C; D) with respect to p satisfies the follow-ing two conditions:  (a) 8i 2 D and q 2 X, jqi \\Gamma  pij ^ w; (b) 8i 62 D, 9q 2 X such that jqi \\Gamma  pij ? w. If we know p 2 C\\Lambda  and a discriminating set X with respect to p,we determine  D\\Lambda  as follows: If jqi \\Gamma  pij ^ w for all q 2 X,then i 2 D\\Lambda , otherwise i 2 ([d] n D\\Lambda ). We guess both p and X via random sampling. The optimality of (C\\Lambda ; D\\Lambda ) will implythat it admits a small discriminating set of size  O(log d). This isimportant for the efficiency of our method, which has polynomial  dependency on d (recall that other density-based approaches areexponential in the dimensionality).  Figure 2 describes our Monte Carlo algorithm, called DOC(S, ff, fi). As is the case with many Monte Carlo methods, the algo-rithm itself is extremely simple. It consists of repeatedly choosing  pand X via random sampling, computing the corresponding cluster (C; D) as outlined above, and then reporting the best found clus-ter. The challenge consists in proving that it works correctly. The  values for r (size of random sample) and m (number of inner it-erations) are chosen according to the theoretical analysis, which is provided in Section 3.1. 3.1 Correctness and running time  We prove the correctness of our algorithm using the followinglemmas.  LEMMA 1. Let ff be such that  max (C;D)2Pff  _(jCj; jDj) = max  (C;D)2Pff0  _(jCj; jDj); 8ff0 ^ ff; (\\Lambda )  Then, any optimal cluster (C\\Lambda ; D\\Lambda ) of Pff satisfies the followingproperty:  8i 62 D\\Lambda ; 8a 2 R; jfp 2 C\\Lambda  j pi 2 [a; a + w]gj ^ fijC\\Lambda j: We say that (C\\Lambda ; D\\Lambda ) is fi-balanced.  The proof follows immediately from the fact that (C\\Lambda ; D\\Lambda ) isoptimal and the fact that the quality measure is  fi-balanced. It iseasy to see that any ff ^ fid satisfies equation (\\Lambda ), but we expect ffto be much larger in practice.  LEMMA 2. Let 1=(4d) ^ fi ^ 1=2. Let pi be the point chosenduring the  ith outer iteration of DOC(S, ff, fi), and let Xj be the  set chosen during the corresponding jth inner iteration of DOC(S, ff, fi). If pi 2 C\\Lambda  then, with probability at least 3=4, there exists jsuch that  Xj is discriminating for (C\\Lambda ; D\\Lambda ) with respect to pi.  PROOF. In the following, (a) and (b) refer to the two conditionsin the definition of a discriminating set. Let  j be a fixed inner iter-ation. Define Xjk (resp., pik) to be projection of Xj (resp., pi) ontodimension k. If Xj ` C\\Lambda  then Xj satisfies (a). Hence  PrfXj satisfies (a) and (b)g * PrfXj ` C\\Lambda g(1 \\Gamma  PrfXj violates (b)jXj ` C\\Lambda g):  Since jC\\Lambda j * ffjSj, PrfXj ` C\\Lambda g * ffr. Xj violates (b) if andonly if there exists  k 62 D\\Lambda  such that Xjk ` [pik\\Gamma w; pik+w]. Recallthat (C\\Lambda ; D\\Lambda ) is fi-balanced. This implies that for any k 62 D\\Lambda , jC\\Lambda  \" [pik \\Gamma  w; pik + w]j ^ 2fijC\\Lambda j. We deduce  PrfXjsatisfies (a) and (b)g * ffr(1 \\Gamma  d(2fijC\\Lambda j=jSj)r) * ffr=2; which implies  Prf8j; Xj not discriminating for pig ^ (1 \\Gamma  ffr=2)m ^ 1=4 (we used the fact that fi * 1=(4d) to deduce r * 1, and so ffr=2 * ffr=2r). Hence, with probability at least 3=4, there exists j suchthat  Xj is discriminating for (C\\Lambda ; D\\Lambda ) with respect to pi.  LEMMA 3. Let pi be the point chosen during the ith outer it-eration of the procedure DOC(  S,ff, fi). Then with probability atleast 3=4 there exists i such that pi 2 C\\Lambda .  PROOF. Since jC\\Lambda j * ffjSj, we deduce  Prf8i; pi 62 C\\Lambda g ^ (1 \\Gamma  ff)2=ff ^ e\\Gamma 2 ! 1=4:  Lemmas 2 and 3 imply that the probability of success of ouralgorithm is at least  3=4 \\Delta  3=4 ? 1=2. We thus conclude with thefollowing.  THEOREM 1. Let _ be a fi-balanced quality measure, 1=(4d) ^ fi ! 1=2, and let 0 ! ff ! 1. Then, with probability at least 1=2,DOC(  S, ff, fi) returns a 2-approximate solution.  By a standard technique, the probability of success can be boostedto 1\\Gamma 1=2m by repeating DOC(S, ff, fi) m times and reporting thebest overall cluster. In our experiments we noticed that one or two  calls to DOC were sufficient to generate a good projective cluster.  REMARK 1. The approximation guarantee can be improved ifwe have information on the distribution of cluster points inside the subspace. Let C\\Lambda D\\Lambda  denote the projection of C\\Lambda  onto the subspacespanned by  D\\Lambda . Then C\\Lambda D\\Lambda  lies in a jD\\Lambda j-dimensional hypercube H of size w. If, for example, we know that C\\Lambda D\\Lambda  has a normal dis-tribution with mean at the center of  H and with known standarddeviation, then we can easily modify DOC to compute a  (1 + \")-approximate solution, for any \" ? 0. However, since we cannotalways rely on having such a priori information on the data, we  presented the algorithm in its most general form, which is indepen-dent of data distribution.  Running time. During each iteration we compute D in O(d)time and  C in O(nd) time. The total number of iterations is m = (2d)C ln 4, where C is a constant that depends only on ff and fi(more precisely,  C = log(2=ff)= log(1=(2fi))). Hence, the overallrunning time is O(ndC+1).  FASTDOC(S, ff, fi)  r = log(2d)= log(1=2fi); m = minfMAXIT ER; (2=ff)r ln 4g;  1. DM = ;;2. for  i = 1 to 2=ff3. Choose  p 2 S uniformly at random.4. for j = 1 to m5. Choose  X ` S of size r uniformly at random;6. D = fk j jqk \\Gamma  pkj ^ w; 8q 2 Xg;7. if (  jDj * jDM j) then8. DM = D;9. if (  jDM j * d0) then10. go to 11;  endforendfor 11. C = S \" Bp;DM ;12. return cluster  (C; DM ).  Figure 3: Heuristic method for the projective cluster problem.  3.2 Speeding up the algorithm  From a theoretical point of view the DOC algorithm is very effi-cient, since its running time is linear in the size of data and polynomial in the dimensionality. However, it is important to note that thealgorithm scans the entire data during each inner iteration to compute the set C. This process can be very time consuming, since thepoints are high dimensional and the data files are usually large (in our experiments, they are in the range of 10-100M). Therefore, wepropose a few simple heuristics to reduce the number of data scans and speed up the algorithm. These heuristics come with a price: welose some of the quality guarantees proven in the previous subsection. However, as we discuss below, it is likely that the computedclusters are relevant in most practical applications.  We speed up our algorithm as follows. During each inner itera-tion, we only compute the set  D. After the m inner iterations areexecuted, let DM be the largest set among the m sets of dimen-sions computed. We compute  C = S \" Bp;DM . Thus, we now readthe data only once per outer iteration. We can no longer guarantee  (in a probabilistic sense) that we return a cluster of quality at least _\\Lambda . However, we can prove using arguments similar to the proof ofTheorem 1 that we return a cluster of size at most  2w which is ff-dense and has large dimensionality (at least as large as the highest  dimensionality of an ff-dense fi-balanced cluster). For most appli-cations requiring pattern discovery or data indexing, a projective cluster with these properties is good enough. We can further re-duce the amount of computation as follows. Given a user-specified threshold d0, once we discover a set D with jDj * d0, we computethe corresponding set  C and return (C; D). Moreover, in order toreduce CPU time, we upperbound the number of inner iterations  m by a value MAXITER. The resulting heuristic method, calledFastDOC, is described in Figure 3. This approach no longer guarantees an ff-dense cluster, but it is likely that it returns one withsufficiently many points.  Note that we now compute C outside the iterations. Hence, wescan the data only once for each computed cluster. In addition, we also have to access the data in order to choose the random samples.However, we can choose all random samples in one scan and store them in memory, since the size of each sample is small, and thenumber of samples  m is at most MAXITER. In our experiments,we choose MAXITER to be  d2 ! 106 when d is of the order ofhundreds of dimensions. Hence, this approach requires two data  scans per computed cluster. However, we can reduce the I/O com-plexity even further, by pipelining the computation across clusters.  Recall that we call FastDOC repeatedly, in order to compute theprojective clusters one by one. During the computation of the  ithcluster, the data is scanned in step 11 of the algorithm in order to  compute the set C. While doing so, we also choose and store therandom samples that will be used in the computation of the  (i+1)stcluster. Thus, during the computation of the (i + 1)st cluster, steps3 and 5 no longer require accessing the data on disk. The only exception is the computation of the first cluster, which does need toselect its random samples directly from the disk. We conclude that the overall number of data scans we need is exactly one more thanthe number of computed clusters. "},{"aspect":"expcomparison","tweet":" 4.1 Comparison with ORCLUS  Because ORCLUS requires all projective clusters to exist in thesame number of dimensions, we generate projective clusters under  this additional restriction. Unless otherwise specified, the clusterdimensionality is  40. Note that this is a fairly strong restriction,since real data is unlikely to exhibit subspace patterns of the same  dimensionality. However, DOC is also able to discover clusters ofdifferent dimensionality. In fact, our method is completely oblivious to the dimensionality in which the clusters have been generated,and it decides for itself the appropriate dimensionality of each cluster. In order to test its accuracy in this respect, the results we reportfor DOC are averaged over two data files: one in which all clusters have dimensionality 40, and one in which each cluster has differentdimensionality, and the average cluster dimensionality is 40. ORCLUS is only tested on the first set. Moreover, all reported resultsare averaged over three runs on the same file(s), in order to offset the effects of randomization. We note that DOC returned qualita-tively similar results on both types of files, i.e. the averaging over different files does not make any of the two cases look better.The accuracy results that we report throughout this section are computed as follows. For each output cluster i, we identify the in-put cluster  j with which it shares the largest number of points. Wesay that output cluster  i corresponds to input cluster j, and that allpoints in their common intersection are labeled correctly. All the  other points of output cluster i are labeled incorrectly. The outputand input outlier sets are treated similarly. We then report the percentage of points that are correctly labeled by each algorithm. Wenoticed a rather surprising instability of ORCLUS over the MGsets and N-set. In each case, one or two runs returned an accurateclustering, while the remaining runs computed clusterings with significant error (mostly, a large percentage of cluster points were la0.2 0.40 0.2 0.4 0.6 0.8  1  Accuracy  ORCLUSDOC  N-set MG2-set MG4-set U-set Figure 4: Dependence of accuracy on data distribution.  beled outliers). By contrast, ORCLUS was very stable on the U-set, on which it consistently returned a good clustering. DOC did not exhibit any instability over the different runs or data distribu-tions. This confirmed our intuition that, since we do not use any information on the data distribution, the algorithm should be sta-ble under any distribution. ORCLUS makes implicit assumptions on the data distribution, since it uses measures of data correlationwhen deciding which dimensions to choose for each cluster. However, this does not seem to fully explain its instability. This issuemay be worthy of further examination, since it has the potential for improving the performance of ORCLUS.  We also report our results on the accuracy of the two methodswhen the cluster sizes vary significantly. The x-axis in Figure 5 is  labeled by the ratio between the sizes of the smallest and largestclusters. The smaller the ratio, the more imbalance there is in the cluster sizes. All data sets we used were U-sets, since ORCLUSproved stable on them. As expected, DOC does not miss the small clusters and has very high accuracy (the accuracy is not 1, since5 points are mislabeled in some cases, but they are statistically insignificant). The accuracy of ORCLUS is also reasonably good.However, a closer look shows that in fact ORCLUS entirely misses one or two small clusters in each case. The fact that few points aremislabeled is due to the fact that those clusters do not have many points to begin with. Parameter Choices. In all the experiments reported so far, weset  ff = 0:1, fi = 0:25, and w = 15. We used the followingconsiderations and guidelines for choosing these values.  One heuristic for choosing w is as follows: For a data point pi,let  qi denote its nearest neighbor. Let wi =  Pd  j=1 jp  i j \\Gamma  q  i jj=d bethe average distance between  pi and qi along one dimension. Thenone can choose w = C \\Delta   Pn  i=1 w  i=n for some small constant C.  We also use this strategy with good results on our real dataset.Intuitively,  ff can be viewed as a minimum required cluster den-sity. The smaller we choose  ff, the more likely it is that we discovera particular input cluster. However, if  ff is too small we may exe-cute too many outer iterations. On the other hand, if  ff is too large,we can miss an input cluster entirely. Figure 6 illustrates how  ffinfluences the accuracy of the result (we fix fi = 0:25). In these ex-periments output clusters are not required to be disjoint, i.e., we do  not eliminate the points that belong to a computed cluster. Hence,when  ff is too large, we miss smaller clusters. We also ran the ex0 0.1 0.2 0.3 0.4 0.5 0.6 0.70.8 0.95  1.1 Accuracy  ORCLUSDOC  Figure 5: Dependence of accuracy on cluster sizes. periments with the requirement that output clusters be disjoint. Inthat case  ff did not influence the accuracy. This can be explained bythe fact that, when we eliminate the points of a computed cluster,  the density of the remaining clusters with respect to the remainingset of points increases.  0.1 0.15 0.2 0.25 0.3 0.35 0.40 0.2 0.4 0.6 0.8  1  Accuracy  ff Figure 6: Dependence of accuracy on ff. Parameter fi represents the user's \"opinion\" on the relative im-portance of points versus number of dimensions in a cluster. Figure 7 illustrates how fi influences the accuracy. As expected, when fi is small, the algorithm chooses more dimensions for a cluster atthe cost of throwing away cluster points. At the other end, when  fi is large (i.e., fi = 0:35), the algorithm merges two input clus-ters. The resulting output cluster has fewer dimensions than any of the original clusters, but it has twice as many points. Hence, inthis case the algorithm favors points over dimensions. However, one could argue that the output is still interesting, as the resultingcluster has  22 dimensions and it contains about 40% of the entiredataset.  The accuracy of the method changes if the cluster width is larger.We report experiments on a dataset for which the maximum cluster width is 40. We run the algorithm with w = 25, ff = 0:1,and  fi = 0:4. Since the width is large, we need a larger fi toavoid choosing too many extra dimensions in the cluster (recall,  0.1 0.15 0.2 0.25 0.3 0.350 0.2 0.4 0.6 0.8  1  Accuracy  fi Figure 7: Dependence of accuracy on fi. though, that Theorem 1 requires fi ! 1=2). Even for fi = 0:4, thealgorithm generates a corresponding output cluster with more dimensions and fewer points. Therefore, to avoid having many clus-ter points thrown away as outliers, we generate  15 output clusters(three times the number of input clusters). Only  0:1% of clusterpoints are mis-labeled outliers by the algorithm.  In Figure 8 we show a snapshot of the Confusion Matrix, de-fined as follows: entry  (i; j) is equal to the number of data pointsassigned to output cluster  i, that were generated as part of inputcluster j. The last row and column are output, resp. input, out-liers. Each output cluster contains about  5 \\Gamma  10% of the data, andmost clusters have at least 85% of dimensions in common with thecorresponding input clusters. The lowest such percentage is  50%.Thus, although the algorithm does not discover the original clusters, it nevertheless detects clusters that are interesting and likely tobe useful.  In A B C D E OOut  1 5842 0 0 0 0 04 230 13944 0 0 0 0 5 0 0 347 13177 1242 06 1835 0 0 0 0 0 15 2084 11247 0 0 0 0  O 7 14 65 30 10 5000  Figure 8: Confusion Matrix for large cluster width (snapshot).  Influence of dimensionality and data size. Finally, westudy how the accuracy of our method is influenced by the cluster dimensionality and the size of the dataset.Figure 9 shows how the accuracy varies with the average number of dimensions in a cluster. Each point on the graph corresponds to adifferent file containing five clusters  (Ci; Di) so that  P  i jDij=5 isapproximately the x-value on the graph. For each file we report the  most accurate result out of three executions. However, the varia-tion in the accuracy across different executions on the same set was  generally insignificant. The experiments show that the algorithmworks very well unless the average value of  jDj is very small. Fig-ure 10 shows that the accuracy does not change when the size of the  10 20 30 40 50 60 70 80 90 1000.4 0.5 0.6 0.7 0.8 0.9  1 1.1  Avg. no. of dims in cluster Accuracy  Figure 9: Dependence of accuracy on average number of cluster di-mension.  10 20 30 40 50 60 70 80 90 1000.4 0.5 0.6 0.7 0.8 0.9  1 1.1  Number of points Accuracy  Figure 10: Dependence of accuracy on number of points.  training data decreases. The method takes between 2-30 secondsto generate a projective cluster, depending on the input size and the outcome of coin flips. The variation is due to randomization.For the right choice of parameters  ff and fi (see above), the methodusually computes a projective cluster in less than  5 seconds. "}]}