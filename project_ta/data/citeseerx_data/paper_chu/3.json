{"user_name":" Density-Based Clustering in Spatial Databases: The Algorithm GDBSCAN and its Applications ","user_timeline":[{"aspect":"abstract","tweet":" Abstract. The clustering algorithm DBSCAN relies on a density-based notion of clusters and is designed to discover clusters of arbitrary shape as well as to distinguish noise. In this paper, we generalize this algorithm in two important directions. The generalized algorithm - called GDBSCAN - can cluster point objects as well as spatially extended objects according to both, their spatial and their non-spatial attributes. In addition, four applications using 2D points (astronomy), 3D points (biology), 5D points (earth science) and 2D polygons (geography) are presented, demonstrating the applicability of GDBSCAN to real world problems. Keywords: Clustering Algorithms, Spatial Databases, Efficiency, Applications. "},{"aspect":"expanalysis","tweet":" 5.1 Analytical Evaluation The runtime of GDBSCAN obviously is O(n * runtime of a neighborhood query): n objects are visited and exactly one neighborhood query is performed for each of them. The number of neigh- borhood queries cannot be reduced since a clusterId for each object is required. Thus, the overall runtime depends on the performance of the neighborhood query. Fortunately, the most interest- ing neighborhood predicates are based on spatial proximity - like distance predicates or intersec- tion - which can be efficiently supported by spatial index structures. Such index structures are as- sumed to be available in a SDBS for efficient processing of several types of spatial queries (Brinkhoff et al., 1994). In the following, we will introduce a typical spatial index, the R*-tree (Beckmann et al., 1990). The R*-tree (see figure 9) generalizes the 1-dimensional B-tree to d-dimensional data spaces, specifically an R*-tree manages k-dimensional hyperrectangles instead of 1-dimension- al keys. An R*-tree may organize extended objects such as polygons using minimum bounding rectangles (MBR) as approximations as well as point objects as a special case of rectangles. The leaves store the MBRs of the data objects and a pointer to the exact geometry of the polygons. Internal nodes store a sequence of pairs consisting of a rectangle and a pointer to a child node. These rectangles are the MBRs of all data or directory rectangles stored in the subtree having the referenced child node as its root. To answer a region query, starting from the root, the set of rectangles intersecting the query region is determined and then their referenced child nodes are searched until the data pages are reached. polygons The height of an R*-tree is O(log n) for a database of n objects in the worst case and a query with a \u201aÄúsmall\u201aÄù query region has to traverse only a limited number of paths in the R*-tree. Since most NPred-neighborhoods are expected to be small compared to the size of the whole database, the average runtime complexity of a single neighborhood query is O(log n). Table 1 lists the runtime complexity of GDBSCAN with respect to the underlying spatial in- dex structure. Without any index support, the runtime of GDBSCAN is O(n 2 ). This does not scale well with the size of the database. But, if we use a spatial index, the runtime is reduced to O (n log n). If we have a direct access to the NPred-neighborhood, e.g. if the objects are organized in a grid, the runtime is further reduced to O(n).  "},{"aspect":"expdata","tweet":""},{"aspect":"background","tweet":" 1. Introduction J√∂rg Sander, Martin Ester, Hans-Peter Kriegel, Xiaowei Xu Institute for Computer Science, University of Munich Oettingenstr. 67, D-80538 M√ºnchen, Germany {sander | ester | kriegel | xwxu}@informatik.uni-muenchen.de Spatial Database Systems (SDBS) (Gueting 1994) are database systems for the management of spatial data, i.e. point objects or spatially extended objects in a 2D or 3D space or in some high dimensional vector space. While a lot of research has been conducted on knowledge discovery in relational databases in the last years, only a few methods for knowledge discovery in spatial databases have been proposed in the literature. Knowledge discovery becomes more and more important in spatial databases since increasingly large amounts of data obtained from satellite images, X-ray crystallography or other automatic equipment are stored in spatial databases. Data mining is a step in the KDD process consisting of the application of data analysis and dis- covery algorithms that, under acceptable computational efficiency limitations, produce a partic- ular enumeration of patterns over the data (Fayyad et al.,1996). Clustering, i.e. grouping the ob- jects of a database into meaningful subclasses, is one of the major data mining methods (Matheus et al., 1993). There has been a lot of research on clustering algorithms for decades but the application to large spatial databases introduces the following new requirements: (1) Minimal requirements of domain knowledge to determine the input parameters, because ap- propriate values are often not known in advance when dealing with large databases. (2) Discovery of clusters with arbitrary shape, because the shape of clusters in spatial databases may be non-convex, spherical, drawn-out, linear, elongated etc. (3) Good efficiency on large databases, i.e. on databases of significantly more than just a few thousand objects. (Ester et al., 1996) present the density-based clustering algorithm DBSCAN. For each point of a cluster its Eps-neighborhood for some given Eps >0 has to contain at least a minimum number of points, i.e. the \u201aÄúdensity\u201aÄù in the Eps-neighborhood of points has to exceed some threshold. DB- SCAN meets the above requirements in the following sense: first, DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it. Second, it dis- covers clusters of arbitrary shape and can distinguish noise, and third, using spatial access meth- ods, DBSCAN is efficient even for large spatial databases. In this paper, we present the algorithm GDBSCAN generalizing DBSCAN in two important ways. First, we can use any notion of a neighborhood of an object if the definition of the neigh- borhood is based on a binary predicate which is symmetric and reflexive. For example, when clustering polygons, the neighborhood may be defined by the intersect predicate. Second, in- stead of simply counting the objects in the neighborhood of an object, we can use other mea- sures, e.g. considering the non-spatial attributes such as the average income of a city, to define the \u201aÄúcardinality\u201aÄù of that neighborhood. Thus, the generalized GDBSCAN algorithm can cluster point objects as well as spatially extended objects according to both, their spatial and their non- spatial attributes. Furthermore, we present four applications using 2D points (astronomy), 3D points (biology), 5D points (earth science) and 2D polygons (geography) demonstrating the ap- plicability of GDBSCAN to real world problems. The rest of the paper is organized as follows. We discuss well-known clustering algorithms in section 2 evaluating them according to the above requirements. In section 3, we present our den- sity-based notion of clusters and section 4 introduces the algorithm GDBSCAN which discovers such clusters in a spatial database. In section 5, we present an analytical as well as an experimen- tal evaluation of the effectiveness and efficiency of GDBSCAN. Furthermore, a comparison with the well-known clustering algorithms CLARANS and BIRCH is performed. In section 6, four applications of GDBSCAN are discussed and section 7 concludes with a summary and some directions for future research. 2. Related Work on Clustering Algorithms Two main types of clustering algorithms can be distinguished (Kaufman and Rousseeuw, 1990): partitioning and hierarchical algorithms. Partitioning algorithms construct a partition of a data- base D of n objects into a set of k clusters. The partitioning algorithms typically start with an ini- tial partition of D and then use an iterative control strategy to optimize an objective function. Each cluster is represented by the gravity center of the cluster (k-means algorithms) (MacQueen, 1967) or by one of the objects of the cluster located near its center (k-medoid algo- rithms) (Vinod, 1969). Consequently, a partition is equivalent to a voronoi diagram and each cluster is contained in one of the voronoi polygons. Thus, the shape of all clusters found by a par- titioning algorithm is convex (Kaufman and Rousseeuw, 1990) which is very restrictive for many applications. Ng and Han (Ng and Han, 1994) explore partitioning algorithms for mining in spatial databas- es. An algorithm called CLARANS (Clustering Large Applications based on RANdomized Search) is introduced which is an improved k-medoid method restricting the huge search space using two additional user-supplied parameters. Compared to former k-medoid algorithms, CLARANS is more effective and more efficient. Our experimental evaluation indicates that the runtime of a single call of CLARANS is close to quadratic (see table 1, section 5). Consequently, it is possible to run CLARANS efficiently on databases of some thousands of objects, but not for really large n. Methods to determine the \u201aÄúnatural\u201aÄù number k nat of clusters in a database are also discussed (Ng and Han, 1994). They propose to run CLARANS once for each k from 2 to n. For each of the discovered clusterings the silhouette coefficient (Kaufman and Rousseeuw, 1990) is calculated, and finally, the clustering with the maximum silhouette coefficient is chosen as the \u201aÄúnatural\u201aÄù clustering. Obviously, this approach is very expensive for large databases, because it implies O(n) calls of CLARANS. Hierarchical algorithms create a hierarchical decomposition of a database D. The hierarchical decomposition is represented by a dendrogram, a tree that iteratively splits D into smaller sub- sets until each subset consists of only one object. In such a hierarchy, each level of the tree repre- sents a clustering of D. The dendrogram can either be created from the leaves up to the root (ag- glomerative approach) or from the root down to the leaves (divisive approach) by merging or dividing clusters at each step. In contrast to partitioning algorithms, hierarchical algorithms do not need k as an input parameter. However, a termination condition has to be defined indicating when the merge or division process should be terminated, e.g. the critical distance D min between all the clusters of D. Alternatively, an appropriate level in the dendrogram has to be selected manually after the creation of the whole dendrogram. The single-link method is a commonly used agglomerative hierarchical clustering method. Different algorithms for the single-link method have been suggested (e.g. (Sibson, 1973), (Jain and Dubes, 1988), (Hattori and Torii, 1993)). We will only describe the basic idea. The single-link method starts with the disjoint clustering obtained by placing every object in a unique cluster. In every step the two closest clusters in the current clustering are merged until all points are in one cluster. The runtime of algorithms which construct the single-link hierarchy depends on the technique for retrieving nearest neighbors. Without any spatial index support (see section 5.1 for a brief introduction into spatial access methods) for nearest neighbor queries, the runtime complexity of single-link algorithms is O(n 2 ). This runtime can be significantly improved when using multidimensional hash- or tree-based index structures (see (Murtagh, 1983)). Unfortunately, the runtime of most of the above algorithms is very high on large databases. Therefore, some focusing techniques have been proposed to increase the efficiency of clustering algorithms: (Ester et al., 1995) presents an R*-tree based focusing technique (1) creating a sam- ple of the database that is drawn from each R*-tree data page and (2) applying the clustering algorithm only to that sample. In (Zhang et al., 1997), compact descriptions of subclusters, i.e. Clustering Features (CF), are incrementally computed containing the number of points, the linear sum and the square sum of all points in the cluster. The CF-values are organized in a balanced tree. In the first phase, BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies) performs a linear scan of all data points and builds a CF-tree, i.e. a balanced tree with branching factor B and threshold T. A nonleaf node represents a cluster consisting of all the subclusters represented by its entries. A leaf node has to contain at most L entries and the diameter of each entry in a leaf node has to be less than T. A point is inserted by inserting the corresponding CF-value into the closest leaf of the tree. If an entry in the leaf can absorb the new point without violating the threshold condition, then the CF-values for this entry are updated, otherwise a new entry in the leaf node is created. In an optional phase 2, the CF-tree can be further reduced until a desired number of leaf nodes is reached. In phase 3, an arbitrary clustering algorithm, e.g. CLARANS, is used to cluster the CF-values of the leaf nodes of the CF-tree. The efficiency of BIRCH is similar to the R*-tree based focusing technique cited above. Experiments with synthetic data sets show that the clustering quality using BIRCH in combina- tion with CLARANS is even higher than the quality obtained by using CLARANS alone.  "},{"aspect":"expintro","tweet":" 5. Performance Evaluation In this section, we evaluate the performance of GDBSCAN. In section 5.1, we discuss the per- formance of GDBSCAN with respect to the underlying spatial index structure. In section 5.2, an experimental evaluation of GDBSCAN and a comparison with the well-known clustering algo- rithms CLARANS and BIRCH is presented. "},{"aspect":"problemdef","tweet":" 3. Density-Connected Sets In section 3.1, we present \u201aÄúdensity-connected sets\u201aÄù which are a significant generalization of \u201aÄúdensity-based clusters\u201aÄù (see Ester et al. 1996), and indicate some important specializations of density-connected sets in section 3.2 illustrating the high expressiveness of this concept. In the following, we assume a spatial database D to be a finite set of objects characterized by spatial and non-spatial attributes. The spatial attributes may represent, e.g., points or spatially extended objects such as polygons in some d-dimensional space S. The non-spatial attributes of an object in D may represent additional properties of a spatial object, e.g., the unemployment rate for a community represented by a polygon in a geographic information system. 3.1 A Generalized Definition of Density Based Clusters The key idea of a density-based cluster is that for each point of a cluster its Eps-neighborhood for some given Eps > 0 has to contain at least a minimum number of points, i.e. the \u201aÄúdensity\u201aÄù in the Eps-neighborhood of points has to exceed some threshold (Ester et al. 1996). This idea is il- lustrated by the sample sets of points depicted in figure 1. In these examples, we can easily and unambiguously detect clusters of points and noise points not belonging to any of those clusters, mainly because we have a typical density of points inside the clusters which is considerably higher than outside of the clusters. Furthermore, the density within the areas of noise is lower than the density in any of the clusters. This idea of \u201aÄúdensity-based clusters\u201aÄù can be generalized in two important ways. First, we can use any notion of a neighborhood instead of an Eps-neighborhood if the definition of the neigh- borhood is based on a binary predicate which is symmetric and reflexive. Second, instead of sim- ply counting the objects in a neighborhood of an object we can as well use other measures to de- fine the \u201aÄúcardinality\u201aÄù of that neighborhood. database 1 database 2 database 3 Definition 1: (neighborhood of an object) Let NPred be a binary predicate on D which is reflex- ive and symmetric, i.e., for all p, q \u201aàà D: NPred(p, p) and, if NPred(p, q) then NPred(q, p). Then the NPred-neighborhood of an object o \u201aàà D is defined as N NPred (o) = {o\u201aÄô \u201aàà D| NPred(o, o\u201aÄô)}. The definition of a cluster in (Ester et al., 1996) is restricted to the special case of a distance based neighborhood, i.e., N Eps (o) = {o\u201aÄô \u201aàà D| |o - o\u201aÄô| \u201aâ§ Eps}. A distance based neighborhood is a natural notion of a neighborhood for point objects, but if clustering spatially extended objects such as a set of polygons of largely differing sizes it may be more appropriate to use neighbor- hood predicates like intersects or meets for finding clusters of polygons. Although in many applications the neighborhood predicate will be defined by using only spa- tial properties of the objects, the formalism is in no way restricted to purely spatial neighbor- hoods. We can as well use non-spatial attributes and combine them with spatial properties of ob- jects to derive a neighborhood predicate (see application 4 in section 6.4). Another way to take into account the non-spatial attributes of objects is as a kind of \u201aÄúweight\u201aÄù when calculating the \u201aÄúcardinality\u201aÄù of the neighborhood of an object. To keep things as simple as possible, we will not introduce a weight function operating on objects, but a weighted cardinality function wCard for sets of objects. The \u201aÄúweight\u201aÄù of a single object o can then be expressed by the weighted cardinality of the singleton containing o, i.e. wCard({o}). Definition 2: (MinWeight of a set of objects) Let wCard be a function from the powerset of the Database D into the non-negative Real Numbers, wCard: 2 D \u201aÜí \u201aÑú \u201aâ\u20220 and MinCard be a positive real number. Then, the predicate MinWeight for a set S of objects is defined to be true iff wCard(S) \u201aâ\u2022 MinCard. Figure 1. Sample databases The expression wCard(S) \u201aâ\u2022 MinCard generalizes the condition | N Eps (o) | \u201aâ\u2022 MinPts in the def- inition of density-based clusters where cardinality is just a special case of a wCard function. There are numerous possibilities to define wCard(S) for subsets of the database D. Simply sum- ming up the values of some non-spatial attribute for the objects in S is another example of a wCard function. E.g., if we want to cluster objects represented by polygons and if the size of the objects should be considered to influence the \u201aÄúdensity\u201aÄù in the data space, then the area of the polygons could be used as a weight for these objects. A further possibility is to sum up a value derived from several non-spatial attributes, e.g. by specifying ranges for some non-spatial at- tribute values of the objects (i.e. a selection condition). We can realize the clustering of only a subset of the database D by attaching a weight of 1 to objects that satisfy the selection condition and a weight of 0 to all other objects. Note that using non-spatial attributes as a weight for objects one can \u201aÄúinduce\u201aÄù different densities, even if the objects are equally distributed in the space of the spatial attributes. Note also that by means of the wCard function the combination of a clustering with a selection on the database is possible, i.e., performing a selection \u201aÄúon the fly\u201aÄù while clus- tering the database. This may be more efficient than performing the selection first under certain circumstances because GDBSCAN can use existing spatial index structures (see section 5.1). We can now define density-connected sets, analogously to the definition of density-based clusters in (Ester et al. 1996), in a straightforward way (see also Ester et al. 1997). A naive approach could require for each object in a density-connected set that the weighted cardinality of the NPred-neighborhood of that object has at least a value MinCard. However, this approach fails because there may be two kinds of objects in a density-connected set, objects in- side (core object) and objects \u201aÄúon the border\u201aÄù of the density-connected set (border objects). In general, an NPred-neighborhood of a border object has a significantly lower wCard than an NPred-neighborhood of a core object. Therefore, we would have to set the value MinCard to a relatively low value in order to include all objects belonging to the same density-connected set. This value, however, will not be characteristic for the respective density-connected set - particu- larly in the presence of noise objects. Therefore, for every object p in a density-connected set C there must be an object q in C so that p is inside of the NPred-neighborhood of q and the weight- ed cardinality wCard of NPred(q) is at least MinCard. We also require the objects of the set C to be somehow \u201aÄúconnected\u201aÄù to each other. This idea is elaborated in the following definitions and illustrated by 2D point objects by using a distance based neighborhood for the points and cardi- nality as wCard function. Definition 3: (directly density-reachable) An object p is directly density-reachable from an object q with respect to NPred, MinWeight if 1) p \u201aàà N NPred (q) and 2) MinWeight(N NPred (q)) = true (core object condition). p: border object q: core object Obviously, directly density-reachable is symmetric for pairs of core objects. In general, how- ever, it is not symmetric if one core object and one border object are involved. Figure 2 shows the asymmetric case. Definition 4: (density-reachable) An object p is density-reachable from an object q with respect to NPred and MinWeight if there is a chain of objects p 1 , ..., p n , p 1 = q, p n = p such that for all i=1, ..., n: p i+1 is directly density-reachable from p i with respect to NPred and MinWeight. Density-reachability is a canonical extension of direct density-reachability. This relation is transitive, but it is not symmetric. Figure 3 depicts the relations of some sample objects and, in particular, the asymmetric case. Although not symmetric in general, it is obvious that density- reachability is symmetric for core objects because a chain from q to p can be reversed if also p is a core object. Two border objects of the same density-connected set C are possibly not density reachable from each other because the core objects condition might not hold for both of them. However, for a density-connected set C we require that there must be a core object in C from which both bor- der objects of C are density-reachable. Therefore, we introduce the notion of density-connectiv- ity which covers this relation of border objects. Definition 5: (density-connected) An object p is density-connected to an object q with respect to NPred, MinWeight if there is an object o such that both, p and q are density-reachable from o with respect to NPred, MinWeight. p q Figure 2. Core objects and border objects p q p directly densityreachable from q q not directly densityreachable from p Density-connectivity is a symmetric relation. For density reachable objects, the relation of density-connectivity is also reflexive (c.f. figure 3). p densityreachable from q q not densityreachable from p Now, a density-connected set is defined to be a set of density-connected objects which is max- imal with respect to density-reachability. Definition 6: (density-connected set) A density-connected set C with respect to NPred, MinWeight in D is a non-empty subset of D satisfying the following conditions: 1) Maximality: For all p, q \u201aàà D: if p \u201aààC and q is density-reachable from p with respect to NPred, MinWeight, then q \u201aààC. 2) Connectivity: For all p,q \u201aàà C: p is density-connected to q with respect to NPred, MinWeight. Note that a density-connected set C with respect to NPred and MinWeight contains at least one core object: since C contains at least one object p, p must be density-connected to itself via some object o (which may be equal to p). Thus, at least o has to satisfy the core object condition. Con- sequently, the NPred-Neighborhood of o has to satisfy MinWeight. The following lemmata are important for validating the correctness of our clustering algo- rithm. Intuitively, they state the following. Given NPred and MinWeight, we can discover a den- sity-connected set in a two-step approach. First, choose an arbitrary object from the database sat- isfying the core object condition as a seed. Second, retrieve all objects that are density-reachable from the seed obtaining the density-connected set containing the seed. Lemma 1: Let p be an object in D and MinWeight(N NPred (p)) = true. Then the set O = {o \u201aààD | o is density-reachable from p with respect to NPred, MinWeight} is a density-connected set with respect to NPred, MinWeight. p q Figure 3. Density-reachability and density-connectivity p and q densityconnected to each other by o Proof: 1) O is non-empty: p is a core object by assumption. Therefore p is density-reachable from p. Then p is in O. 2) Maximality: Let q 1 \u201aààO and q 2 be density-reachable from q 1 with re- spect to NPred, MinWeight. Since q 1 is density-reachable from p and density-reachability is tran- sitive with respect to NPred, MinWeight, it follows that also q 2 is density-reachable from p with p o q respect to NPred, MinWeight. Hence, q 2 \u201aààO. 3) Connectivity: All objects in O are density-con- nected via object p. \u201aùè Furthermore, a density-connected set C with respect to NPred, MinWeight is uniquely deter- mined by any of its core objects, i.e., each object in C is density-reachable from any of the core objects of C and, therefore, a density-connected set C contains exactly the objects which are den- sity-reachable from an arbitrary core object of C. Lemma 2: Let C be a density-connected set with respect to NPred, MinWeight and let p be any object in C with MinWeight(N NPred (p)) = true. Then C equals to the set O = {o \u201aààD | o is density-reachable from p with respect to NPred, MinWeight}. Proof: 1) O \u201aäÜ C by definition of O. 2) C \u201aäÜ O: Let q \u201aàà C. Since also p \u201aàà C and C is a density- connected set, there is an object o \u201aàà C such that p and q are density-connected via o, i.e. both p and q are density-reachable from o. Because both p and o are core objects, it follows that also o is density-reachable from p (symmetry for core objects). With transitivity of density-reachability with respect to NPred, MinWeight it follows that q is density-reachable from p. Then q \u201aàà O. \u201aùè We will now define a clustering CL of a database D with respect to NPred and MinWeight as the set of all density-connected sets with respect to NPred and MinWeight in D, i.e. all clusters from a clustering CL are density-connected sets with regard to the same \u201aÄúparameters\u201aÄù NPred and MinWeight. Noise will then be defined relative to a given clustering CL of D, simply as the set of objects in D not belonging to any of the clusters of CL. Definition 7: (clustering) A clustering CL of D with respect to NPred, MinWeight is a set of den- sity-connected sets with respect to NPred, MinWeight in D, CL = {C 1 ,. . ., C k }, such that for all C the following holds: if C is a density-connected set with respect to NPred, MinWeight in D, then C \u201aààCL. Definition 8: (noise) Let CL={C 1 ,. . .,C k } be a clustering of the database D with respect to NPred, MinWeight. Then we define the noise in D as the set of objects in the database D not be- longing to any density-connected set C i , i.e. noise CL = D \\ (C 1 \u201aà™ . . . \u201aà™ C k ). There are other possibilities to define a clustering based on definition 6. But this simple notion of a clustering has the nice property that two clusters can at most overlap in objects which are border objects in both clusters. Figure 4 illustrates the overlap of two clusters using cardinality and MinCard = 4. Lemma 3: Let CL be a clustering of D with respect to NPred, MinWeight. If C 1 , C 2 \u201aààCL and C 1 \u201aâ\u2020 C 2 , then for all p \u201aààC 1 \u201aà© C 2 it holds that p is not a core object, i.e. wCard(NPred(p)) < MinCard. Proof: Since NPred and MinWeight are the same for all clusters in CL it follows that if p \u201aàà C1 \u201aà© C2 would be a core object for C1 , then p would also be a core object for C2 . But then it follows from Lemma 2 that C1 = C2 , in contradiction to the assumption. Hence, p is not a core object. \u201aùè 3.2 Important Specializations The first specialization of a density-connected set obviously is a density-based cluster as de- fined in (Ester et al., 1996): \u201aÄ¢ NPred: \u201aÄúdistance \u201aâ§ Eps\u201aÄù, wCard: cardinality, MinWeight(N): | N | \u201aâ\u2022 MinPts Specializing this instance further will yield a description of a level in the single-link hierarchy determined by a \u201aÄúcritical distance\u201aÄù D min = Eps (Sibson, 1973): \u201aÄ¢ NPred: \u201aÄúdistance \u201aâ§ NN-dist\u201aÄù, wCard: cardinality, MinWeight(N): | N | \u201aâ\u2022 2, every point p in the set noise CL must be considered as a single cluster. Note that if cardinality is used and MinCard \u201aâ§ 3 there exists no overlap between the clusters of a clustering CL. But then, the well-known \u201aÄúsingle-link effect\u201aÄù can occur, i.e., if there is a chain of points between two clusters where the distance of each point in the chain to the neighboring point in the chain is less than Œµ then the two clusters will not be separated. Higher values for Min- Card will significantly weaken this effect and even for regular distributions where the k-distance values may not differ from the 1-distance values for almost all points, a clustering according to definition 7 and 8 will in general not be equivalent to a level in the single-link hierarchy. A further specialization of density-connected sets allows the clustering of spatially extended objects such as polygons: cluster 1 cluster 2 borderpoint in both clusters Figure 4. Overlap of two clusters for MinCard = 4 \u201aÄ¢ NPred: \u201aÄúintersects\u201aÄù or \u201aÄúmeets\u201aÄù, wCard: sum of areas, MinWeight(N): sum of areas \u201aâ\u2022 MinArea There are also specializations equivalent to simple forms of region growing (Niemann, 1990), i.e. only local criteria for expanding a region can be defined by the weighted cardinality function. For instance, the neighborhood may be given simply by the neighboring cells in a grid and the weighted cardinality function may be some aggregation of the non-spatial attribute values. \u201aÄ¢ NPred: \u201aÄúneighbor\u201aÄù, MinWeight(N): aggr(non-spatial values) \u201aâ\u2022 threshold While region growing algorithms are highly specialized to pixels, density-connected sets can be defined for any data types. Figure 5 illustrates some specializations of density-connected sets. "},{"aspect":"solution","tweet":" 4. GDBSCAN: Generalized Density Based Spatial Clustering of Applications with Noise Density-based clusters Clustering of polygons Simple region growing In section 4.1, we present the algorithm GDBSCAN (Generalized Density Based Spatial Clus- tering of Applications with Noise) which is designed to discover the density-connected sets and the noise in a spatial database. To apply the algorithm, we have to know the NPred-neighbor- hood, MinCard and the wCard function. In section 4.2, the issue of determining these \u201aÄúparame- ters\u201aÄù is discussed and a simple and effective heuristic to determine Eps and MinCard for Eps- neighborhoods combined with cardinality as wCard function is presented. 4.1 The Algorithm Figure 5. Different specializations of density-connected sets To find a density-connected set, GDBSCAN starts with an arbitrary object p and retrieves all ob- jects density-reachable from p with respect to NPred and MinWeight. If p is a core object, this procedure yields a density-connected set with respect to NPred and MinWeight (see Lemma 1 and 2). If p is not a core object, no objects are density-reachable from p and p is assigned to NOISE. This procedure is iteratively applied to each object p which has not yet been classified. Thus, a clustering and the noise according to definitions 7 and 8 are detected. In figure 6, we present a basic version of GDBSCAN omitting details of data types and gener- ation of additional information about clusters: GDBSCAN (SetOfObjects, NPred, MinCard, wCard) // SetOfObjects is UNCLASSIFIED ClusterId := nextId(NOISE); FOR i FROM 1 TO SetOfObjects.size DO Object := SetOfObjects.get(i); IF Object.ClId = UNCLASSIFIED THEN IF ExpandCluster(SetOfObjects,Object,ClusterId, NPred,MinCard,wCard) THEN ClusterId:=nextId(ClusterId) END IF END IF END FOR END; // GDBSCAN SetOfObjects is either the whole database or a discovered cluster from a previous run. NPred and MinCard are the global density parameters and wCard is a pointer to a function wCard(Ob- jects) that returns the weighted cardinality of the set Objects. ClusterIds are from an ordered and countable datatype (e.g. implemented by Integers) where UNCLASSIFIED < NOISE < \u201aÄúother Ids\u201aÄù, and each object is marked with a clusterId Object.ClId. The function nextId(clusterId) returns the successor of clusterId in the ordering of the datatype (e.g. implemented as Id := Id+1). The function SetOfObjects.get(i) returns the i-th element of SetOfObjects. In figure 7, function Expand- Cluster constructing a density-connected set for a core object Object is presented in more detail. A call of SetOfObjects.neighborhood(Object,NPred) returns the NPred-neighborhood of Point in SetOfPoints as a list of objects. Obviously the efficiency of the above algorithm depends on the efficiency of the neighborhood query because such a query is performed exactly once for each object in SetOfObjects which satisfies the selection condition. The performance of GDBSCAN will be discussed in detail in section 5, where we will see that neighborhood predicates based on spatial proximity like distance predicates or intersection can be evaluated very efficiently by us- ing spatial index structures. Figure 6. Algorithm GDBSCAN The clusterId of some objects p which are marked to be NOISE because wCard(NPred(p)) < MinCard may be changed later if they are density-reachable from some other object of the data- base. This may happen only for border objects of a cluster. Those objects are then not added to ExpandCluster(SetOfObjects, Object, ClId, NPred, MinCard, wCard): Boolean; IF wCard({Object}) \u201aâ§ 0 THEN // point not in selection SetOfPoints.changeClId(Object,UNCLASSIFIED); RETURN False; END IF seeds:=SetOfObjects.neighborhood(Object,NPred); IF wCard(seeds) < MinCard THEN // no core point SetOfObjects.changeClId(Object,NOISE); RETURN False; END IF // still here? Object is a core object SetOfObjects.changeClIds(seeds,ClId); seeds.delete(Object); WHILE seeds \u201aâ\u2020 Empty DO currentObject := seeds.first(); result := SetOfObjects.neighborhood(currentObject, NPred); IF wCard(result) \u201aâ\u2022 MinCard THEN FOR i FROM 1 TO result.size DO P := result.get(i); IF wCard({P}) > 0 AND P.ClId IN {UNCLASSIFIED, NOISE} THEN IF P.ClId = UNCLASSIFIED THEN seeds.append(P); END IF; SetOfObjects.changeClId(P,ClId); END IF; // wCard > 0 and UNCLASSIFIED or NOISE END FOR; END IF; // wCard \u201aâ\u2022 MinCard seeds.delete(currentObject); END WHILE; // seeds \u201aâ\u2020 Empty RETURN True; END; // ExpandCluster the seeds-list because we already know that an object with a ClusterId of NOISE is not a core ob- ject, i.e., no other objects are density-reachable from them. If two clusters C 1 and C 2 are very close to each other, it might happen that some object p be- longs to both C 1 and C 2 . Then p must be a border object in both clusters according to Lemma 3. In this case, object p will only be assigned to the cluster discovered first. Except from these rare situations, the result of GDBSCAN is independent of the order in which the objects of the data- base are visited due to Lemma 1 and 2. Figure 7. Function ExpandCluster There may be reasons to apply a postprocessing to the clustering obtained by GDBSCAN. Ac- cording to definition 7, each set of objects having MinWeight is a density-connected set. In some applications (see, e.g., section 6.1), however, density-connected sets of this minimum size are too small to be accepted as clusters. Furthermore, GDBSCAN produces clusters and noise. But for some applications a non-noise class label for each object is required. For this purpose, we can reassign each noise object and each object of a rejected cluster to the closest of the accepted clus- ters. This postprocessing requires just a simple scan over the whole database without much com- putation, in particular no region queries are necessary. Therefore, such postprocessing does not increase the runtime complexity of GDBSCAN. To conclude this section, we introduce the algorithm DBSCAN (Ester et al., 1996) as an im- portant specialization of GDBSCAN. Definition 9: (DBSCAN) DBSCAN is a specialization of the algorithm GDBSCAN using the following parameters: NPred: \u201aÄúdistance\u201aâ§Eps\u201aÄù, wCard: cardinality, MinWeight(N): |N | \u201aâ\u2022 MinPts. 4.2 Determining the Parameters for GDBSCAN GDBSCAN requires a neighborhood predicate NPred, a weight function wCard and a minimum weight MinCard. Which concrete parameters we will use, depends on the goal of the application. In some applications there may be a natural way to provide values without any further parameter determination. In other cases, we may only know the type of neighborhood that we want to use, e.g. a distance based neighborhood for the clustering of point objects. In these cases we have to use a heuristic to determine the appropriate parameters. In this section, we present a simple heuristic which is effective in many cases to determine the parameters Eps and MinCard for DBSCAN (c.f. definition 9) which is the most important spe- cialization of GDBSCAN. DBSCAN uses a distance based neighborhood \u201aÄúdistance less or equal than Eps\u201aÄù and cardinality as the wCard function. Thus, we have to determine appropriate values for Eps and MinCard. The density parameters of the \u201aÄúthinnest\u201aÄù, i.e. least dense, cluster in the da- tabase are good candidates for these global values specifying the lowest density which is not considered to be noise. For a given k \u201aâ\u2022 1 we define a function k-distance, mapping each object to the distance from its k-th nearest neighbor. When sorting the objects of the database in descending order of their k-dis- tance values, the plot of this function gives some hints concerning the density distribution in the database. We call this plot the sorted k-distance plot (see figure 8 for an example). If we choose an arbitrary object p, set the parameter Eps to k-distance(p) and set the parameter MinCard to k+1, all objects with an equal or smaller k-distance value will be core objects, because there are at least k+1 objects in an Eps-neighborhood of an object p if Eps is set to k-distance(p). If we can now find a threshold object with the maximum k-distance value in the \u201aÄúthinnest\u201aÄù cluster of D, we would obtain the desired parameter values. Therefore, we have to answer the following ques- tions. 1) Which value of k is appropriate? 2) How can we determine a threshold object p? We will discuss the value k first, assuming it is possible to set the appropriate value for Eps. The smaller we choose the value for k, the lower are the computational costs to calculate the k- distance values and the smaller is the corresponding value for Eps in general. But a small change of k for an object p will in general only result in a small change of k-distance(p). Furthermore, our experiments indicate that the k-distance plots for \u201aÄúreasonable\u201aÄù k (e.g. 1 \u201aâ§ k \u201aâ§ 10 in 2D space) do not significantly differ in shape and that also the results of DBSCAN for the corresponding parameter pairs (k, Eps) do not differ very much. Therefore, the choice of k is not very crucial for the algorithm. We can even fix the value for k (with respect to the dimension of the dataspace), eliminating the parameter MinCard for DBSCAN. Considering only the computational cost, we would like to set k as small as possible. On the other hand, if we set k = 1, the k-distance value for an object p will be the distance to the nearest neighbor of p and the \u201aÄúsingle-link effect\u201aÄù can occur. To weaken this effect, we must choose a value for k > 1. We propose to set k to 2*dimension - 1. Our experiments indicate that this value works well for databases D where each point occurs only once, i.e. if D is really a set of points. Thus in the fol- lowing, if not stated otherwise, k will be set to this value, and the value for MinCard will be fixed according to the above strategy (MinCard = k + 1, e.g. MinCard = 4 in 2D space). To determine the parameter Eps for DBSCAN, we have to know an object in the \u201aÄúthinnest\u201aÄù cluster of the database with a high k-distance value for that cluster. Figure 8 shows a sorted k-dis- tance plot for sample database 3 which is very typical for databases where the density of clusters and noise are significantly different. Our experiments indicate that the threshold object is an ob- ject near the first \u201aÄúvalley\u201aÄù of the sorted k-distance plot (see figure 8). All objects with a higher k- distance value (to the left of the threshold) will then be noise, all other objects (to the right of the threshold) will be assigned to some cluster. In general, it is very difficult to detect the first \u201aÄúvalley\u201aÄù automatically, but it is relatively simple for a user to recognize this valley in a graphical representation. Additionally, if the user can esti- 3-distance noise clusters threshold point objects Figure 8. Sorted 3-distance plot for sample database 3 mate the percentage x of noise, a proposal for the threshold object can be derived, because we know that most of the noise objects have a higher k-distance value than objects of clusters. The k-distance values of noise objects are located on the left of the k-distance plot, so that we can simply select an object after x percent of the sorted k-distance plot. There is always a range of values for the parameter Eps that yield the same clustering because not all objects of the \u201aÄúthinnest\u201aÄù cluster need to be core objects. They will also belong to the clus- ter if they are only density-reachable. Furthermore, the Eps value may be larger than needed if the clusters are well separated and the density of noise is clearly lower than the density of the thinnest cluster. Thus the robustness of the parameter determination, i.e. the width of the range of appropriate Eps values, depends on the application. However, in general the width of this range is broad enough to allow the parameters to be determined in a sorted k-distance plot for only a very small sample of the whole database (1% - 10%). To summarize, we propose the following interactive approach for determining the parameters for DBSCAN . The user gives a value for k (default is k = 2*dimension - 1). . The system computes and displays the k-distance plot for a sample of the database. . The user selects an object as the threshold object and the k-distance value of this object is used as the Eps value; MinCard is set to k+1 (if the user can estimate the percentage of noise, the system can derive a proposal for the threshold object from it). Obviously, the shape of the sorted k-distance plot and hence the effectiveness of the proposed heuristic depends on the distribution of the k-nearest neighbor distances. For example, the plot will look more \u201aÄústairs-like\u201aÄù if the objects are distributed more regularly within each cluster, or the first \u201aÄúvalley\u201aÄù will be less clear if the densities of the clusters differ not much from the density of noise (which also means that the clusters are not well separated). Then, knowing the approxi- mate percentage of noise in the data may be very helpful. Though for some applications it may be difficult to determine the correct parameters, we want to point out that the parameters may be re-used in different but similar applications, e.g., if the different datasets are produced by a similar process. And, we will see in section 6 that there are even applications where the appropriate parameter values for DBSCAN can be derived analyti- cally (e.g. section 6.2), or a natural notion of a neighborhood for the application exists which does not require any further parameters (e.g. intersects for polygons). "},{"aspect":"expcomparison","tweet":" 5.2 Experimental Evaluation directory level 1 directory level 2 data- pages Table 1. runtime complexity of GDBSCAN runtime complexity of a single neighborhood query the GDBSCAN algorithm without index O(n) O(n 2 ) with spatial index O(log n) O(n * log n) with direct access O(1) O(n) We have implemented GDBSCAN in C++ based on an implementation of the R*-tree (Beck- mann et al., 1990). All experiments were run on HP 735 / 100 workstations. In order to allow a comparison with CLARANS and BIRCH - which both use a distance based neighborhood defi- . . . Figure 9. Structure of an R*-tree nition - we evaluated the specialized DBSCAN algorithm (c.f. definition 9). For an evaluation of the effectivity of the more general GDBSCAN, see the applications in section 6. To compare DBSCAN with CLARANS in terms of effectiveness (accuracy), we use the three synthetic sample databases which are depicted in figure 1. Since DBSCAN and CLARANS are clustering algorithms of different types, they have no common quantitative measure of the clas- sification accuracy. Therefore, we evaluate the accuracy of both algorithms by visual inspection. In sample database 1, there are four ball-shaped clusters of significantly differing sizes. Sample database 2 contains four clusters of nonconvex shape. In sample database 3, there are four clus- ters of different shape and size with additional noise. To show the results of both clustering algo- rithms, we visualize each cluster by a different color. To give CLARANS some advantage, we set the parameter k (number of clusters) to 4 for these sample databases. The clusterings discov- ered by CLARANS are depicted in figure 10. x x For DBSCAN, the parameter Eps was set, giving a noise percentage of 0% for sample databas- es 1 and 2, and 10% for sample database 3, respectively. The clusterings discovered by DB- SCAN are depicted in figure 11. x x database 1 database 2 database 3 Figure 10. Clusterings discovered by CLARANS DBSCAN discovers all clusters (according to definition 7) and detects the noise points (ac- cording to definition 8) from all sample databases. CLARANS, however, splits clusters if they are relatively large or if they are close to some other cluster. Furthermore, CLARANS has no ex- plicit notion of noise. Instead, all points are assigned to their closest medoid. x x To test the efficiency of DBSCAN and CLARANS, we use the SEQUOIA 2000 benchmark data. The SEQUOIA 2000 benchmark database (Stonebraker et al., 1993) uses real data sets that are typical for Earth Science tasks. There are four types of data in the database: raster data, point x x x x x x cluster 2 data, polygon data and directed graph data. The point data set contains 62,584 Californian names of landmarks, extracted from the US Geological Survey\u201aÄôs Geographic Names Information Sys- tem, together with their location. The point data set occupies about 2.1 MB. Since the runtime of CLARANS on the whole data set is very high, we have extracted a series of subsets of the SE- QUIOA 2000 point data set containing from 2% to 20% representatives of the whole set. The runtime comparison of DBSCAN and CLARANS on these databases is shown in table 2. The re- sults of our experiments show that the runtime of DBSCAN is almost linear to the number of points. The runtime of CLARANS, however, is close to quadratic to the number of points. The results show that DBSCAN outperforms CLARANS by a factor of between 250 and 1,900 which grows with increasing size of the database. number of points cluster 3 cluster 1 cluster 4 cluster 1 cluster 2 cluster 3 cluster 4 database 1 database 2 Figure 11. Clusterings discovered by DBSCAN Table 2. comparison of runtime (in sec.) cluster 1 cluster 3 cluster 2 database 3 cluster 4 1252 2503 3910 5213 6256 7820 8937 10426 12512 62584 DBSCAN 3 7 11 16 18 25 28 33 42 233 CLARANS 758 3,026 6,845 11,745 18,029 29,826 39,265 60,540 80,638 ??? Since we found it rather difficult to set the parameters of BIRCH appropriately for the SEQUI- OA 2000 point data, we used the test data sets 1, 2 and 3 introduced by Zhang et al. (Zhang et al., 1997) to compare DBSCAN with BIRCH. The used implementation of BIRCH - using CLARANS in phase 3 - was provided by its authors. The runtime of DBSCAN was 1.8, 1.8 and 12.0 times the runtime of BIRCH. Note, however, that in general the same restrictions with re- spect to clusters of arbitrary shape apply to BIRCH as they apply to CLARANS. Furthermore, the clustering features - on which BIRCH is based - can be only defined in a Euclidean vector space implying a limited applicability of BIRCH compared to DBSCAN (and compared to CLARANS).  "}]}