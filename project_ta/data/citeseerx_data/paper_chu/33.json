{"user_name":" Exploring the Similarity Space ","user_timeline":[{"aspect":"abstract","tweet":" Abstract Ranked queries are used to locate relevant documents in text databases. In a ranked query a list of terms is specified, then the documents that most closely match the query are returned\u201aÄîin decreasing order of similarity\u201aÄîas answers. Crucial to the efficacy of ranked querying is the use of a similarity heuristic, a mechanism that assigns a numeric score indicating how closely a document and the query match. In this note we explore and categorise a range of similarity heuristics described in the literature. We have implemented all of these measures in a structured way, and have carried out retrieval experiments with a substantial subset of these measures. Our purpose with this work is threefold: first, in enumerating the various measures in an orthogonal framework we make it straightforward for other researchers to describe and discuss similarity measures; second, by experimenting with a wide range of the measures, we hope to observe which features yield good retrieval behaviour in a variety of retrieval environments; and third, by describing our results so far, to gather feedback on the issues we have uncovered. We demonstrate that it is surprisingly difficult to identify which techniques work best, and comment on the experimental methodology required to support any claims as to the superiority of one method over another. "},{"aspect":"expanalysis","tweet":"  Measurement of retrieval effectiveness For each query, each collection, and each similarity measure the top 1,000 ranked documents were identified, and a postprocessing program (trec_eval, available from ftp://cs.cornell.edu) used to obtain statistics about the relative performance of that mechanism. Of the many effectiveness measures reported by that program, those used in our experiments were the 11point recall-precision average at 1,000 retrieved documents (averaging the precision attained at 0%, 10%, 20%, . . . , 100% recall levels); the precision at 20 retrieved documents; and (after modifying trec_eval) the average value of 1/r1, wherer1 is the rank of the first relevant document returned. The use of three different effectiveness metrics added a final dimension to our burgeoning collection of statistics. Results Tables 10, 11, and 12 show a partial summary of the data that was collected. Each of the six sections in each of the tables shows one of the six experimental domains: title, narrative, or full queries; and one of the two document collections. Inside each section there are 15 rows of data. The first of these, marked ZZ-ZZZ-ZZ, is discussed below. The next 10 rows show, for that combination of query set and collection, the best 10 of the 720 similarity measures that were explored, where best in Table 10 is judged by average 11-point recall-precision average over the query set, best in Table 11 is assessed by considering the average precision-at-20 value for the query set, and best in Table 12 is scored by average reciprocal rank of the first relevant document retrieved. The final four rows in each section of the tables show in snapshot fashion the extent to which the effectiveness degrades further down the ordering. Each section of the three tables also contains a row marked ZZ-ZZZ-ZZZ. The score associated with this measure for each query is the best score achieved by any of the tested measures for that query. That is, each of the ZZ-ZZZ-ZZZ scores reflects the average (over the query set) of the best (over the 720 formulations) performing heuristic, and so represents the score that a clairvoyant user of the system\u201aÄîone able to decide in advance what formulation is best suited for each query\u201aÄîwould obtain. The ZZ-ZZZ-ZZ measure gives an indication of the \u201aÄúgoodness\u201aÄù (or otherwise) of the performance of the non-clairvoyant (and hence practical) mechanisms listed in the three tables. It is obvious from these results that there is no measure that is a clear winner. There is little overlap between the successful measures in the eighteen cases\u201aÄînot only are different measures best for the different queries and data sets, but recall-precision yields somewhat different results to precision-at-20 and top-rank. For example, of the 20 factors we tested, just one of them (formulation E for the relative document frequency, Table 4) does not appear in a \u201aÄútop 10\u201aÄù measure listed in the three tables, and 87 different combinations appear amongst these \u201aÄútop 10\u201aÄù tables. Furthermore, it would be incorrect to try and draw conclusions such as that certain measures work well in certain domains\u201aÄîas the results for the synthetic ZZ-ZZZ-ZZ measure show, none of the measures is particularly good for either a certain data set, a certain style of query, or a certain effectiveness metric. Particular measures do seem to work well for individual queries, but it is likely to be extremely difficult to recognise in advance which combinations will work in which cases. These results not only make it difficult to identify successful measures, but also to identify and explain successful components. For example, pivoting is valuable some of the time but not all of the time; and it is not possible to categorically state that any particular weighting scheme is valuable. However, some trends do emerge. Not surprisingly, document length is important\u201aÄîcombining function A is highly ranked only with relative term frequency F, the only relative term frequency to incorporate document length; and it seems as if the A*-*F formulations are better for the short queries (except when effectiveness is measured ap2wsj2 fr2ziff2 Size (megabytes) 479.3 384.8 Documents 154,439 76,780 Average document length (kilobytes) 3.2 5.1 Maximum document length (kilobytes) 133.0 1,836.5 Table 7: Statistics of document collections Query set Example query Title impact foreign textile imports textile industry Narrative impact positive negative qualitative may include expansion shrinkage markets manufacturing volume influence methods strategies textile industry textile industry includes production purchase raw materials basic processing techniques dyeing spinning knitting weaving manufacture marketing finished goods research textile field Full impact foreign textile imports textile industry foreign textiles textile products influenced impacted textile industry qualitative shrinkage markets manufacturing volume influence methods strategies textile industry textile industry production purchase raw materials basic processing techniques dyeing spinning knitting weaving manufacture marketing finished goods research textile field Table 8: Examples of the three query sets Collection Query Set Title Narrative Full Number ap2wsj2 150 150 150 fr2ziff2 101 101 101 Average terms ap2wsj2 78.6 31.8 3.8 fr2ziff2 79.0 33.5 3.7 Average answers ap2wsj2 90.4 90.4 90.4 fr2ziff2 17.7 17.7 17.7 Table 9: Statistics of query sets Collection Title Narrative Full ap2wsj2 1 2 3 4 5 6 7 8 9 10 100 200 400 720 fr2ziff2 1 2 3 4 5 6 7 8 9 10 100 200 400 720 Q 11-pt ZZ-ZZZ-ZZZ 0.312 AI-AFD-BCA 0.265 AI-AFD-BEA 0.265 AI-BFD-ACA 0.265 AI-BFD-AEA 0.265 AI-AFD-BAA 0.265 AI-BFD-AAA 0.265 AI-AFK-BCA 0.263 AI-BFK-ACA 0.263 AI-AFK-BEA 0.262 AI-BFK-AEA 0.262 AI-ACK-BCA 0.246 BB-BCI-AEA 0.240 BB-ACI-AEA 0.206 BI-AED-AAA 0.074 Q 11-pt ZZ-ZZZ-ZZZ 0.349 AB-BFD-BAA 0.231 AB-BFD-BCA 0.231 AB-BFD-BEA 0.231 AD-BFD-BAA 0.230 AD-BFD-BCA 0.230 AD-BFD-BEA 0.230 AB-BFK-BAA 0.228 AD-BFK-BAA 0.228 AB-BFK-BCA 0.228 AB-BFK-BEA 0.228 AD-BEB-BCA 0.194 AI-AEB-BCA 0.190 AI-BCD-AAA 0.161 BI-AED-AEA 0.054 Q 11-pt ZZ-ZZZ-ZZZ 0.342 BI-BCK-BCA 0.288 BI-BCI-BCA 0.286 BI-ACI-BCA 0.282 BB-BCK-BCA 0.280 BI-BCD-BCA 0.280 BD-BCK-BCA 0.280 BB-BCI-BCA 0.279 BD-BCI-BCA 0.279 BD-ACI-BCA 0.277 BB-ACI-BCA 0.277 AI-BCB-BCA 0.232 AB-BCI-BEA 0.204 AB-BCI-ACA 0.161 AI-ACK-AAA 0.046 Q 11-pt ZZ-ZZZ-ZZZ 0.415 BB-BCI-BCA 0.241 BD-BCI-BCA 0.241 BD-BFK-BCA 0.237 BB-BFK-BCA 0.236 BD-BCD-BCA 0.232 BB-BCD-BCA 0.232 BI-BCI-BCA 0.230 BB-BCI-BEA 0.229 BD-BCI-BEA 0.229 BD-BFK-BEA 0.218 BI-BEI-BEA 0.164 BI-BCI-AEA 0.133 AD-BED-AAA 0.091 BI-AED-AAA 0.033 Q 11-pt ZZ-ZZZ-ZZZ 0.426 BI-ACI-BCA 0.362 BB-ACI-BCA 0.362 BD-ACI-BCA 0.362 BI-BCI-BCA 0.356 BI-BCK-BCA 0.355 BB-BCI-BCA 0.353 BD-BCI-BCA 0.353 BB-BCK-BCA 0.350 BD-BCK-BCA 0.350 BI-ACD-BCA 0.350 BB-BEK-ACA 0.298 BB-BEK-AEA 0.270 BD-BFD-AAA 0.226 AI-ACK-AAA 0.077 Q 11-pt ZZ-ZZZ-ZZZ 0.472 BD-BFK-BCA 0.294 BB-BFK-BCA 0.293 BB-BCI-BCA 0.281 BD-BCI-BCA 0.281 BI-BFK-BCA 0.275 BD-ACI-BCA 0.274 BB-ACI-BCA 0.274 BI-BCI-BCA 0.270 BD-BCD-BCA 0.266 BB-BCD-BCA 0.266 BI-BCB-ACA 0.201 BD-AFD-BAA 0.158 AD-BEB-AEA 0.103 BI-AED-AAA 0.049 Table 10: Eleven-point recall-precision average at 1,000 documents returned by precision and the collection is fr2ziff2), while B*-[AB] formulations handle the long queries better. But these two observations are about all that can be claimed. In other experiments, not detailed here, we tested apparently trivial variants to the weighting schemes such as varying the base of the logarithm in relative term frequency C. These changes (from say log 2 to log e) could have substantial effect on recall-precision. While testing mg we have at times been puzzled by our inability to exactly reproduce the results obtained by other researchers. It is now apparent that large differences in results can easily be the consequence of minor variations to the similarity measures such as the base of logarithms, and whether, as in another case we encountered, the \u201aÄú+1\u201aÄù addition takes place before or after the logarithms are taken. Tables 13, 14, and 15 further illustrate this volatility. By normalising each recall-precision, precisionat-20, and top-rank score to a percentage of the ZZ-ZZZ-ZZZ score for that experimental domain, a set of \u201aÄúsame unit\u201aÄù quantities can be calculated and further combined in different ways. Table 13 averages these scores over collections and effectiveness metrics, to show any trend that might be a result of the different characteristics of the three query sets; Table 14 averages the raw scores over collections and query sets, to identify any influences that can be attributed to the use of different effectiveness metrics; and Table 15 completes the third leg of the analysis, showing combined scores broken down by collection. In any of these three tables similarity formulations that work consistently well for that combination of parameters should appear near the top of a list of overall percentage, while those that are variable in their behaviour will Collection Title Narrative Full ap2wsj2 1 2 3 4 5 6 7 8 9 10 100 200 400 720 fr2ziff2 1 2 3 4 5 6 7 8 9 10 100 200 400 720 Q pr@20 ZZ-ZZZ-ZZZ 0.505 AB-AFD-BAA 0.405 AB-AFD-BEA 0.405 AB-BFD-AAA 0.405 AB-BFD-AEA 0.405 AI-AFK-BAA 0.405 AI-AFK-BCA 0.405 AI-AFK-BEA 0.405 AI-BFK-AAA 0.405 AI-BFK-ACA 0.405 AI-BFK-AEA 0.405 AD-BCK-AAA 0.400 AB-BCB-BCA 0.377 AI-AEI-AEA 0.342 BI-AED-AEA 0.143 Q pr@20 ZZ-ZZZ-ZZZ 0.195 BD-ACK-BAA 0.139 BD-ACK-BCA 0.139 BD-ACK-BEA 0.139 BD-BCK-AAA 0.139 BD-BCK-ACA 0.139 BD-BCK-AEA 0.139 BB-ACK-BAA 0.139 BB-ACK-BCA 0.139 BB-ACK-BEA 0.139 BB-BCK-AAA 0.139 AB-AFD-BCA 0.125 AI-AEB-BCA 0.118 AB-ACD-BAA 0.093 BI-AED-AEA 0.048 Q pr@20 ZZ-ZZZ-ZZZ 0.556 BI-ACI-BCA 0.448 BB-ACI-BCA 0.447 BD-ACI-BCA 0.447 BI-ACK-BCA 0.436 BI-BCK-ACA 0.436 BB-ACK-BCA 0.432 BB-BCK-ACA 0.432 BD-ACK-BCA 0.432 BD-BCK-ACA 0.432 BI-ACI-BEA 0.432 AD-BFD-BCA 0.382 BB-BFD-AEA 0.349 BB-BEB-BEA 0.297 AI-ACK-AAA 0.134 Q pr@20 ZZ-ZZZ-ZZZ 0.218 BB-BCI-BCA 0.150 BD-BCI-BCA 0.150 BB-BCI-BEA 0.148 BD-BCI-BEA 0.148 BB-BFK-BCA 0.146 BD-BFK-BCA 0.145 BI-BCI-BCA 0.143 BB-BCB-BEA 0.140 BB-ACB-BCA 0.140 BB-BCD-BCA 0.140 BI-ACI-BCA 0.108 BB-AEI-BAA 0.088 AD-AEI-BCA 0.049 AI-ACK-AAA 0.031 Table 11: Precision at 20 documents returned Q pr@20 ZZ-ZZZ-ZZZ 0.636 BI-ACI-BCA 0.521 BB-ACI-BCA 0.519 BD-ACI-BCA 0.519 BI-ACD-BCA 0.514 BI-BCD-ACA 0.514 BB-ACD-BCA 0.511 BB-BCD-ACA 0.511 BD-ACD-BCA 0.511 BD-BCD-ACA 0.511 BB-ACK-BCA 0.509 BD-BCB-AEA 0.459 BI-BCI-AAA 0.430 BI-BEI-AAA 0.379 AI-ACK-AAA 0.186 Q pr@20 ZZ-ZZZ-ZZZ 0.256 BB-ACB-BCA 0.184 BD-ACB-BCA 0.184 BB-BFK-BCA 0.180 BD-BFK-BCA 0.180 BD-BCI-BCA 0.177 BB-BCI-BCA 0.176 BI-BCI-BCA 0.174 BB-ACB-BEA 0.170 BD-ACB-BEA 0.170 BB-BCD-BCA 0.169 BB-BFD-BAA 0.142 BB-AEI-BAA 0.117 AB-BEB-ACA 0.060 AI-ACK-AAA 0.035 appear lower. The most striking feature of the three tables is again the poor performance. Even if we were lucky enough to select the right \u201aÄúbest\u201aÄù measure for a combination of collection, effectiveness metric, and query set, that best mechanism would still only do roughly two thirds as well as if we could somehow include as a further parameter the actual query to be processed. Finally, Table 16 averages the relative scores over all factors, to arrive at a single ranking of similarity functions. Given the lack of consistency in the individual experiments, the fairness of giving a single score to each mechanism is, however, debatable. The BB-ACB-BAA mechanism illustrated in Table 6 is ranked 88th of the 720 mechanisms, with a score of 57.1%. Statistical significance One obvious possibility is that the volatility of the results is a consequence not of fundamentally altered behaviour, but of random fluctuation. To be sure that this was not the case, we applied the correlated t test [Graziano and Raulin 1993, p. 373] to the individual query results for pairs of measures. The results were unequivocal. For example, consider measures BI-BCI-BCA and BB-BCI-BCA on the full queries and the ap2wsj2 collection. These methods both score highly (Table 10), differ in just one factor, and differ in performance by just 0.003 averaged over the 150 queries. Yet the first measure outperforms the second on 99 queries, while the second is better on only 49; and this difference is enough to give a t value of 2.58, which is equal to the 99% confidence for this many observations. That is, the Collection Title Narrative Full ap2wsj2 1 2 3 4 5 6 7 8 9 10 100 200 400 720 fr2ziff2 1 2 3 4 5 6 7 8 9 10 100 200 400 720 Q rank ZZ-ZZZ-ZZZ 0.840 AB-AFK-AAA 0.644 AB-AFK-ACA 0.644 AB-AFK-AEA 0.644 AD-AFK-AAA 0.644 AD-AFK-ACA 0.644 AD-AFK-AEA 0.644 AI-AFK-AAA 0.644 AI-AFK-ACA 0.644 AI-AFK-AEA 0.644 AI-ACB-BAA 0.642 AI-BCD-BAA 0.622 BD-BCK-AEA 0.590 AD-BEB-BAA 0.533 BI-AED-AAA 0.308 Q rank ZZ-ZZZ-ZZZ 0.662 AD-AFD-BAA 0.400 AD-AFD-BEA 0.400 AD-BFD-AAA 0.400 AD-BFD-AEA 0.400 AB-AFD-BAA 0.400 AB-AFD-BEA 0.400 AB-BFD-AAA 0.400 AB-BFD-AEA 0.400 AD-AFD-BCA 0.399 AD-BFD-ACA 0.399 AI-BEB-BEA 0.377 AD-BEB-BEA 0.361 BD-BCD-BCA 0.317 BI-AED-AAA 0.121 Q rank ZZ-ZZZ-ZZZ 0.936 BI-ACK-BCA 0.676 BI-BCK-ACA 0.676 BI-ACI-BEA 0.670 BD-ACI-BCA 0.669 BB-ACI-BCA 0.669 BB-ACK-BCA 0.667 BB-BCK-ACA 0.667 BD-ACK-BCA 0.667 BD-BCK-ACA 0.667 BI-ACI-BCA 0.666 BD-BCK-AAA 0.600 BI-BEK-BCA 0.568 BI-AFD-ACA 0.506 AI-ACK-AAA 0.257 Q rank ZZ-ZZZ-ZZZ 0.758 BB-BCI-BCA 0.467 BD-BCI-BCA 0.467 BB-BCI-BEA 0.449 BD-BCI-BEA 0.448 BI-BCI-BCA 0.447 BD-BFK-BCA 0.437 BB-BFK-BCA 0.436 BD-BCD-BCA 0.424 BB-BCD-BCA 0.423 BD-ACI-BCA 0.419 BD-AFK-BCA 0.365 BB-BEB-AEA 0.304 AD-AFD-BEA 0.203 BI-ACK-AAA 0.123 Table 12: Average (reciprocal) rank of first relevant document Q rank ZZ-ZZZ-ZZZ 0.966 BD-ACI-BCA 0.746 BB-ACI-BCA 0.746 BI-ACI-BCA 0.739 BB-ACK-BCA 0.734 BB-BCK-ACA 0.734 BD-ACK-BCA 0.734 BD-BCK-ACA 0.734 BB-ACI-BEA 0.733 BD-ACI-BEA 0.733 BI-ACI-BEA 0.732 BI-AEI-BEA 0.685 BB-AEK-ACA 0.653 AB-BED-BEA 0.591 AI-ACK-AAA 0.338 Q rank ZZ-ZZZ-ZZZ 0.801 BI-BCI-BCA 0.490 BB-ACB-BCA 0.489 BD-ACB-BCA 0.489 BI-BFK-BCA 0.488 BB-BCI-BCA 0.483 BD-BCI-BCA 0.482 BB-BFK-BCA 0.482 BD-BFK-BCA 0.482 BB-ACB-BEA 0.476 BD-ACB-BEA 0.476 BB-BFK-BAA 0.407 BI-BCK-BEA 0.336 AB-AEB-BCA 0.224 BI-ACK-AAA 0.135 superiority of BI-BCI-BCA over BB-BCI-BCA is almost certainly significant on collection ap2wsj2 and the full queries with effectiveness measured by 11-point recall-precision average at 1,000 documents retrieved. Table 17 reports six more pairwise tests, comparing on each of the three types of query the three \u201aÄúbest\u201aÄù mechanisms (for recall-precision on ap2wsj2) in a \u201aÄúhome and away\u201aÄù competition. The first of the three sections in the table shows that AI-AFD-BCA is clearly better than either BI-BCK-BCA or BI-ACI-BCA when the queries are short; while the remaining two sections show that AI-AFD-BCA should very definitely not be used for long queries on this collection\u201aÄîin each case the high calculated t values indicate that random chance has nothing to do with the relative performance of the various methods. Similar confidence scores are calculated when the the fr2ziff2 best methods are compared with their partners in ap2wsj2, again using recall-precision as the effectiveness metric. For example, using the title queries, AI-AFD-BCA is better than AB-BFD-BAA with a confidence of t =2.53 in ap2wsj2, andinfr2ziff2 AB-BFD-BAA is better than AI-AFD-BCA with a confidence of t =2.40, which both exceed the 95% confidence limit of 2.0. Changing from one effectiveness metric to another gives similar chaotic results. Using ap2wsj2 and the title queries, an 11-point recall-precision comparison between AI-AFD-BCA and AB-AFD-BAA (which is the best according to the precision metric) gives a score of t =3.57. That is, the choice of retrieval mechanism is governed not just by the characteristics of the queries being processed, but also by the characteristics of the data that is being handled (even for the same query 1 2 3 4 5 6 7 8 9 10 100 200 400 720 Title Narrative Full ZZ-ZZZ-ZZZ 100.000 AI-BFD-BAA 70.926 AI-BFD-BEA 70.852 AD-AFD-BEA 70.821 AD-BFD-AEA 70.821 AB-AFD-BEA 70.814 AB-BFD-AEA 70.814 AD-AFD-BAA 70.811 AD-BFD-AAA 70.811 AB-AFD-BAA 70.803 AB-BFD-AAA 70.803 AB-BCB-BEA 65.516 AB-AEB-BEA 63.887 AD-AEI-AAA 58.685 BI-AED-AAA 24.498 ZZ-ZZZ-ZZZ 100.000 BB-BCI-BCA 68.153 BD-BCI-BCA 68.129 BI-BCI-BCA 67.398 BD-ACI-BCA 66.037 BB-ACI-BCA 65.992 BB-BCI-BEA 65.814 BD-BCI-BEA 65.790 BD-BCD-BCA 64.895 BB-BCD-BCA 64.884 BD-ACB-BCA 64.384 BD-BEK-ACA 54.835 BI-BCB-AAA 48.594 AD-BEB-BAA 37.517 AI-ACK-AAA 19.428 ZZ-ZZZ-ZZZ 100.000 BD-ACI-BCA 70.916 BB-ACI-BCA 70.877 BD-BCI-BCA 70.425 BD-ACB-BCA 70.418 BB-BCI-BCA 70.401 BB-ACB-BCA 70.397 BI-BCI-BCA 70.162 BD-BFK-BCA 68.480 BB-BFK-BCA 68.479 BI-ACI-BCA 68.182 BI-AEB-BCA 58.996 BB-BEB-ACA 52.762 BD-AEK-AEA 42.283 AI-ACK-AAA 22.099 Table 13: Mechanisms with best overall performance when grouped by query type 1 2 3 4 5 6 7 8 9 10 100 200 400 720 Eleven-point average Precision at 20 Rank of first relevant ZZ-ZZZ-ZZZ 100.000 BD-ACI-BCA 67.873 BB-ACI-BCA 67.828 BB-BCI-BCA 67.167 BD-BCI-BCA 67.157 BI-BCI-BCA 67.093 BD-BCK-BCA 66.501 BB-BCK-BCA 66.482 BD-BCD-BCA 65.904 BB-BCD-BCA 65.899 BI-ACI-BCA 65.629 BD-BCK-AEA 54.078 BB-BCD-AAA 49.760 AI-AEB-BEA 41.814 BI-AED-AAA 18.740 ZZ-ZZZ-ZZZ 100.000 BD-ACI-BCA 71.180 BB-ACI-BCA 71.156 BD-BCI-BCA 70.358 BB-BCI-BCA 70.357 BI-BCI-BCA 70.044 BI-ACI-BCA 69.194 BB-BCI-BEA 68.735 BD-BCI-BEA 68.725 BD-ACI-BEA 68.266 BB-ACI-BEA 68.247 BB-BCI-AAA 59.352 BD-AEB-BEA 54.232 BI-BED-ACA 47.367 BI-AED-AAA 28.173 ZZ-ZZZ-ZZZ 100.000 BD-ACI-BCA 64.507 BB-ACI-BCA 64.490 BI-ACI-BCA 62.260 BD-ACI-BEA 61.588 BB-ACI-BEA 61.566 BB-BCI-ACA 61.535 BD-BCI-ACA 61.513 BB-BCI-BCA 61.426 BD-BCI-BCA 61.412 BI-BCI-BCA 61.207 BB-BCB-BCA 54.659 BD-BFD-ACA 52.312 AI-BEB-ACA 48.346 BI-AED-AAA 32.139 Table 14: Mechanisms with best overall performance when grouped by effectiveness metric characteristics), by the effectiveness metric that is being used (even for the same query characteristics and same database), and by the terms present in those queries (even for the same query characteristics and same database and same effectiveness metric). 4 Conclusions We commenced this investigation with several aims. First, we wished to enumerate previously articulated similarity functions and cast them into a uniform framework with systematic nomenclature. This alone we felt to be worthwhile, and we believe that others will find our notation and formulations useful and worth adoption. Our second goal was to experiment with these measures, explore the space they define, and identify good measures and good components. We did not achieve this goal\u201aÄîindeed, we now suspect that it is unattainable. The measures do not form a space that can be explored in any meaningful way, other than by exhaustion. Even restricting ourselves to a subspace including several measures that were thought to work well, we not only failed to find any particular measure that really stood out but discovered that no 1 2 3 4 5 6 7 8 9 10 100 200 400 720 ap2wsj2 fr2ziff2 ZZ-ZZZ-ZZZ 100.000 BD-ACI-BCA 78.273 BB-ACI-BCA 78.268 BI-ACI-BCA 78.258 BI-ACK-BCA 77.388 BI-BCK-ACA 77.388 BD-ACK-BCA 77.074 BD-BCK-ACA 77.074 BB-ACK-BCA 77.071 BB-BCK-ACA 77.071 BI-ACI-BEA 75.996 AB-BCK-BCA 69.376 BD-AEI-BCA 64.005 AD-AEI-BCA 57.391 BI-AED-AAA 34.827 ZZ-ZZZ-ZZZ 100.000 BD-BCI-BCA 59.279 BB-BCI-BCA 59.268 BI-BCI-BCA 57.953 BB-BCD-BCA 57.457 BD-BFK-BCA 57.453 BB-BFK-BCA 57.449 BD-ACI-BCA 57.433 BD-BCD-BCA 57.419 BB-ACI-BCA 57.382 BB-BCI-BEA 57.269 BI-BCD-BAA 46.080 BD-ACK-BCA 40.729 AB-AEI-BAA 34.829 BI-AED-AAA 17.874 Table 15: Mechanisms with best overall performance when grouped by collection 1 2 3 4 5 6 7 8 9 10 100 200 400 720 Overall ZZ-ZZZ-ZZZ 100.000 BD-ACI-BCA 67.853 BB-ACI-BCA 67.825 BB-BCI-BCA 66.317 BD-BCI-BCA 66.309 BI-BCI-BCA 66.114 BI-ACI-BCA 65.694 BB-BCI-BEA 64.189 BD-ACI-BEA 64.185 BB-ACI-BEA 64.165 BD-BCI-BEA 64.165 BB-BFK-BAA 55.947 BB-BFD-BEA 52.169 BD-BED-BCA 46.260 BI-AED-AAA 26.351 Table 16: Mechanisms with best overall performance measure consistently worked well across all of the queries in a query set. Both average-case performance and individual-case performance is poor for even the best measures overall. Likewise, no component or weighting scheme was shown to be consistently valuable across all of the experimental domains; that is, success in one domain was a poor predictor for success in another. Moreover, variations such as choice of base for a logarithm could have as profound an effect as more principled modifications; it is thus difficult to assess whether \u201aÄúimprovements\u201aÄù are the result of better understanding of the problem of information retrieval, or are simply the chance peaks that arise in complex domains. It is, however, clear that better performance can be obtained\u201aÄîby choosing a similarity measure to suit each query on an individual basis. But it seems implausible to suppose that a mechanism for making such a choice could be found, or that the weights for a grand combination-of-evidence mechanism could be sensibly chosen. When evaluating a query, the type of data, the type of query, the query itself, the evaluation metric, and as far as we know the type of answer, all matter. Looked at another way, the variability is sufficiently great that no single method attains better than about two thirds of what we now know can be attained by an ideal mechanism. This work is not complete, but we cannot see any clear route forward that would allow us to bring it to a satisfactory conclusion. Indeed, in many ways the contradictory and confusing results we have achieved Correlated t test results Query set Title Title Formulation AI-AFD-BCA BI-BCK-BCA AI-AFD-BCA BI-ACI-BCA Mean value 0.265 0.245 0.265 0.247 Wins 96 54 94 56 Calculated t 3.4 3.5 Query set Narrative Narrative Formulation BI-BCK-BCA AI-AFD-BCA BI-BCK-BCA BI-ACI-BCA Mean value 0.288 0.211 0.288 0.282 Wins 118 31 73 76 Calculated t 9.2 1.3 Query set Full Full Formulation BI-ACI-BCA AI-AFD-BCA BI-ACI-BCA BI-BCK-BCA Mean value 0.362 0.296 0.362 0.355 Wins 123 27 88 62 Calculated t 9.8 1.4 Table 17: Significance results using the correlated t test. All entries refer to eleven-point recall-precision average (Table 10) and the ap2wsj2 collection discourage us from hoping that a \u201aÄúsilver bullet\u201aÄù for information retrieval can ever be found. We welcome any suggestions that you may have that might help us discern the patterns in this apparently chaotic behaviour. In the meantime, when in doubt, use BD-ACI-BCA. Data The retrieval effectiveness results summarised in Tables 10, 11, and 12 are available in full at http://www.cs.mu.oz.au/~alistair/exploring/. We hope that other researchers will avail themselves of this resource. Acknowledgements Tim Shimmin and Owen de Kretser undertook the programming work involved for this paper, and we thank them for their patience in dealing with the many conflicting requirements we placed on them. We are also grateful to Ross Wilkinson for his valuable advice. This work was supported by the Australian Research Council and the Key Centre for Knowledge-Based Systems.  "},{"aspect":"expdata","tweet":" Database and relevance judgements We used two databases in our experiments. In order to have the maximum number of queries that could be tested, we focussed on the collections on disk 2 of the trec data. Disk 2 has been part of the testing for several years, and there are 300 queries for which disk 2 1 Combinations **-*FB-*** and **-*FI-*** are not viable, since the average document length is used. This is why the number of combinations is less than 2 √ó 3 √ó 2 √ó 2 √ó 3 √ó 4 √ó 2 √ó 3 = 864. Note also that some of the combinations can be equivalent for some query sets; in particular, in-query frequency has no effect for the queries based on titles, in which query terms are not repeated. Others are mathematically equivalent, thus giving ties in the rankings shown later. relevance judgements are available. Disk 2 contains four document collections, and we partitioned these to make two experimental collections: a collection of newspaper articles (ap2 and wsj2); and a collection of non-newspaper text (fr2 and ziff2). Table 7 gives some statistics for these two collections. Queries Of the various trec topics, those numbered from 1 to 300 (excluding 201) have relevance judgements against the data of disk 2. Of these, topics 51\u201aÄì200 have been used extensively in previous trec-related work as long queries; and they can also be used in two shorter forms\u201aÄîby taking the title section only, and by taking the narrative section only. These are what we used in our experiments. Table 8 shows an example of each of the three categories of query, namely those derived from trec topic 200. Table 9 lists some information about the query sets. "},{"aspect":"background","tweet":" 1 Introduction It is now commonplace for large document databases to be queried using content-based ranked queries, and generally accepted that the alternative mechanisms for searching for information (such as Boolean queries and hierarchical subject descriptors) do not in general provide the same levels of retrieval effectiveness. The implementation of ranked querying is also now well understood [Frakes and Baeza-Yates 1992; Witten et al. 1994; Korfhage 1997], and as a consequence of the spur provided by the trec project [Harman 1995] there are several publicly-available retrieval systems that support fast ranking on document collections in the multi-gigabyte range. This leads to the question as to what similarity calculation should be used for each type of query, or type of document, or type of desired performance (high precision versus high recall being one obvious distinction); or even whether such categorisations are possible or meaningful. It is extremely difficult\u201aÄîas illustrated by the results of this paper\u201aÄîto identify a single all-encompassing \u201aÄúbest\u201aÄù similarity measure, and we do not propose one here. What we do observe, however, is that there has been convergence towards a small number of good measures, in particular, those that perform well in the trec environment, and that there is considerable doubt as to what components of those formulae are responsible for the good performance. In this note we take a fresh look at the various facets of a similarity measure, proposing an eight-way orthogonal decomposition into factors that in one form or another appear consistently in most of the measures we have found in the literature. The decomposition allows similarity measures to be specified as points in the eight-space, and so permits the space of similarity measures to be explored in systematic and coherent manner. We believe that our eight-way categorisation is sufficiently general that most measures can be described in the same framework. Having identified these eight components we are able to regard each component as a dimension that can be explored. To allow each formulation to be tested we extended the public-domain text database system mg [Moffat and Zobel 1994; Witten et al. 1994; Bell et al. 1995; MG-software 1995] to permit each of the eight \u201aàóDepartment of Computer Science, RMIT, GPO Box 2476V, Melbourne 3001, Victoria, Australia. Email: jz@cs.rmit.edu.au \u201aÄ\u2020 Department of Computer Science, the University of Melbourne, Parkville, 3052, Melbourne, Victoria, Australia. Email: alistair@cs.mu.oz.au components to be modified independently. That is, we developed a version of mg that, at database creation time, is parameterised by a Q-expression, an eight-position string specifying the similarity computation to be performed. At query time the mg system now allows a Q-expression to be specified; if the various index and weights files required to evaluate the specified Q-expression have not been created then the software will print as an error diagnostic the command line or lines that should be executed to create the necessary files. Using this version of mg we explored a large number of variant measures to test whether particular formulations for some components work well regardless of the combination in which they are used, and whether there are new combinations that are more effective than the measures in common use. We used two test collections and three sets of queries, giving six experimental domains. The breadth of experimentation was deliberate; one of the goals of the investigation was to measure the extent to which good performance in one domain implies good performance in another. Intrinsic to these experiments is the notion that performance can be compared in a reliable way. A standard method of comparison is to use a recall-precision average, but in the context of the trec data recall-precision cannot be completely evaluated because the number of relevant documents is unknown [Zobel 1998]. Recall-precision is even less reliable when used to gauge subcollections as there are proportionately fewer relevance judgements. For this reason we also used as measures of performance the precision at a fixed (and relatively small) number of documents retrieved, and the rank of the first relevant document retrieved. These latter measures are important when, for example, a single screenful of top-ranked documents is to be returned to a user as the answer to an information search. We expected in this experimental phase of our investigation to confirm that standard formulations of similarity measures are effective, and indeed this is what occurred. What was surprising, however, was that there was no overall winner, and most of the techniques that worked well in one of the six experimental domains worked poorly in at least one of the other five. Nor did good performance according to one metric necessarily correspond to good performance according to another. The results of the experiments were sufficiently contradictory that we used statistical tools to ensure that the variations being observed were the reflection of genuine differences in behaviour, and not the result of random fluctuation. "},{"aspect":"expintro","tweet":" 3 Experiments t\u201aààTd Wd = |Td| Wd = ÔøΩ |Td| w 2 d,t Wd =log 2 |Td| Wd = fd Wd = ÔøΩ fd Wd =(1\u201aàí s)+s ¬∑ Table 5: Document lengths Wd and query lengths Wq W \u201aÄ≤ d avd\u201aààDW \u201aÄ≤ d We now describe the experimental investigation that was pursued. Our aim was to identify successful combinations\u201aÄîsimilarity measures that give good effectiveness\u201aÄîand to determine whether there were components, such as particular term weightings, that worked well in all combinations. The space of similarity measures can be partially explored by the various Monte Carlo search mechanisms such as simulated annealing, genetic algorithms, and so on. These methods rely upon the presence of continuity in the objective function, so that \u201aÄúbetter\u201aÄù solutions are likely to be found in the neighbourhood of \u201aÄúalready good\u201aÄù solutions. In early experiments we used a primitive hill-climbing method to attempt to search for the best combinations, but it became clear that the space of similarity measures had a highly irregular topology, and the assumption that immediate neighbours of a combination would have similar performance is unwarranted. Changing any element of a combination can have, and indeed usually does have, a non-trivial effect. Another method of search is to undertake an exhaustive enumeration, examining every combination. For small search spaces this is tractable, but for large spaces it is implausibly expensive. Each of the experiments we sought to run would require 5\u201aÄì10 minutes of CPU time for each query set to be tested, and an exhaustive search over even 100,000 similarity heuristics with six experiments to run would thus take as long as ten years. We chose instead to investigate a subspace of combinations that were likely to work well, limiting the dimensions of the search space to make exhaustive evaluation possible. These limits were chosen by identifying a small number of measures that are known to be effective, then allowing all possible mix-andmatch combinations that could be derived from these measures. In imposing such limits we have, of course, closed off parts of the search space. Some of the excluded regions will be explored in future work. Component Position Number of Sample Sample Variants Q-expression Calculation Combining function for document d and query q: Sq,d 1 8 B ÔøΩÔøΩ Sq,d = t\u201aààd\u201aà©q wq,t Weight of term t: ÔøΩ ¬∑ wd,t /(Wd ¬∑ Wq) wt 2 9 B wt =loge(1 + N/ft) Weight of term t in document d: wd,t 3 2 A wd,t = rd,t Relative frequency of term t in document d: rd,t 4 6 C rd,t =1+log e fd,t Weight of document d: Wd 5 7+6 B Wd = ÔøΩ ÔøΩ t\u201aààd w2 d,t Weight of term t in query q: wq,t 6 2 B wq,t = rq,t ¬∑ wt Relative frequency of term t in query q: rq,t 7 5 A rq,t =1 Weight of query q: Wq 8 7 A Wq =1 Table 6: Example similarity measure BB-ACB-BAA Choice of similarity formulations We fixed one of the eight dimensions to what we believed to be a reasonable value, and tested a subset of each of the other seven dimensions. The search space explored is described by the regular expression [AB][BDI]-[AB][CEF][BDIK]-[AB][ACE]A . That is, we considered combining functions A and B (Table 1); term weight formulae B, D, and I (Table 2); and so on. The only factor that was fixed was the query length Wq. This generated a total of 720 legal mechanisms, 1 containing several mechanisms that are known to work well and a good proportion of mechanisms that are at least in theory likely candidates. At about 5\u201aÄì10 minutes per mechanism for each of the six experimental domains (2\u201aÄì4 seconds per query per collection) our experiments took about four weeks of computation. We made the search-space restrictions with some trepidation, but also in the belief that doing so would not substantially handicap our findings. For example, we expected the cosine combining function (a B in the first position) to be important, in which case the mechanism used for calculating the query weight has no effect upon the performance. Most of the mechanisms that have been successful at trec do lie within the space we explored. We also included pivoting [Singhal et al. 1996] and several variant mechanisms for calculating Sq,d and wt. "},{"aspect":"problemdef","tweet":""},{"aspect":"solution","tweet":" 2 Similarity Measures Many similarity measures have been proposed, based on the vector space model and the probabilistic model as well as naive co-occurrence of terms between document and query. In this section we describe several standard similarity measures, mostly based on the vector space model, in a consistent framework and notation. We have used a range of sources but are particularly indebted to van Rijsbergen [1979]; Salton and McGill [1983]; Salton [1989]; and the various authors that contributed to Frakes and Baeza-Yates [1992]. Interested readers are referred to the bibliographies of these books for citations to original publications. We have also made use of the proceedings of the first five trec conferences. Our work is, in many ways, an extension of a previous taxonomy due to Salton and Buckley [1988]. They examined a variety of ways for assigning weights to terms in documents and queries, supposing throughout that the cosine combining mechanism was used to derive a final similarity score. Salton and Buckley used five smaller test collections\u201aÄîthe best that were available prior to the trec initiative\u201aÄîand listed results for eight good combinations. In their six-dimensional space they describe alternative formulations that admit (3 √ó 3 √ó 3) 2 = 729 different similarity measures. The placement of their methods in our scheme is discussed further below. Atomic components A collection is a body of information, usually, but by no means always, consisting of text. A document is the smallest unit of access within the collection, for example, one newspaper article. A term is some identified concept within a document. For text documents the terms are commonly taken to be the words of the document, after stemming and similar transformations; but a term might also be a word pair, a phrase, or an externally assigned descriptor that does not appear in the document at all. A general property of almost all similarity measures is that each is a combination of simple statistics, or primitive information, about the document collection, including: \u201aÄ¢ the number N of documents; \u201aÄ¢ the number n of distinct terms used in the collection; \u201aÄ¢ for each term t and each document d containing t, the frequency fd,t of t in d; \u201aÄ¢ for each term t, the total number Ft of occurrences of t in the collection; \u201aÄ¢ the number ft of documents containing term t; \u201aÄ¢ for each term t, the frequency fq,t of t in query q; \u201aÄ¢ for each document d, thevaluefd = |d|, the number of term occurrences in d; \u201aÄ¢ for each document d, f m d ,thelargestfd,t of any term in d; and \u201aÄ¢ f m ,thelargestft in the collection. Documents and terms are then gathered into sets that restrict the domain of the operations used to combine the statistics into similarity values. We denote these various sets as: \u201aÄ¢ the set D of documents; \u201aÄ¢ for each term t, thesetDtof documents containing t; \u201aÄ¢ the set T of distinct terms in the database; and \u201aÄ¢ the set Td of distinct terms in document d, and similarly Tq for queries, and Tq,d = Tq \u201aà©Td. Thus ft = |Dt| and Ft = ÔøΩ d\u201aààDt fd,t. Notethatfd\u201aâ\u2022|Td|. The basic statistics are combined in different ways by different similarity measures, and are detailed below. There are, however, three important monotonicity assumptions that are present in all formulations, and it is worth stating these explicitly. They are that rare terms are no less important than frequent terms; that multiple appearances of a term in a document are no less important than single appearances; and that, for the same quantity of term matching, long documents are no more important than short documents. Combining functions The similarity Sq,d of a document to a query, which we refer to as the combining function, is usually derived from wd,t, wq,t, Wd, andWq, which correspond respectively to the importance of each term in the document, the importance of that term in the query, the length or weight of the document, and the length of the query. All of these quantities are defined in detail below. The similarity measures Sq,d we consider are shown in Table 1. In all cases Sq,d is intended to numerically indicate how close the document d and query q are in their information content. High scores indicate substantial overlap in term usage, and low scores indicate dissimilar term usage. When Tq,d is empty all of these Sq,d formulations yield zero. The formulations have been assigned alphabetic labels for later reference. Term weight Terms that appear in many documents in the collection should be discounted compared with terms that appear in only a few documents. Thus, it is usual to take into account a term weight (also known as an inverse document frequency or IDF ), denoted here as wt. Many methods have been suggested for calculating term weight; the formulations we consider are shown in Table 2. Term discrimination [Salton and McGill 1983] is also of interest, but we are not aware of a practical method of computing it. Salton and Buckley [1988] describe three different term weighting rules, and also allow the weight to differ between the document and the query. Their three weighting rules are noted in Table 2 with the acronym \u201aÄúSB\u201aÄù and the code assigned by Salton and Buckley to that combination. Document-term and query-term weights Given a term weight, the next decision is the specification of where it should be used\u201aÄîin constructing the document-term weight denoted wd,t, thequery-term weight denoted wq,t, in neither, or in both. When it is used it biases the relative term frequency rd,t, defined in the next paragraph. The two alternative methods for doing this are shown in Table 3. The quantities wd,t and wq,t are derived from other calculated values; nevertheless, it is useful to distinguish them. Doing so allows whichever formulation of wt is chosen to be applied selectively to either or both of query terms and document terms. This allows a wider range of possibilities than when wt =1is used, the first formulation in Table 2. Description Formulation A Inner product. Sq,d = ÔøΩ B Cosine measure. C Simple probabilistic measure. The variable C is a tuning constant, set to 0 in this context [Frakes and Baeza-Yates 1992, p. 369]. D More sophisticated probabilistic measure. Variable C is again a tuning constant set to 0. E Alternative inner product. F Dice formulation. (Ozkarahan [1986, p. 496] and Salton and McGill [1983, pp. 202\u201aÄì3] use Wx = ÔøΩ t\u201aààTx wx,t rather than W 2 x , for Dice, Jaccard, and overlap.) G Jaccard formulation. H Overlap formulation. Sq,d = Table 1: Combining functions Sq,d ÔøΩ Sq,d = t\u201aààTq,d (wq,t ¬∑ wd,t) t\u201aààTq,d (wq,t ¬∑ wd,t) Wq ¬∑ Wd Sq,d = ÔøΩ (C + wt) t\u201aààTq,d Sq,d = ÔøΩ (C + wt) ¬∑ rd,t t\u201aààTq,d Sq,d = ÔøΩ ÔøΩ 2 Sq,d = ÔøΩ W 2 q + W 2 d ÔøΩ Sq,d = t\u201aààTq,d wd,t Wd t\u201aààTq,d (wq,t ¬∑ wd,t) W 2 q + W 2 d t\u201aààTq,d (wq,t ¬∑ wd,t) \u201aàí ÔøΩ t\u201aààTq,d (wq,t ¬∑ wd,t) t\u201aààTq,d (wq,t ¬∑ wd,t) min(W 2 q ,W2 d ) Relative term frequency Most ranking rules attempt to emphasise the effect of terms that are frequent in either document or query or both. This quantity\u201aÄîdenoted rd,t in the document and rq,t in the query\u201aÄî is known as the relative frequency, or relative term frequency. The formulations of relative frequency we consider are shown in Table 4. The table shows rd,t; valuesofrq,t are calculated in a corresponding manner based upon fq,t. Either (or both) of these values are sometimes known as the TF component, and so similarity formulations that are described as TF-IDF make use of a relative term frequency and an inverse document frequency somewhere in their calculation. Salton and Buckley [1988] made use of three different relative term frequency formulations; these are noted in the table. Document and query length Some formulations of document length Wd and query length Wq are shown in Table 5, which are often (but not always) derived from the wd,t and wq,t values respectively. Here the desire is to allow for long documents, which may contain many appearances of query terms but be no more relevant than a succinct document containing only a few appearances. Thus, Wd and Wq are used in the calculation of the combining function (Table 1); Table 5 describes some of the many possible ways of quantifying the length of a document. For example, the first formulation might be of use when documents are known to be of fairly uniform length, perhaps in a bibliographic retrieval system that stores title and author but not abstract. The last formulation\u201aÄîthe pivoted method\u201aÄîis a means of adjusting the cosine method to account for experimentally-determined bias [Singhal et al. 1996], and can be applied to any of the non-unit measures. For orthogonality we also list pivoting as being applicable to the unit length calculation. Analogous formulations are used for Wq, but note that there is no query-length equivalent of pivoting. Salton and Buckley [1988] described two length calculations for each of documents and queries. Description Formulation A Formulation used for binary match. SB = x wt =1 B Logarithmic formulation. SB = f ÔøΩ wt =loge 1+ N ÔøΩ ft C Hyperbolic formulation. wt = 1 D Normalised formulation. wt =log e ft ÔøΩ 1+ E Another normalised formulation. N \u201aàí ft SB = p wt =loge ft These formulas define noise and entropy, where nt is the noise of t and st is the signal. nt = ÔøΩ Using these measures, possible definitions of wt are as shown. The last of these is the entropy measure. d\u201aààDt ÔøΩ m f ft ÔøΩ \u201aàí fd,t log2 Ft st = log 2(Ft \u201aàí nt) F. wt = st G. wt = st H. wt = nt ÔøΩ max t \u201aÄ≤ nt \u201aÄ≤ \u201aààT I. wt = 1\u201aàí nt log 2 N Table 2: Term weights wt (inverse document frequencies) Description Formulation A TF-only formulation. wd,t = rd,t B Standard formulation, TF-IDF. wd,t = rd,t ¬∑ wt Table 3: Document-term weights wd,t and query-term weights wq,t fd,t Ft ÔøΩ ÔøΩ \u201aàí nt Description Formulation A Formulation used for binary match. SB = b rd,t = B Standard formulation. SB = t C Logarithmic formulation. D Normalised formulation. E Alternative normalised formulation. Variable K is a tuning constant, with reported optimums 0.3 and 0.5 [Frakes and Baeza-Yates 1992, page 370]. A similar formulation can be used for query terms, with f m q in the denominator [Frakes and Baeza-Yates 1992, page 375]. In our experiments K =0.5 wasused. SB = n F Okapi formulation [Robertson et al. 1995]. Not defined for query terms. rd,t = Table 4: Relative term frequencies rd,t and rq,t ÔøΩ 1 if t \u201aààTd 0 otherwise rd,t = fd,t rd,t =1+log e fd,t rd,t = fd,t f m d rd,t = K +(1\u201aàí K) fd,t f m d fd,t fd,t + Wd/avd\u201aààD(Wd) Putting it all together Tables 1 to 5 provide for a bewildering array of query-document similarity measures. There are eight possible combining functions (Table 1), nine ways of choosing term weights (Table 2), two ways of choosing document-term weights and query-term weights (Table 3), six ways of setting relative term frequencies for the document and another five for the relative term frequencies in the query (Table 4), and so on. These options are summarised in Table 6, which describes the composition of an eight-character Q-expression that selects one possible combination of options. Table 6 also gives as an example, the calculation that corresponds to the Q-expression BB-ACB-BAA, where the hyphens in the Q-expression serve to separate the global selection of combining method and term weight from the three factors that affect each of document terms (in the middle group) and query terms (in the final group). The simplest Q-expression is AA-AAA-AA, which scores a document according to the number of the query terms that are present without incorporating any frequency or weight information\u201aÄîan approach sometimes known as co-ordinate matching. In a similar way, the Q-expression BB-BBB-BBB describes a method using the cosine similarity coefficient, TF-IDF calculation of term weights in both document and query, and document and query weights calculated as the Euclidean length of vectors in n-space\u201aÄîin other words, the traditional stock-standard cosine vector-space rule. The best formulation investigated by Salton and Buckley\u201aÄîin their terms the measure tfc¬∑ nfx\u201aÄîwould be described in our notation by the Q-expression BB-BBB-BEA. Each other 8-character Q-expression describes a variation. In total there are more than 1,500,000 combinations. However, not all choices give distinct measures. For example, all methods AA-A**-A** (where a * indicates an unspecified or \u201aÄúdoesn\u201aÄôt matter\u201aÄù position) are mathematically identical, since combining function A is independent of both Wd and Wq, andwt = 1 makes the two alternative formulations for both wd,t and wq,t identical. Similarly, some of the formulations are logically identical, since they result in the same ranking. For example, in the cosine formulations BB-BBB-BB* the query weight (which for a given query is a constant) serves only to scale the final similarity values, and so does not alter the effectiveness score for a particular experiment. There are other families of equivalent formulations, and, while we have not attempted to exactly determine the number of different measures, estimate it to be of the order of 100,000. Description Formulation A Unit length. SB = x Wd =1 B Vector space formulation. ÔøΩ ÔøΩ SB = c Wd = C Approximate formulation. D Another approximate formulation. E Yet another approximate formulation. F Byte size. (Alternatively, Wd = bd can be used, where bd is the length of d in bytes.) G Further alternative approximate formulation. H\u201aÄìN Pivoted cosine method (used only for document weights), where W \u201aÄ≤ d is calculated using another length formulation such as method A (to get method H) or method B (to get method I) and where s is the slope, typically about 0.7.  "},{"aspect":"expcomparison","tweet":""}]}