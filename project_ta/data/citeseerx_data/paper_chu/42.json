{"user_name":" Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysis ","user_timeline":[{"aspect":"abstract","tweet":" Abstract Computing semantic relatedness of natural language texts requires access to vast amounts of common-sense and domain-specific world knowledge. We propose Explicit Semantic Analysis (ESA), a novel method that represents the meaning of texts in a high-dimensional space of concepts derived from Wikipedia. We use machine learning techniques to explicitly represent the meaning of any text as a weighted vector of Wikipedia-based concepts. Assessing the relatedness of texts in this space amounts to comparing the corresponding vectors using conventional metrics (e.g., cosine). Compared with the previous state of the art, using ESA results in substantial improvements in correlation of computed relatedness scores with human judgments: from r =0.56 to 0.75 for individual words and from r =0.60 to 0.72 for texts. Importantly, due to the use of natural concepts, the ESA model is easy to explain to human users. "},{"aspect":"expanalysis","tweet":" 3.2 Results Table 4 shows the results of applying our methodology to estimating relatedness of individual words. As we can see, both ESA techniques yield substantial improvements over prior studies. ESA also achieves much better results than the other Wikipedia-based method recently introduced [Strube and Ponzetto, 2006]. Table 5 shows the results for computing relatedness of entire documents. On both test collections, Wikipedia-based semantic interpretation is superior to that of the ODP-based one. Two factors contribute to this phenomenon. First, axes of a multidimensional interpretation space should ideally be as orthogonal as possible. However, the hierarchical organization of the ODP defines the generalization relation between concepts and obviously violates this orthogonality requirement. Second, to increase the amount of training data for building the ODP-based semantic interpreter, we crawled all the URLs cataloged in the ODP. This allowed us to increase the amount of textual data by several orders of magnitude, but also brought about a non-negligible amount of noise, which 2 http://www.cs.technion.ac.il/Àúgabr/ resources/data/wordsim353 3 Despite its name, this test collection is designed for testing word relatedness and not merely similarity, as instructions for human judges specifically directed the participants to assess the degree of relatedness of the words. For example, in the case of antonyms, judges were instructed to consider them as \u201aÄúsimilar\u201aÄù rather than \u201aÄúdissimilar\u201aÄù. IJCAI-07 1609 Algorithm Correlation with humans WordNet [Jarmasz, 2003] 0.33\u201aÄì0.35 Roget\u201aÄôs Thesaurus [Jarmasz, 2003] 0.55 LSA [Finkelstein et al., 2002] 0.56 WikiRelate! [Strube and Ponzetto, 2006] 0.19 \u201aÄì 0.48 ESA-Wikipedia 0.75 ESA-ODP 0.65 Table 4: Computing word relatedness Algorithm Correlation with humans Bag of words [Lee et al., 2005] 0.1\u201aÄì0.5 LSA [Lee et al., 2005] 0.60 ESA-Wikipedia 0.72 ESA-ODP 0.69 Table 5: Computing text relatedness is common in Web pages. On the other hand, Wikipedia articles are virtually noise-free, and mostly qualify as Standard Written English. "},{"aspect":"expdata","tweet":" We implemented our ESA approach using a Wikipedia snapshot as of March 26, 2006. After parsing the Wikipedia XML dump, we obtained 2.9 Gb of text in 1,187,839 articles. Upon # Input: \u201aÄúU.S. intelligence cannot say conclusively that Saddam Hussein has weapons of mass destruction, an information gap that is complicating White House efforts to build support for an attack on Saddam\u201aÄôs Iraqi regime. The CIA has advised top administration officials to assume that Iraq has some weapons of mass destruction. But the agency has not given President Bush a \u201aÄúsmoking gun,\u201aÄù according to U.S. intelligence and administration officials.\u201aÄù Input: \u201aÄúThe development of T-cell leukaemia following the otherwise successful treatment of three patients with X-linked severe combined immune deficiency (X-SCID) in gene-therapy trials using haematopoietic stem cells has led to a re-evaluation of this approach. Using a mouse model for gene therapy of X- SCID, we find that the corrective therapeutic gene IL2RG itself can act as a contributor to the genesis of T-cell lymphomas, with one-third of animals being affected. Gene-therapy trials for X- SCID, which have been based on the assumption that IL2RG is minimally oncogenic, may therefore pose some risk to patients.\u201aÄù 1 Iraq disarmament crisis Leukemia 2 Yellowcake forgery Severe combined immunodeficiency 3 Senate Report of Pre-war Intelligence on Iraq Cancer 4 Iraq and weapons of mass destruction Non-Hodgkin lymphoma 5 Iraq Survey Group AIDS 6 September Dossier ICD-10 Chapter II: Neoplasms; Chapter III: Diseases of the blood and blood-forming organs, and certain disorders involving the immune mechanism 7 Iraq War Bone marrow transplant 8 Scott Ritter Immunosuppressive drug 9 Iraq War- Rationale Acute lymphoblastic leukemia 10 Operation Desert Fox Multiple sclerosis Table 2: First ten concepts of the interpretation vectors for sample text fragments. # Ambiguous word: \u201aÄúBank\u201aÄù Ambiguous word: \u201aÄúJaguar\u201aÄù \u201aÄúBank of America\u201aÄù \u201aÄúBank of Amazon\u201aÄù \u201aÄúJaguar car models\u201aÄù \u201aÄúJaguar (Panthera onca)\u201aÄù 1 Bank Amazon River Jaguar (car) Jaguar 2 Bank of America Amazon Basin Jaguar S-Type Felidae 3 Bank of America Plaza (Atlanta) Amazon Rainforest Jaguar X-type Black panther 4 Bank of America Plaza (Dallas) Amazon.com Jaguar E-Type Leopard 5 MBNA Rainforest Jaguar XJ Puma 6 VISA (credit card) Atlantic Ocean Daimler Tiger 7 Bank of America Tower, Brazil British Leyland Motor Panthera hybrid New York City Corporation 8 NASDAQ Loreto Region Luxury vehicles Cave lion 9 MasterCard River V8 engine American lion 10 Bank of America Corporate Center Economy of Brazil Jaguar Racing Kinkajou Table 3: First ten concepts of the interpretation vectors for texts with ambiguous words. removing small and overly specific concepts (those having fewer than 100 words and fewer than 5 incoming or outgoing links), 241,393 articles were left. We processed the text of these articles by removing stop words and rare words, and stemming the remaining words; this yielded 389,202 distinct terms, which served for representing Wikipedia concepts as attribute vectors. To better evaluate Wikipedia-based semantic interpretation, we also implemented a semantic interpreter based on another large-scale knowledge repository\u201aÄîthe Open Directory Project (ODP, http://www.dmoz.org). The ODP is the largest Web directory to date, where concepts correspond to categories of the directory, e.g., TOP/COMPUTERS/ARTIFICIAL INTELLIGENCE. In this case, interpretation of a text fragment amounts to computing a weighted vector of ODP concepts, ordered by their affinity to the input text. We built the ODP-based semantic interpreter using an ODP snapshot as of April 2004. After pruning the Top/World branch that contains non-English material, we obtained a hierarchy of over 400,000 concepts and 2,800,000 URLs. IJCAI-07 1608 Textual descriptions of the concepts and URLs amounted to 436 Mb of text. In order to increase the amount of training information, we further populated the ODP hierarchy by crawling all of its URLs, and taking the first 10 pages encountered at each site. After eliminating HTML markup and truncating overly long files, we ended up with 70 Gb of additional textual data. After removing stop words and rare words, we obtained 20,700,000 distinct terms that were used to represent ODP nodes as attribute vectors. Up to 1000 most informative attributes were selected for each ODP node using the document frequency criterion [Sebastiani, 2002]. A centroid classifier was then trained, whereas the training set for each concept was combined by concatenating the crawled content of all the URLs cataloged under this concept. Further implementation details are available in [Gabrilovich and Markovitch, 2005]. Using world knowledge requires additional computation. This extra computation includes the (one-time) preprocessing step where the semantic interpreter is built, as well as the actual mapping of input texts into interpretation vectors, performed online. On a standard workstation, the throughput of the semantic interpreter is several hundred words per second. 3.1 Datasets and Evaluation Procedure Humans have an innate ability to judge semantic relatedness of texts. Human judgements on a reference set of text pairs can thus be considered correct by definition, a kind of \u201aÄúgold standard\u201aÄù against which computer algorithms are evaluated. Several studies measured inter-judge correlations and found them to be consistently high [Budanitsky and Hirst, 2006; Jarmasz, 2003; Finkelstein et al., 2002], r =0.88 \u201aàí 0.95. These findings are to be expected\u201aÄîafter all, it is this consensus that allows people to understand each other. In this work, we use two such datasets, which are to the best of our knowledge the largest publicly available collections of their kind. To assess word relatedness, we use the WordSimilarity-353 collection 2 [Finkelstein et al., 2002], which contains 353 word pairs. 3 Each pair has 13\u201aÄì16 human judgements, which were averaged for each pair to produce a single relatedness score. Spearman rank-order correlation coefficient was used to compare computed relatedness scores with human judgements. For document similarity, we used a collection of 50 documents from the Australian Broadcasting Corporation\u201aÄôs news mail service [Lee et al., 2005]. These documents were paired in all possible ways, and each of the 1,225 pairs has 8\u201aÄì12 human judgements. When human judgements have been averaged for each pair, the collection of 1,225 relatedness scores have only 67 distinct values. Spearman correlation is not appropriate in this case, and therefore we used Pearson\u201aÄôs linear correlation coefficient. "},{"aspect":"background","tweet":" 1 Introduction How related are \u201aÄúcat\u201aÄù and \u201aÄúmouse\u201aÄù? And what about \u201aÄúpreparing a manuscript\u201aÄù and \u201aÄúwriting an article\u201aÄù? Reasoning about semantic relatedness of natural language utterances is routinely performed by humans but remains an unsurmountable obstacle for computers. Humans do not judge text relatedness merely at the level of text words. Words trigger reasoning at a much deeper level that manipulates concepts\u201aÄîthe basic units of meaning that serve humans to organize and share their knowledge. Thus, humans interpret the specific wording of a document in the much larger context of their background knowledge and experience. It has long been recognized that in order to process natural language, computers require access to vast amounts of common-sense and domain-specific world knowledge [Buchanan and Feigenbaum, 1982; Lenat and Guha, 1990]. However, prior work on semantic relatedness was based on purely statistical techniques that did not make use of background knowledge [Baeza-Yates and Ribeiro-Neto, 1999; Deerwester et al., 1990], or on lexical resources that incorporate very limited knowledge about the world [Budanitsky and Hirst, 2006; Jarmasz, 2003]. IJCAI-07 1606 We propose a novel method, called Explicit Semantic Analysis (ESA), for fine-grained semantic representation of unrestricted natural language texts. Our method represents meaning in a high-dimensional space of natural concepts derived from Wikipedia (http://en.wikipedia.org), the largest encyclopedia in existence. We employ text classification techniques that allow us to explicitly represent the meaning of any text in terms of Wikipedia-based concepts. We evaluate the effectiveness of our method on automatically computing the degree of semantic relatedness between fragments of natural language text. The contributions of this paper are threefold. First, we present Explicit Semantic Analysis, a new approach to representing semantics of natural language texts using natural concepts. Second, we propose a uniform way for computing relatedness of both individual words and arbitrarily long text fragments. Finally, the results of using ESA for computing semantic relatedness of texts are superior to the existing state of the art. Moreover, using Wikipedia-based concepts makes our model easy to interpret, as we illustrate with a number of examples in what follows. "},{"aspect":"expintro","tweet":""},{"aspect":"problemdef","tweet":""},{"aspect":"solution","tweet":" 2 Explicit Semantic Analysis Our approach is inspired by the desire to augment text representation with massive amounts of world knowledge. We represent texts as a weighted mixture of a predetermined set of natural concepts, which are defined by humans themselves and can be easily explained. To achieve this aim, we use concepts defined by Wikipedia articles, e.g., COMPUTER SCI- ENCE, INDIA, orLANGUAGE. An important advantage of our approach is thus the use of vast amounts of highly organized human knowledge encoded in Wikipedia. Furthermore, Wikipedia undergoes constant development so its breadth and depth steadily increase over time. We opted to use Wikipedia because it is currently the largest knowledge repository on the Web. Wikipedia is available in dozens of languages, while its English version is the largest of all with 400+ million words in over one million articles (compared to 44 million words in 65,000 articles in Encyclopaedia Britannica1 ). Interestingly, the open editing approach yields remarkable quality\u201aÄîa recent study [Giles, 2005] found Wikipedia accuracy to rival that of Britannica. 1 http://store.britannica.com (visited on May 12, 2006). We use machine learning techniques to build a semantic interpreter that maps fragments of natural language text into a weighted sequence of Wikipedia concepts ordered by their relevance to the input. This way, input texts are represented as weighted vectors of concepts, called interpretation vectors. The meaning of a text fragment is thus interpreted in terms of its affinity with a host of Wikipedia concepts. Computing semantic relatedness of texts then amounts to comparing their vectors in the space defined by the concepts, for example, using the cosine metric [Zobel and Moffat, 1998]. Our semantic analysis is explicit in the sense that we manipulate manifest concepts grounded in human cognition, rather than \u201aÄúlatent concepts\u201aÄù used by Latent Semantic Analysis. Observe that input texts are given in the same form as Wikipedia articles, that is, as plain text. Therefore, we can use conventional text classification algorithms [Sebastiani, 2002] to rank the concepts represented by these articles according to their relevance to the given text fragment. It is this key observation that allows us to use encyclopedia directly, without the need for deep language understanding or pre-cataloged common-sense knowledge. The choice of encyclopedia articles as concepts is quite natural, as each article is focused on a single issue, which it discusses in detail. Each Wikipedia concept is represented as an attribute vector of words that occur in the corresponding article. Entries of these vectors are assigned weights using TFIDF scheme [Salton and McGill, 1983]. These weights quantify the strength of association between words and concepts. To speed up semantic interpretation, we build an inverted index, which maps each word into a list of concepts in which it appears. We also use the inverted index to discard insignificant associations between words and concepts by removing those concepts whose weights for a given word are too low. We implemented the semantic interpreter as a centroidbased classifier [Han and Karypis, 2000], which, given a text fragment, ranks all the Wikipedia concepts by their relevance to the fragment. Given a text fragment, we first represent it as a vector using TFIDF scheme. The semantic interpreter iterates over the text words, retrieves corresponding entries from the inverted index, and merges them into a weighted vector of concepts that represents the given text. Let T = {wi} be input text, and let \u201aå©vi\u201aå™ be its TFIDF vector, where vi is the weight of word wi. Let \u201aå©kj\u201aå™ be an inverted index entry for word wi, wherekjquantifies the strength of association of word wi with Wikipedia concept cj, {cj \u201aàà c1,...,cN} (where N is the total number of Wikipedia concepts). Then, the semantic interpretation vector V for text T is a vector of length N, in which the weight of each concept cj is defined as ÔøΩ wi\u201aààT vi ¬∑ kj. Entries of this vector reflect the relevance of the corresponding concepts to text T . To compute semantic relatedness of a pair of text fragments we compare their vectors using the cosine metric. Figure 1 illustrates the process of Wikipedia-based semantic interpretation. Further implementation details are available in [Gabrilovich, In preparation]. In our earlier work [Gabrilovich and Markovitch, 2006], we used a similar method for generating features for text categorization. Since text categorization is a supervised learning task, words occurring in the training documents serve as valu- IJCAI-07 1607 Building Semantic I nterpreter Wikipedia B uilding weighted inverted index Using Semantic I nterpreter Text1 Text2 Semantic Interpreter Weighted vector of W ikipedia concepts word1 wordi wordn Weighted list of concepts (= Wikipedia articles) Weighted inverted index Vector comparison Figure 1: Semantic interpreter R elatedness estimation # Input: \u201aÄúequipment\u201aÄù Input: \u201aÄúinvestor\u201aÄù 1 Tool Investment 2 Digital Equipment Corporation Angel investor 3 Military technology and equipment Stock trader 4 Camping Mutual fund 5 Engineering vehicle Margin (finance) 6 Weapon Modern portfolio theory 7 Original equipment manufacturer Equity investment 8 French Army Exchange-traded fund 9 Electronic test equipment Hedge fund 10 Distance Measuring Equipment Ponzi scheme Table 1: First ten concepts in sample interpretation vectors. able features; consequently, in that work we used Wikipedia concepts to augment the bag of words. On the other hand, computing semantic relatedness of a pair of texts is essentially a \u201aÄúone-off\u201aÄù task, therefore, we replace the bag of words representation with the one based on concepts. To illustrate our approach, we show the ten highest-scoring Wikipedia concepts in the interpretation vectors for sample text fragments. When concepts in each vector are sorted in the decreasing order of their score, the top ten concepts are the most relevant ones for the input text. Table 1 shows the most relevant Wikipedia concepts for individual words (\u201aÄúequipment\u201aÄù and \u201aÄúinvestor\u201aÄù, respectively), while Table 2 uses longer passages as examples. It is particularly interesting to juxtapose the interpretation vectors for fragments that contain ambiguous words. Table 3 shows the first entries in the vectors for phrases that contain ambiguous words \u201aÄúbank\u201aÄù and \u201aÄùjaguar\u201aÄù. As can be readily seen, our semantic interpretation methodology is capable of performing word sense disambiguation, by considering ambiguous words in the context of their neighbors. "},{"aspect":"expcomparison","tweet":""}]}