{"user_name":" Discovering Frequent Closed Itemsets for Association Rules ","user_timeline":[{"aspect":"abstract","tweet":" Abstract. In this paper, we address the problem of finding frequent itemsets in a database. Using the closed itemset lattice framework, we show that this problem can be reduced to the problem of finding frequent closed itemsets. Based on this statement, we can construct efficient data mining algorithms by limiting the search space to the closed itemset lattice rather than the subset lattice. Moreover, we show that the set of all frequent closed itemsets suffices to determine a reduced set of association rules, thus addressing another important data mining problem: limiting the number of rules produced without information loss. We propose a new algorithm, called A-Close, using a closure mechanism to find frequent closed itemsets. We realized experiments to compare our approach to the commonly used frequent itemset search approach. Those experiments showed that our approach is very valuable for dense and/or correlated data that represent an important part of existing databases. "},{"aspect":"expanalysis","tweet":" 6 Conclusion  We presented a new algorithm, called A-Close, for discovering frequent closed itemsets in large databases. This algorithm is based on the pruning of the closed itemset lattice instead of the itemset lattice, which is the commonly used approach. This lattice being a sub-order of the itemset lattice, for many datasets, the number of itemsets considered will be significantly reduced. Given the set of frequent closed itemsets and their support, we showed that we can either deduce all frequent itemsets, or construct a reduced set of valid association rules needless the search for frequent itemsets. We realized experiments in order to compare our approach to the itemset lattice exploration approach. We implemented A-Close and an optimized version of Apriori using prefix-trees. The choice of Apriori leads form the fact that, in practice, it remains one of the most general and powerful algorithms. Those experiments showed that A-Close is very efficient for mining dense and/or correlated data (such as statistical data): on such datasets, the number of itemsets considered and the number of database passes made are significantly reduced  0 500 1000 1500 2000 2500 3000 20% 15% 10% 7.5% 5% 4% 3%  Time (seconds) Minimum support A-Close Apriori Execution times  10 11 12 13 14 15 16 3% 4% 5% 7.5% 10% 15% 20%  Number of passes Minimum support A-Close Apriori Number of database passes  Fig. 4. Performance of Apriori and A-Close on census data C20D10K 10000 20000 30000 40000 50000 60000 70% 75% 80% 85% 90%  Time (seconds) Minimum support A-Close Apriori Execution times  10 12 14 16 18 70% 75% 80% 85% 90%  Number of passes Minimum support A-Close Apriori Number of database passes  Fig. 5. Performance of Apriori and A-Close on census data C73D10K 0 500 1000 1500 2000 2500 3000 3500 4000 10 12 14 16 18 20 22 24  Number of items per transaction A-Close Apriori Fig. 6. Scale-up properties of Apriori and A-Close on census data  compared to those Apriori needs. They also showed that A-Close leads to equivalent performances of the two algorithms for weakly correlated data (such as synthetic data) in which many generators are closed. This leads from the adaptive characteristic of A-Close that consists in determining the first iteration for which it is necessary to compute closures of generators. Such a way, we avoid A-Close many useless closure computations. We think these results are very interesting since dense and/or correlated data represent an important part of all existing data, and since mining such data is considered as very difficult. Statistical, text, biological and medical data are examples of such correlated data. Supermarket data are weakly correlated and quite sparse, but experimental results showed that mining such data is considerably less difficult than mining correlated data. In the first case, executions take some minutes at most whereas in the second case, executions sometimes take several hours. Moreover, A-Close gives an efficient unsupervised classification technic: the closed itemset lattice of an order is dually isomorphic to the Dedekind-MacNeille completion of an order [7], which is the smallest lattice associated with an order. The closest work is Ganter's algorithm [9] which works only in main memory. This feature is very interesting since unsupervised classification is another important problem in data mining [6] and in machine learning.  "},{"aspect":"expdata","tweet":" Test Data We used two kinds of datasets: synthetic data, that simulate market basket data, and census data, that are typical statistical data. The synthetic datasets were generated using the program described in [2]. The census data were extracted from the Kansas 1990 PUMS file (Public Use Microdata Samples), in the same way as [5] for the PUMS file of Washington (unavailable through Internet at the time of the experiments). Unlike in [5] though, we did not put an upper bound on the support, as this distorts each algorithm results in different ways. We therefore took smaller datasets containing the first 10,000 persons. "},{"aspect":"background","tweet":" 1 Introduction  The discovery of association rules was first introduced in [1]. This task consists in determining relationships between sets of items in very large databases. Agrawal's statement of this problem is the following [1, 2]. Let I = fi 1 ; i 2 ; : : : ; i m g  be a set of m items. Let the database D = ft 1 ; t 2 ; : : : ; t n g be a set of n transactions, each one identified by its unique TID. Each transaction t consists of a set of items I from I. If kIk = k, then I is called a k-itemset. An itemset I is  contained in a transaction t 2 D if I ` t. The support of an itemset I is the percentage of transactions in D containing I . Association rules are of the form  r : I 1  c  \\Gamma! I 2 , with I 1 ; I 2 ae I and I 1 \" I 2 = ;. Each association rule r has a support defined as support(r) = support(I 1 [ I 2 ) and a confidence c defined as  confidence(r) = support(I 1 [I 2 ) = support(I 1 ). Given the user defined minimum support minsup and minimum confidence minconf thresholds, the problem of mining association rules can be divided into two sub-problems [1]: 1. Find all frequent itemsets in D, i.e. itemsets with support greater or equal to minsup.  2. For each frequent itemset I 1 found, generate all association rules I 2  c  \\Gamma! I 1 \\GammaI 2  where I 2 ae I 1 , with confidence c greater or equal to minconf.  Once all frequent itemsets and their support are known, the association rule generation is straightforward. Hence, the problem of mining association rules is reduced to the problem of determining frequent itemsets and their support. Recent works demonstrated that the frequent itemset discovery is also the key stage in the search for episodes from sequences and in finding keys or inclusion as well as functional dependencies from a relation [12]. All existing algorithms use one of the two following approach: a levelwise [12] bottom-up search [2, 5, 13, 16, 17] or a simultaneous bottom-up and top-down search [3, 10, 20]. Although they are dissimilar, all those algorithms explore the subset lattice (itemset lattice) for finding frequent itemsets: they all use the basic properties that all subsets of a frequent itemset are frequent and that all supersets of an infrequent itemset are infrequent in order to prune elements of the itemset lattice. In this paper, we propose a new efficient algorithm, called A-Close, for finding frequent closed itemsets and their support in a database. Using a closure mechanism based on the Galois connection, we define the closed itemset lattice  which is a sub-order of the itemset lattice, thus often much smaller. This lattice is closely related to the Galois lattice [4, 7] also called concept lattice [19]. The closed itemset lattice can be used as a formal framework for discovering frequent itemsets given the basic properties that the support of an itemset I is equal to the support of its closure and that the set of maximal frequent itemsets is identical to the set of maximal frequent closed itemsets. Then, once A-Close has discovered all frequent closed itemsets and their support, we can directly determine the frequent itemsets and their support. Hence, we reduce the problem of mining association rules to the problem of determining frequent closed itemsets and their support. Using the set of frequent closed itemsets, we can also directly generate a reduced set of association rules without having to determine all frequent itemsets, thus lowering the algorithm computation cost. Moreover, since there can be thousands of association rules holding in a database, reducing the number of rules produced without information loss is an important problem for the understandability of the result [18]. Empirical evaluations comparing A-Close to an optimized version of Apriori showed that they give nearly always equivalent results for weakly correlated data (such as synthetic data) and that A-Close clearly outperforms Apriori for correlated data (such as statistical or text data). The rest of the paper is organized as follows. In Section 2, we present the closed itemset lattice. In Section 3, we propose a new model for association rules based on the Galois connection and we characterize a reduced set of association rules. In Section 4, we describe the A-Close algorithm. Section 5 gives experimental results on synthetic data  1  and census data using the PUMS file for Kansas USA  2  and Section 6 concludes the paper.  1  http://www.almaden.ibm.com/cs/quest/syndata.html  2  ftp://ftp2.cc.ukans.edu/pub/ippbr/census/pums/pums90ks.zip "},{"aspect":"expintro","tweet":" We implemented the Apriori and A-Close algorithms in C++, both using the same prefix-tree structure that improves Apriori efficiency. Experiments were realized on a 43P240 bi-processor IBM Power-PC running AIX 4.1.5 with a CPU clock rate of 166 MHz, 1GB of main memory and a 9GB disk. Each execution uses only one processor (the application was single-threaded) and was allowed a maximum of 128MB. "},{"aspect":"problemdef","tweet":" 2 Closed Itemset Lattices  In this section, we define data mining context, Galois connection, Galois closure operators, closed itemsets and closed itemset lattice. Interested readers should read [4, 7, 19] for further details on order and lattice theory.  Definition 1 (Data mining context). A data mining context  3  is a triple D = (O; I; R). O and I are finite sets of objects and items respectively. R ` O \\Theta I  is a binary relation between objects and items. Each couple (o; i) 2 R denotes the fact that the object o 2 O is related to the item i 2 I.  Definition 2 (Galois connection). Let D = (O; I; R) be a data mining context. For O ` O and I ` I, we define:  f(O) : 2  O  ! 2  I  g(I) : 2  I  ! 2  O  f(O)=fi 2 I j 8o 2 O; (o; i) 2 Rg g(I)=fo 2 O j 8i 2 I ; (o; i) 2 Rg  f(O) associates with O the items common to all objects o 2 O and g(I) associates with I the objects related to all items i 2 I. The couple of applications (f; g) is a Galois connection between the power set of O (i.e. 2  O  ) and the power set of I  (i.e. 2  I  ). The following properties hold for all I ; I 1 ; I 2 ` I and O; O 1 ; O 2 ` O:  (1) I 1 ` I 2 ) g(I 1 ) ' g(I 2 ) (1') O 1 ` O 2 ) f(O 1 ) ' f(O 2 )  (2) O ` g(I) () I ` f(O)  Definition 3 (Galois closure operators). The operators h = fffig in 2  I  and  h  0  = gffif in 2  O  are Galois closure operators  4  . Given the Galois connection (f; g),  the following properties hold for all I ; I 1 ; I 2 ` I and O; O 1 ; O 2 ` O [4, 7, 19]: Extension : (3) I ` h(I) (3') O ` h  0  (O)  Idempotency : (4) h(h(I)) = h(I) (4') h  0  (h  0  (O)) = h  0  (O)  Monotonicity : (5) I 1 ` I 2 ) h(I 1 ) ` h(I 2 ) (5') O 1 ` O 2 ) h  0  (O 1 ) ` h  0  (O 2 )  Definition 4 (Closed itemsets). An itemset C ` I from D is a closed itemset iff h(C) = C. The smallest (minimal) closed itemset containing an itemset I is obtained by applying h to I. We call h(I) the closure of I.  Definition 5 (Closed itemset lattice). Let C be the set of closed itemsets derived from D using the Galois closure operator h. The pair L C = (C; ) is a complete lattice called closed itemset lattice. The lattice structure implies two properties: i) There exists a partial order on the lattice elements such that, for every elements  C 1 ; C 2 2 L C , C 1  C 2 , iff C 1 ` C 2 5  . ii) All subsets of L C have one greatest lower bound, the Join element, and one lowest upper bound, the Meet element.  3  By extension, we call database a data mining context afterwards.  4  Here, we use the following notation: fffig(I) = f(g(I)) and g ffif (O) = g(f(O)).  5  C1 is a sub-closed itemset of C2 and C2 is a sup-closed itemset of C1 .  Below, we give the definitions of the Join and Meet elements extracted from the basic theorem on Galois (concept) lattices [4, 7, 19]. For all S ` L C : Join (S) = h(  [  C2S  C); Meet (S) =  \"  C2S  C OID Items 1 A C D 2 B C E 3 A B C E 4 B E 5 A B C E  ABCDE ABCE AC BE C ACD BCE  Fig. 1. The data mining context D and its associated closed itemset lattice. "},{"aspect":"solution","tweet":" 3 Association Rule Model  In this section, we define frequent and maximal frequent itemsets and closed itemsets using the Galois connection. We then define association rules and valid association rules, and we characterise a reduced set of valid association rules in a data mining context D. 3.1 Frequent Itemsets Definition 6 (Itemset support). Let I ` I be a set of items from D. The support count of the itemset I in D is:  support(I) =  kg(I)k kOk  Definition 7 (Frequent itemsets). The itemset I is said to be frequent if the support of I in D is at least minsup. The set L of frequent itemsets in D is:  L = fI ` I j support(I)  minsupg  Definition 8 (Maximal frequent itemsets). Let L be the set of frequent itemsets. We define the set M of maximal frequent itemsets in D as:  M = fI 2 L j 6 9I  0  2 L; I ae I  0  g  Property 1. All subsets of a frequent itemset are frequent (intuitive in [2]).  Proof. Let I ; I  0  ` I, I 2 L and I  0  ` I . According to Property (1) of the Galois connection: I  0  ` I =) g(I  0  ) ' g(I) =) support(I  0  )  support(I)  minsup.  So, we get: I  0  2 L.  Property 2. All supersets of an infrequent itemset are infrequent (intuitive in [2]).  Proof. Let I ; I  0  ` I, I  0  = 2 L and I  0  ` I . According to Property (1) of the Galois connection: I ' I  0  =) g(I) ` g(I  0  ) =) support(I)  support(I  0  )  minsup.  So, we get: I = 2 L. 3.2 Frequent Closed Itemsets Definition 9 (Frequent closed itemsets). The closed itemset C is said to be frequent if the support of C in D is at least minsup. We define the set FC of frequent closed itemsets in D as:  FC = fC ` I j C = h(C)  support(C)  minsupg  Definition 10 (Maximal frequent closed itemsets). Let FC be the set of frequent closed itemsets. We define the set MC of maximal frequent closed itemsets in D as:  MC = fC 2 FC j 6 9C  0  2 FC; C ae C  0  g  Property 3. The support of an itemset I is equal to the support of its closure:  support(I) = support(h(I)). Proof. Let I ` I be an itemset. The support of I in D is:  support(I) =  kg(I)k kOk  Now, we consider h(I), the closure of I . Let's show that h  0  (g(I)) = g(I). We have g(I) ` h(g(I)) (extension property of the Galois closure) and I ` h(I) )  g(h(I)) ` g(I) (Property (1) of the Galois connection). We deduce that h  0  (g(I))=  g(I), and therefore we have: support(h(I)) =  kg(h(I))k kOk  =  kh  0  (g(I))k  kOk  =  kg(I)k kOk  = support(I)  Property 4. The set of maximal frequent itemsets M is identical to the set of maximal frequent closed itemsets MC. Proof. It suffices to demonstrate that 8I 2 M , I is closed, i.e. I = h(I). Let  I 2 M be a maximal frequent itemset. According to Property (3) of the Galois connection I ` h(I) and, since I is maximal and support(h(I)) = support(I)   minsup, we conclude that I = h(I). I is a maximal frequent closed itemset. Since all maximal frequent itemsets are also maximal frequent closed itemsets, we get: M = MC.  3.3 Association Rule Semantics Definition 11 (Association rules). An association rule is an implication between itemsets of the form I 1  c  \\Gamma! I 2 where I 1 ; I 2 ae I and I 1 \" I 2 = ;. Below, we define the support and confidence c of an association rule r : I 1  c  \\Gamma! I 2 using the Galois connection:  support(r) =  kg(I 1 [ I 2 )k  kOk  ; confidence(r) =  support(I 1 [ I 2 )  support(I 1 ) =  kg(I 1 [ I 2 )k  kg(I 1 )k  Definition 12 (Valid association rules). A valid association rules is an association rules with support and confidence greater or equal to the minsup and minconf thresholds respectively. We define the set AR of valid association rules in D using the set MC of maximal frequent closed itemsets as:  AR(D; minsup, minconf) = fr : I 2  c  \\Gamma! I 1 \\Gamma I 2 ; I 2 ae I 1 j I 1 2 L =  [  C2MC  2  C  and  confidence(r)  minconfg 3.4 Reduced Set of Association Rules  Let I 1 ; I 2 ae I and I 1 \" I 2 = ;. An association rule r : I 1  c  \\Gamma! I 2 is an exact association rule if c = 1. Then, r is noted r : I 1 ) I 2 . An association rule  r : I 1  c  \\Gamma! I 2 where c ! 1 is called an approximate association rule. Let D be a data mining context.  Definition 13 (Pseudo-closed itemsets). An itemset I ` I from D is a pseudo-closed itemset iff h(I) 6= I and 8I  0  ae I such as I  0  is a pseudo-closed itemset, we have h(I  0  ) ` I.  Theorem 1 (Exact association rules basis [8]). Let P be the set of pseudoclosed itemsets and R the set of exact association rules in D. The set E = fr :  I 1 )h(I 1 ) \\Gamma I 1 j I 1 2 Pg is a basis for all exact association rules. 8r  0  2 R where confidence(r  0  ) = 1  minconf we have E j= r  0  .  Corollary 1 (Exact valid association rules basis). Let FP be the set of frequent pseudo-closed itemsets in D. The set BE = fr : I 1 )h(I 1 ) \\Gamma I 1 j I 1 2 FPg  is a basis for all exact valid association rules. 8r  0  2 AR where confidence(r  0  ) = 1 we have BE j= r  0  .  Theorem 2 (Reduced set of approximate association rules [11]). Let C  be the set of closed itemsets and R the set of approximate association rules in  D. The set A = fr : I 1  c  \\Gamma! I 2 \\Gamma I 1 j I 2 ae I 1  I 1 ; I 2 2 Cg is a correct reduced set for all approximate association rules. 8r  0  2 R where minconf  confidence(r  0  )  ! 1 we have A j= r  0  .  Corollary 2 (Reduced set of approximate valid association rules). Let  FC be the set of frequent closed itemsets in D. The set BA = fr : I 1  c  \\Gamma! I 2 \\Gamma  I 1 j I 2 ae I 1  I 1 ; I 2 2 FCg is a correct reduced set for all approximate valid assocition rules. 8r  0  2 AR where confidence(r  0  )  1 we have BA j= r  0  .  4 A-Close Algorithm  In this section, we present our algorithm for finding frequent closed itemsets and their supports in a database. Section 4.1 describes its principle. In Section 4.2 to 4.5, we give the pseudo-close of the algorithm and the sub-functions it uses. Section 4.6 provides an example and the proof of the algorithm correctness. 4.1 A-Close Principle  A closed itemset is a maximal set of items common to a set of objects. For example, in the database D in Figure 1, the itemset BCE is a closed itemset since it is the maximal set of items common to the objects f2; 3; 5g. BCE is called a frequent closed itemset for minsup = 2 as support(BCE) = kf2; 3; 5gk = 3   minsup. In a basket database, this means that 60% of customers (3 customers on a total of 5) purchase at most the items B; C and E. The itemset BC is not a closed itemset since it is not a maximal group of items common to some objects: all customers purchasing the items B and C also purchase the item E.  The closed itemset lattice of a finite relation (the database) is dually isomorphic to the Galois lattice [4, 7], also called concept lattice [19]. Based on the closed itemset lattice properties (Section 2 and 3), using the result of A-Close we can generate all frequent itemsets from a database D through the two following phases: 1. Discover all frequent closed itemsets in D, i.e. itemsets that are closed and have support greater or equal to minsup.  2. Derive all frequent itemsets from the frequent closed itemsets found in phase 1. That is generate all subsets of the maximal frequent closed itemsets and derive their support from the frequent closed itemset supports. A different algorithm for finding frequent closed itemsets and algorithms for deriving frequent itemsets and generating valid association rules are presented in [15]. Using the result of A-Close, we can directly generate the reduced set of valid association rules defined in Section 3.4 instead of determining all frequent itemsets. The procedure is the following: 1. Discover all frequent closed itemsets in D.  2. Determine the exact valid association rule basis: determine the pseudo-closed itemsets in D and then generate all rules r : I 1 ) I 2 \\Gamma I 1 j I 1 ae I 2 where I 2  is a frequent closed itemset and I 1 is a frequent pseudo-closed itemset. 3. Construct the reduced set of approximate valid association rules: generate all rules of the form: r : I 1  c  \\Gamma! I 2 \\Gamma I 1 j I 1 ae I 2 where I 1 and I 2 are frequent closed itemsets. In the two cases, the first phase is the most computationally intensive part. After this phase, no more database pass is necessary and the later phases can be solved easily in a straightforward manner. Indeed, the first phase has given us all information needed by the next ones.  A-Close discovers the frequent closed itemsets as follows. Based on the closed itemset properties, it determines a set of generators that will give us all frequent closed itemsets by application of the Galois closure operator h. An itemset p is a generator of a closed itemset c if it is one of the smallest itemsets (there can be more than one) that will determine c using the Galois closure operator: h(p) = c.  For instance, in the database D (Figure 1), BC and CE are generators of the closed itemset BCE. The itemsets B, C and E are not generators of BCE since  h(C) = C and h(B) = h(E) = BE. The itemset BCE is not a generator of itself since it includes BC and CE: BCE is not one of the smallest itemsets for which closure is BCE.  The algorithm constructs the set of generators in a levelwise manner: (i+1)- generators  6  are created using i-generators in G i . Then, their support is counted and the useless generators are pruned. According to their supports and the supports of their i-subsets in G i , infrequent generators and generators that have the same closure as one of their subsets are deleted from G i+1 . In the previous example, the support of the generator BCE is the same as the support of generators BC and CE since they have the same closure (Property 3). Once all frequent useful generators are found, their closures are determined, giving us the set of all frequent closed itemsets. For reducing the cost of the closure computation when possible, we introduce the following optimization. We determine the first iteration of the algorithm for which a (i+1)-generator was pruned because it had the same closure as one of its i-subsets. In all iterations preceding the i  th  one, the generators created are closed and their closure computation is useless. Hence, we can limit the closure computation to generators of size greater or equal to i. For this purpose, the level variable indicates the first iteration for which a generator was pruned by this pruning strategy. 4.2 Discovering Frequent Closed Itemsets  As in the Apriori algorithm, items are sorted in lexicographic order. The pseudocode for discovering frequent closed itemsets is given in Algorithm 1. The notation is given in Table 1. In each of the iterations that construct the candidate generators, one pass over the database is necessary in order to count the support of the candidate generators. At the end of the algorithm, one more pass is needed for determining the closures of generators that are not closed. If all generators are closed, this pass is not made. First, the algorithm determines the set G 1 of frequent 1-generators and their support (step 1 to 5). Then, the level variable is set to 0 (step 6). In each of the following iterations (step 7 to 9), the AC-Generator function (Section 4.4) is applied to the set of generators G i , determining the candidate (i+1)-generators and their support in G i+1 (step 8). This process takes place until G i is empty. Finally, closures of all generators produced are determined (step 10 to 14). Using the level variable, we construct two sets of generators. The set G which contains generators p for which size is less than level \\Gamma 1, and so that are closed (p = h(p)).  6  A generator of size i is called an i-generator.  Set Field Contains  G i generator A generator of size i.  support Support count of the generator: support = count(generator) G; G  0  generator A generator of size i.  closure Closure of the generator: closure = h(generator). support Support count of the generator and its closure:  support = count(closure) = count(generator) (Property 3).  FC closure Frequent closed itemset (closed itemset with support  minsup).  support Support count of the frequent closed itemset.  Table 1. Notation The set G  0  which contains generators for which size is at least level \\Gamma 1, among which some are not closed, and so for which closure computation is necessary. The closures of generators in G  0  are determined by applying the AC-Generator  function (Section 4.4) to G  0  (step 15). Then, all frequent closed itemsets have been produced and their support is known (see Theorem 3). Algorithm 1 A-Close algorithm  1) generators in G1 / f1-itemsetsg;  2) G1 / Support-Count(G1 ); 3) forall generators p 2 G1 do begin  4) if (support(p) ! minsup) then delete p from G1 ; // Pruning infrequent 5) end  6) level / 0; 7) for (i / 1; G i .generator 6= ;; i++) do begin  8) G i+1 / AC-Generator(G i ); // Creates (i+1)-generators 9) end  10) if (level ? 2) then begin  11) G /  S  fG j j j ! level-1g; // Those generators are all closed 12) forall generators p 2 G do begin  13) p.closure / p.generator;  14) end  15) end  16) if (level 6= 0) then begin  17) G  0  /  S  fG j j j  level-1g; // Some of those generators are not closed 18) G  0  / AC-Closure(G  0  ); 19) end  20) Answer FC / fc.closure,c.supportjc 2 G [ G  0  g; 4.3 Support-Count Function  The function takes the set G i of frequent i-generators as argument. It returns the set G i with, for each generator p 2 G i , its support count: support(p) = kfo 2 O j p ` f(fog)k. The pseudo-code of the function is given in Algorithm 2.  Algorithm 2 Support-Count function  1) forall objects o 2 O do begin  2) Go / Subset(G i .generator,f(fog)); // Generators that are subsets of f(fog) 3) forall generators p 2 Go do begin  4) p.support++;  5) end  6) end The Subset function quickly determines which generators are contained in an object  7  , i.e. generators that are subsets of f(fog). For this purpose, generators are stored in a prefix-tree structure derived from the one proposed in [14]. 4.4 AC-Generator Function  The function takes the set G i of frequent i-generators as argument. Based on Lemma 1 and 2, it returns the set G i+1 of frequent (i+1)-generators. The pseudocode of the function is given in Algorithm 3.  Lemma 1. Let I 1 ; I 2 be two itemsets. We have:  h(I 1 [ I 2 ) = h(h(I 1 ) [ h(I 2 ))  Proof. Let I 1 and I 2 be two itemsets. According to the extension property of the Galois closure operators:  I 1 ` h(I 1 ) and I 2 ` h(I 2 ) =) I 1 [ I 2 ` h(I 1 ) [ h(I 2 ) =) h(I 1 [ I 2 ) ` h(h(I 1 ) [ h(I 2 )) (1) Obviously, I 1 ` I 1 [ I 2 and I 2 ` I 1 [ I 2 . So h(I 1 ) ` h(I 1 [ I 2 ) and h(I 2 ) `  h(I 1 [I 2 ). According to the idempotency property of the Galois closure operators:  h(h(I 1 )[h(I 2 )) ` h(h(I 1 [I 2 )) =) h(h(I 1 )[h(I 2 )) ` h(I 1 [I 2 ) (2) From (1) and (2), we conclude that h(I 1 [ I 2 ) = h(h(I 1 ) [ h(I 2 )).  Lemma 2. Let I 1 be an itemset and I 2 a subset of I 1 where support(I 1 ) = support(I 2 ). Then we have h(I 1 ) = h(I 2 ) and 8I 3 ` I, h(I 1 [ I 3 ) = h(I 2 [ I 3 ).  Proof. Let I 1 ; I 2 be two itemsets where I 2 ae I 1 and support(I 1 ) = support(I 2 ). Then, we have that kg(I 1 )k = kg(I 2 )k and we deduce that g(I 1 ) = g(I 2 ). From this, we conclude f(g(I 1 )) = f(g(I 2 )) =) h(I 1 ) = h(I 2 ). Let I 3 ` I be an itemset. Then according to Lemma 1:  h(I 1 [ I 3 ) = h(h(I 1 ) [ h(I 3 )) = h(h(I 2 ) [ h(I 3 )) = h(I 2 [ I 3 )  7  We say that an itemset I is contained in object o if o is related to all items i 2 I.  Corollary 3. Let I be an i-generator and S = fs 1 ; s 2 ; : : : ; s j g a set of (i \\Gamma 1)-  subsets of I where  S  s2S  s = I. If 9s 2 S such as support(s) = support(I), then  h(I) = h(s).  Proof. Derived from Lemma 2. The AC-Generator function works as follows. We first apply the combinatorial phase of Apriori-Gen [2] to the set of generators G i in order to obtain a set of candidate (i+1)-generators: two generators of size i in G i with the same first  i \\Gamma 1 items are joined, producing a new potential generator of size i + 1 (step 1 to 4). Then, the potential generators produced that will lead to useless computations (infrequent closed itemsets) or redundancies (frequent closed itemsets already produced) are pruned from G i+1 as follows. First, like in Apriori-Gen, G i+1 is pruned by removing every candidate (i+1)- generator c such that some i-subset of c is not in G i (step 8 and 9). Using this strategy, we prune two kinds of itemsets: first, all supersets of infrequent generators (that are also infrequent according to Property 2); second, all generators that have the same support as one of their subset and therefore have the same closure (see Theorem 3). Let's take an example. Suppose that the set of frequent closed itemsets G 2 contains the generators AB;AC. The AC-Generator function will create ABC = AB [ AC as a new potential generator in G 3 and the first pruning will remove ABC since BC = 2 G 2 . Next, the supports of the remaining candidate generators in G i+1 are determined and, based on Property 2, those with support less than minsup are deleted from G i+1 (step 7). The third pruning strategy works as follows. For each candidate generator  c in G i+1 , we test if the support of one of its i-subsets s is equal to the support of c. In that case, the closure of c will be equal to the closure of s (see Corollary 3), so we remove c from G i+1 (step 10 to 13). Let's give another example. Suppose that the final set of generators G 2 contains frequent generators  AB;AC;BC and their respective supports 3; 2; 3. The AC-Generator function will create ABC = AB [ AC as a new potential generator in G 3  and suppose it determines its support is 2. The third prune step will remove  ABC from G 3 since support(ABC) = support(AC). Indeed, we deduce that  closure(ABC) = closure(AC) and the computation of the closure of ABC is useless. For the optimization of the generator closure computation in Algorithm 1, we determine the iteration at which the second prune suppressed a generator (variable level). 4.5 AC-Closure Function  The AC-Closure function takes the set of frequent generators G, for which closures must be determined, as argument. It updates G with, for each generator  p 2 G, the closed itemset p.closure obtained by applying the closure operator  h to p. Algorithm 4 gives the pseudo-code of the function. The method used to compute closures is based on Proposition 1.  Algorithm 3 AC-Generator function  1) insert into G i+1  2) select p.item1 , p.item2 , : : : , p.item i , q.item i  3) from G i p, G i q  4) where p.item1 = q.item1 , : : : , p.item i\\Gamma1 = q.item i\\Gamma1 , p.item i ! q.item i ; 5) forall candidate generators c 2 G i+1 do begin  6) forall i-subsets s of c do begin  7) if (s = 2 G i ) then delete c from G i+1 ; 8) end  9) end  10) G i+1 / Support-Count(G i+1 ); 11) forall candidate generators c 2 G i+1 do begin  12) if (support(c) ! minsup) then delete c from G i+1 ; // Pruning infrequent 13) else do begin  14) forall i-subsets s of c do begin  15) if (support(s) = support(c)) then begin  16) delete c from G i+1 ; 17) if (level = 0) then level / i; // Iteration number of the first prune 18) endif  29) end  20) end  21) end  22) Answer /  S  fc 2 G i+1g; Proposition 1. The closed itemset h(I) corresponding to the closure by h of the itemset I is the intersection of all objects in the database that contain I:  h(I) =  \"  o2O  ff(fog) j I ` f(fog)g Proof. We define H =  T  o2S f(fog) where S = fo 2 O j I ` f(fog)g. We have  h(I) = f(g(I)) =  T  o2g(I) f(fog) =  T  o2S  0 f(fog) where S  0  = fo 2 O j o 2 g(I)g.  Let's show that S  0  = S: I ` f(fog) () o 2 g(I) o 2 g(I) () I ` f(g(I)) ` f(fog)  We conclude that S = S  0  , thus h(I) = H . Using Proposition 1, only one database pass is necessary to compute the closures of the generators. The function works as follows. For each object o in D, the set G o is created (step 2). G o contains all generators in G that are subsets of the object itemset f(fog). Then, for each generator p in G o , the associated closed itemset p.closure is updated (step 3 to 6). If the object o is the first one containing the generator, p.closure is empty and the object itemset f(fog) is assigned to it (step 4). Otherwise, the intersection between p.closure and the object itemset gives the new p.closure (step 5). At the end, the function returns  Algorithm 4 AC-Closure function  1) forall objects o 2 O do begin  2) Go / Subset(G.generator,f(fog)); // Generators that are subsets of f(fog) 3) forall generators p 2 Go do begin  4) if (p.closure = ;) then p.closure / f(fog);  5) else p.closure / p.closure \" f(fog);  6) end  7) end  8) Answer /  S  fp 2 G j 6 9p  0  2 G; closure(p  0  )=closure(p)g; for each generator p in G, the closed itemset p.closure corresponding to the intersection of all objects containing p. 4.6 Example and Correctness  Figure 2 gives the execution of A-Close for a minimum support of 2 (40%) on the data mining context D given in Figure 1. First, the algorithm determines the set  G 1 of 1-generators and their support (step 1 and 2), and the infrequent generator  D is deleted form G 1 (step 3 to 5). Then, generators in G 2 are determined by applying the AC-Generator function to G 1 (step 8): the 2-generators are created by union of generators in G 1 , their support is determined and the three pruning strategies are applied. Generators AC and BE are pruned since support(AC) = support(A) and support(BE) = support(B), and the level variable is set to 2. Calling AC-Generator with G 2 produces 3-generators in G 3 . The only generator created in G 3 is ABE since only AB and AE have the same first item. The three pruning strategies are applied and the second one removes ABE form  G 3 as BE = 2 G 2 . Then, G 3 is empty and the iterative construction of sets G i  terminates (the loop in step 7 to 9 stops). The sets G and G  0  are constructed using the level variable (step 10 and 11): G  is empty and G  0  contains generators from G 1 and G 2 . The closure function ACClosure is applied to G  0  and the closures of all generators in G  0  are determined (step 15). Finally, duplicates closures are removed from G  0  by AC-Closure and the result is returned to the set FC which therefore contains AC,BE,C,ABCE  and BCE, that are all frequent closed itemsets in D.  Lemma 3. For p ` I such as kpk ? 1, if p = 2 G kpk and support(p)  minsup then 9s 1 ; s 2 ` I, s 1 ae s 2 ` p and ks 1 k = ks 2 k \\Gamma 1 such as h(s 1 ) = h(s 2 ) and  s 1 2 G ks1 k . Proof. We show this using a recurrence. For kpk = 2, we have p = s 2 and  9s 1 2 G 1 j s 1 ae s 2 and support(s 1 ) = support(s 2 ) =) h(s 1 ) = h(s 2 ) (Lemma 3 is obvious). Then, supposing that Lemma 3 is true for kpk = i, let's show that it is true for kpk = i + 1. Let p ` I j kpk = i + 1 and p = 2 G kpk . There are two possible cases: (1) 9p  0  ae p j kp  0  k = i and p  0  = 2 G kp  0  k  (2) 9p  0  ae p j kp  0  k = i and p  0  2 G kp  0  k and support(p) = support(p  0  ) =) h(p) =  Support-Count  \\Gamma!  G1  Generator Support  fAg 3  fBg 4  fCg 4  fDg 1  fEg 4 Pruning infrequent generators  \\Gamma!  G1  Generator Support  fAg 3  fBg 4  fCg 4  fEg 4 AC-Generator  \\Gamma!  G2  Generator Support  fABg 2  fACg 3  fAEg 2  fBCg 3  fBEg 4  fCEg 3 Pruning  \\Gamma!  G2  Generator Support  fABg 2  fAEg 2  fBCg 3  fCEg 3 AC-Closure  \\Gamma!  G  0  Generator Closure Support  fAg fACg 3  fBg fBEg 4  fCg fCg 4  fEg fBEg 4  fABg fABCEg 2  fAEg fABCEg 2  fBCg fBCEg 3  fCEg fBCEg 3 Pruning  \\Gamma!  Answer : FC  Closure Support  fACg 3  fBEg 4  fCg 4  fABCEg 2  fBCEg 3 Fig. 2. A-Close frequent closed itemset discovery for minsup = 2 (40%) h(p  0  ) (Lemma 2) If (1) then according to the recurrence hypothesis, 9s 1 ae s 2 ` p  0  ae p such as  h(s 1 ) = h(s 2 ) and s 1 2 G ks1 k . If (2) then we identify s 1 to p  0  and s 2 to p.  Theorem 3. The A-Close algorithm generates all frequent closed itemsets. Proof. Using a recurrence, we show that 8p ` I j support(p)  minsup we have  h(p) 2 FC. We first demonstrate the property for the 1-itemsets: 8p ` I where  kpk = 1, if support(p)  minsup then p 2 G 1 ) h(p) 2 FC. Let's suppose that 8p ` I such as kpk = i we have h(p) 2 FC. We then demonstrate that  8p ` I where kpk = i + 1 we have h(p) 2 FC. If p 2 G kpk then h(p) 2 FC.  Else, if p = 2 G kpk and according to Lemma 3, we have: 9s 1 ae s 2 ` p j s 1 2  G ks1 k and h(s 1 ) = h(s 2 ). Now h(p) = h(s 2 [ p \\Gamma s 2 ) = h(s 1 [ p \\Gamma s 2 ) and  ks 1 [ p \\Gamma s 2 k = i, therefore in conformity with the recurrence hypothesis we conclude that h(s 1 [ p \\Gamma s 2 ) 2 FC and so h(p) 2 FC.  "},{"aspect":"expcomparison","tweet":" Parameter T10I4D100K T20I6D100K C20D10K C73D10K Average size of the objects 10 20 20 73 Total number of items 1000 1000 386 2178 Number of objects 100K 100K 10K 10K Average size of the maximal poten- 4 6 - - -tially frequent itemsets  Table 2. Notation Results on Synthetic Data Figure 3 shows the execution times of Apriori and A-Close on the datasets T10I4D100K and T20I6D100K. We can observe that both algorithms always give similar results except for executions with minsup  = 0.5% and 0.33% on T20I6D100. This similitude comes from the fact that data are weakly correlated and sparse in such datasets. Hence, the sets of generators in A-Close and frequent itemsets in Apriori are identical, and the closure mechanism does not help in jumping iterations. In the two cases where Apriori outperforms A-Close, there was in the 4  th  iteration a generator that has been pruned because it had the same support as one of its subsets. As a consequence, A-Close determined closures of all generators with size greater or equal than 3. Results on Census Data Experiments were conducted on the two census datasets using different minsup ranges to get meaningful response times and to accommodate with the memory space limit. Results for the C20D10K and C73D10K datasets are plotted on Figure 4 and 5 respectively. A-Close always significantly outperforms Apriori, for execution times as well as number of database passes. Here, contrarily to the experiments on synthetic data, the differences between execution times can be measured in minutes for C20D10K and in hours for  0 5 10 15 20 25 30 35 40 2% 1.5% 1% 0.75% 0.5% 0.33%  Time (seconds) Minimum support A-Close Apriori Execution times on T10I4D100K  100 200 300 400 500 600 700 800 0.33% 0.5% 0.75% 1% 1.5% 2%  Time (seconds) Minimum support A-Close Apriori Execution times on T20I6D100K  Fig. 3. Performance of Apriori and A-Close on synthetic data C73D10K. It should furthermore be noted that Apriori could not be run for minsup  lower than 3% on C20D10K and lower than 70% on C73D10K as it exceeds the memory limit. Census datasets are typical of statistical databases: highly correlated and dense data. Many items being extremely popular, this leads to a huge number of frequent itemsets from which few are closed. Scale up Properties on Census Data We finally examined how Apriori and A-Close behave as the object size is increased in census data. The number of objects was fixed to 10,000 and the minsup level was set to 10%. The object size varied from 10 (281 total items) up to 24 (408 total items). Apriori could not be run for higher object sizes. Results are shown in Figure 6. We can see here that, the scale up properties of A-Close are far better than those of Apriori. "}]}