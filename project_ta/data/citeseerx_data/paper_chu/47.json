{"user_name":"  Frequent Patterns without Candidate Generation ","user_timeline":[{"aspect":"abstract","tweet":" Abstract SIGMOD'2000 Paper ID: 196 Mining frequent patterns in transaction databases, time-series databases, and many other kinds of databases has been studied popularly in data mining research. Most of the previous studies adopt an Apriori-like candidate set generation-and-test approach. However, candidate set generation is still costly, especially when there exist long patterns. In this study, we propose a novel frequent pattern tree (FP-tree) structure, which is an extended pre xtree structure for storing compressed, crucial information about frequent patterns, and develop an e cient FP-tree-based mining method, FP-growth, for mining the complete set of frequent patterns by pattern fragment growth. E ciency of mining is achieved with three techniques: (1) a large database is compressed into a highly condensed, much smaller data structure, which avoids costly, repeated database scans, (2) our FP-treebased mining adopts a pattern fragment growth method to avoid the costly generation of a large number of candidate sets, and (3) a partitioning-based divide-and-conquer method is used to dramatically reduce the search space. Our performance study shows that the FP-growth method is e cient and scalable for mining both long and short frequent patterns, and is about an order of magnitude faster than the Apriori algorithm and also faster than some recently reported new frequent pattern mining methods. "},{"aspect":"expanalysis","tweet":" 4.2 Performance study In this subsection, we report our experimental results on the performance analysis of FP-growth in comparison with Apriori and TreeProjection (on scalability and processing e ciency). It shows that FP-growth outperforms other previously proposed algorithms and is e cient and scalable in frequent pattern mining in large databases. All the experiments are performed on a 450-MHz Pentium PC machine with 128 megabytes main memory, running on Microsoft Windows/NT. All the programs are written in Microsoft/Visual C++6.0. Notice that we do not directly compare our absolute numberofruntime with those in some published reports running on the RISC workstations because di erent machine architectures may di er greatly on the absolute runtime for the same algorithms. Instead, we implement their algorithms to the best of our knowledge based on the published reports on the same machine and compare in the same running environment. Please also note that run time used here means the total execution time, i.e., the period between input and output, instead of CPU time measured in the experiments in some literature. We feel that run time is a more comprehensive measure since it takes the total running time consumed as the measure of cost, whereas CPU time considers only the cost of the CPU resource. Also, all reports on the runtime of FP-growth include the time of constructing FP-trees from the original databases. "},{"aspect":"expdata","tweet":" 4.2.1 Data sets The synthetic data sets which we used for our experiments were generated using the procedure described in [3]. We refer readers to it for more details on the generation of data sets. We report experimental results on two data sets. The rst one is T25:I10:D10K with 1K items, which is denoted as D1. In this data set, the average transaction size and average maximal potentially frequent itemset size are set to 25 and 10, respectively, while the number of transactions in the dataset is set to 10K. The second data set, denoted as D2, is T25:I20:D100K with 10K items. Some features of the two test data sets are given in Figure 5. There are exponentially numerous frequent itemsets in both data sets, as the support threshold goes down. There are pretty long frequent itemsets as well as a large number of short frequent itemsets in them. They contain abundant mixtures of short and long frequent itemsets. 13 "},{"aspect":"background","tweet":" 1 Introduction Frequent pattern mining [3, 14] plays an essential role in mining associations [21,19, 23, 11, 17, 15, 8, 20, 25,6,18, 26], correlations [7, 22], sequential patterns [4, 24], episodes [16], multi-dimensional patterns [13], max-patterns [5], partial periodicity [10], emerging patterns [9], and many other important data mining tasks. Most of the previous studies adopt an Apriori-like approach, whose essential idea is to iteratively generate the set of candidate patterns of length (k+1) from the set of frequent patterns of lengthk(fork 1), and check their corresponding occurrence frequencies in the database. An important heuristic adopted in these methods, called Apriori heuristic [3], which may greatly reduce the size of candidate pattern set, is the anti-monotonicity property of frequent sets [3, 18]: if any lengthkpattern is not frequent in the database, its length (k +1) super-pattern can never be frequent. The Apriori heuristic achieves good performance gain by (possibly signi cantly) reducing the size of candidate sets. However, in situations with proli c frequent patterns, long patterns, or quite low minimum support thresholds, an Apriori-like algorithm may still su er from the following two nontrivial costs: It is costly to handle a huge number of candidate sets. For example, if there are 10 4 frequent 1-itemsets, the Apriori algorithm will need to generate more than 10 7 length-2 candidates and accumulate and test their occurrence frequencies. Moreover, to discover a frequent pattern of size 100, such asfa1;:::;a100g, it will need 100 1 length-1 candidates, 100 2 length-2 candidates, and so on, and the total number The work was supported in part by the Natural Sciences and Engineering Research Council of Canada (grant NSERC-A3723), the Networks of Centres of Excellence of Canada (grant NCE/IRIS-3), and the Hewlett-Packard Lab, U.S.A. Due to the approximately equally important contributions to this work from all the co-authors, the names of the co-authors are sorted alphabetically. 1 of candidates needed is 100 1 + 100 2 + + 100 100 =2100 , 1 10 30 This is the inherent cost of candidate generation approach, no matter what implementation technique is applied to try to optimize its detailed processing. It is tedious to repeatedly scan the database and check a large set of candidates by pattern matching, which is especially true for mining long patterns. Is there any other way that one may avoid these major costs in frequent pattern mining? Can one construct some novel data structure to reduce such costs? This is the motivation of this study, especially for mining databases containing a mixture of large numbers of long and short patterns. After some careful examinations, we believe that the bottleneck of the Apriori method is at the candidate set generation and test. If one can avoid generating a huge set of candidate patterns, the performance of frequent pattern mining can be substantially improved. This problem is attacked in the following three aspects. First, we design a novel data structure, called frequent pattern tree, orFP-tree for short, which is an extended pre x-tree structure storing crucial, quantitative information about frequent patterns. To ensure that the tree structure is compact and informative, only frequent length-1 items will have nodes in the tree. The tree nodes are arranged in such away that more frequently occurring nodes will have better chances of sharing nodes than less frequently occurring ones. Our experiments show that such a tree is highly compact, usually orders of magnitude smaller than the original database. This o ers an FP-tree-based mining method a much smaller data set to work on. Second, we develop an FP-tree-based pattern fragment growth mining method, which starts from a frequent length-1 pattern (as an initial su x pattern), examines only its conditional pattern base (a \\sub-database\" which consists of the set of frequent items co-occurring with the su x pattern), constructs its (conditional) FP-tree, and performs mining recursively with such a tree. The pattern growth is achieved via concatenation of the su x pattern with the new ones generated from a conditional FP-tree. Since the frequent itemset in any transaction is always encoded in the corresponding path of the frequent pattern trees, pattern growth ensures the completeness of the result. In this context, our method is not Apriori-like restricted generation-and-test but restricted test only. The major operations of mining are count accumulation and pre x path count adjustment, which are usually much less costly than candidate generation and pattern matching operations performed in most Apriori-like algorithms. Third, the search technique employed in mining is a partitioning-based, divide-and-conquer method rather than Apriori-like bottom-up generation of frequent itemsets combinations. This dramatically reduces the size of conditional pattern base generated at the subsequent level of search aswell as the size of its corresponding conditional FP-tree. Moreover, it transforms the problem of nding long frequent patterns to looking for shorter ones and then concatenating the su x. It employs the least frequent items as su x, which o ers good selectivity. All these techniques contribute to the substantial reduction of search costs. To compare our approach with others, we have noticed a recent study by Agarwal et al. [2] which has proposed a novel tree structure technique, called lexicographic tree, and developed a TreeProjection algorithm for mining frequent patterns. Their study reported that the TreeProjection algorithm achieves an order of magnitude performance gain of over Apriori. A comparative analysis is o ered here to compare our approach with theirs. A performance study has also been conducted to compare the performance of FP-growth with Apriori and TreeProjection. Our study shows that FP-growth is at least an order of magnitude faster than Apriori, and such a margin grows even wider when the frequent patterns grow longer, and FP-growth also outperforms the TreeProjection algorithm. Our FP-tree-based mining method has also been tested in large transaction databases in industrial applications. The remaining of the paper is organized as follows. Section 2 introduces the FP-tree structure and its construction method. Section 3 develops an FP-tree-based frequent pattern mining algorithm, FP-growth . Section 4 presents our performance study and an analytical comparison with other frequent pattern methods. Section 2 5 discusses the extensions, implications, and applications of the method. Section 6 summarizes our study and points out some future research issues. "},{"aspect":"expintro","tweet":" In this section, we rst perform a comparative analysis of FP-growth with a recently proposed lexicographical treebased algorithm, TreeProjection [2], and then present a performance comparison of FP-growth with the classical frequent pattern mining algorithm, Apriori, and TreeProjection. "},{"aspect":"problemdef","tweet":" 2 Frequent Pattern Tree: Design and Construction Like most traditional studies in association mining, we de ne the frequent pattern mining problem as follows. De nition 1(Frequent pattern) LetI= fa1;a2;:::;amg be a set of items, and a transaction database DB = hT1;T2;:::;Tni, whereTi (i 2 [1::n]) is a transaction which contains a set of items inI. The support 1 (or occurrence frequency) of a patternA, which is a set of items, is the number of transactions containingAin DB.A, isafrequent pattern ifA's support is no less than a prede ned minimum support threshold, . 2 Given a transaction databaseDB and a minimum support threshold, , the problem of nding the complete set of frequent patterns is called the frequent pattern mining problem. 2.1 Frequent Pattern Tree To design a compact data structure for e cient frequent pattern mining, let's rst examine a tiny example. Example 1 Let the transaction database,DB, be (the rst two columns of) Table 1 and the minimum support threshold be 3. Transaction ID Items Bought (Ordered) Frequent Items 100f;a;c;d;g;i;m;pf;c;a;m;p 200a;b;c;f;l;m;of;c;a;b;m 300b;f;h;j;of;b 400b;c;k;s;pc;b;p 500a;f;c;e;l;p;m;nf;c;a;m;p Table 1: The transaction databaseDB as our running example. A compact data structure can be designed based on the following observations. 1. Since only the frequent items will play a role in the frequent pattern mining, it is necessary to perform one scan ofDB to identify the set of frequent items (with frequency count obtained as a by-product). 2. If we store the set of frequent items (i.e., notice that the ordering is unimportant) of each transaction in some compact structure, it may avoid repeated scanning ofDB. 3. If multiple transactions share an identical frequent item set, they can be merged into one with the number of occurrences registered as count. It is easy to check whether two sets are identical if all the frequent items in di erent transactions are sorted according to a xed order. 4. If two transactions share a common pre x, according to some sorted order of frequent items, the shared parts can be merged using one pre x structure as long as the count is registered properly. If the frequent items are sorted in their frequency descending order, there are better chances that more pre x strings can be shared. With these observations, one may construct a frequent pattern tree as follows. First, a scan ofDB derives a list of frequent items, h(f :4);(c:4);(a:3);(b:3);(m:3);(p:3)i, (the number after \\:\" indicates the support), and with items ordered in frequency descending order. This ordering 1 For convenience of discussion, support is de ne here as absolute occurrence frequency. Notice it is de ned in some literature as the relative one, i.e., the occurrence frequency vs. the total number of transactions in DB. 3 item f c a b m p Header table head of node-links f:4 root c:3 b:1 Figure 1: The FP-tree built based on the data in Table 1 is important since each path of a tree will follow this order. For convenience of later discussions, the frequent items in each transaction are listed in this ordering in the rightmost column of Table 1. Second, one may create a root of a tree, labeled with \\null\". Scan theDB the second time. The scan of the rst transaction leads to the construction of the rst branch of the tree: h(f :1);(c:1);(a:1);(m:1);(p:1)i. Notice that the branch is not ordered in hf;a;c;m;pi as in the transaction but is ordered according to the order in the list of frequent items. For the second transaction, since its (ordered) frequent item list hf;c;a;b;mi shares a common pre x hf;c;ai with the existing path hf;c;a;m;pi, the count of each node along the pre x is incremented by 1, and one new node (b : 1) is created and linked as a child of (a : 2) and another new node (m : 1) is created and linked as the child of (b : 1). For the third transaction, since its frequent item list is hf;bi shares only the node hfi with thef-pre x subtree,f's count is incremented by 1, and a new node (b :1)is created and linked as a child of (f : 3). The scan of the fourth transaction leads to the construction of the second branch of the tree, h(c :1);(b:1);(p:1)i. For the last transaction, since its frequent item list hf;c;a;m;pi is identical to the rst one, the path is shared with the rst one, with the count ofeach node along the path incremented by 1. To facilitate tree traversal, we build an item header table, in which each item points, via a head of node-link, to its rst occurrence in the tree. Nodes with the same item-name are linked in sequence via such node-links. After scanning all the transactions inDB, the tree with the associated node-links is shown in Figure 1. 2 This example leads to the following design and construction of a frequent pattern tree. De nition 2(FP-tree) A frequent pattern tree (or FP-tree in short) is a tree structure de ned below. 1. It consists of one root labeled as \\null\", a set of item pre x subtrees as the children of the root, and a frequent-item header table. 2. Each nodeintheitem pre x subtree consists of three elds: item-name, count, and node-link, where itemname registers which item this node represents, count registers the number of transactions represented by the portion of the path reaching this node, and node-link links to the next node in the FP-tree carrying the same item-name, or null if there is none. 3. Each entry in the frequent-item header table consists of two elds, (1) item-name and (2) head of node-link, which points to the rst node in the FP-tree carrying the item-name. 2 Based on this de nition, we have the following FP-tree construction algorithm. Algorithm 1 (FP-tree construction) Input: A transaction databaseDB and a minimum support threshold . 4 a:3 m:2 p:2 b:1 m:1 c:1 b:1 p:1 Output: Its frequent pattern tree, FP-tree Method: The FP-tree is constructed in the following steps. 1. Scan the transaction databaseDB once. Collect the set of frequent itemsF and their supports. SortF in support descending order asL, the list of frequent items. 2. Create a root of an FP-tree,T , and label it as \\null\". For each transactionTrans inDB do the following. Select and sort the frequent items inTrans according to the order ofL. Let the sorted frequent item list inTrans be [pjP], wherepis the rst element andP is the remaining list. Callinserttree([pjP ];T). The functioninserttree([pjP ];T) is performed as follows. IfT has a childN such that N.item-name = p.item-name, then incrementN's count by 1; else create a new nodeN, and let its count be 1, its parent link be linked toT , and its node-link be linked to the nodes with the same item-name via the node-link structure. IfP is nonempty, callinserttree(P;N) recursively. Analysis. From the FP-tree construction process, we can see that one needs exactly two scans of the transaction database,DB: the rst collects the set of frequent items, and the second constructs the FP-tree. The cost of inserting a transactionTrans into the FP-tree is O(jTransj), where jTransj is the number of frequent items in Trans. We will show that the FP-tree contains the complete information for frequent pattern mining. 2 2.2 Completeness and Compactness of FP-tree There are several important properties of FP-tree which can be observed from the FP-tree construction process. Lemma 2.1 Given a transaction databaseDB and a support threshold , its corresponding FP-tree contains the complete information ofDB in relevance tofrequent pattern mining. Rationale. Based on the FP-tree construction process, each complete set of frequent items in a transactionT in theDB is recorded in one path of the tree, with the item occurrence information registered in the count of each corresponding node. That is, each transaction in theDB is mapped to one path in the FP-tree, and the frequent itemset information in each transaction is completely stored in the FP-tree. Moreover, one path in the FP-tree may represent frequent itemsets in multiple transactions without ambiguity since the path representing every transaction must start from the root of each item pre x subtree. Thus we have the lemma. 2 Based on this lemma, after an FP-tree forDB is constructed, only the FP-tree is needed in the remaining of the mining process, regardless of the number and length of the frequent patterns. Lemma 2.2 Without considering the (null) root, the size of an FP-tree is bounded by the overall occurrences of the frequent items in the database, and the height of the tree isbounded by the maximal number of frequent items in any transaction in the database. Rationale. Based on the FP-tree construction process, for any transactionT inDB, there exists a path in the FP-tree starting from the corresponding item pre x subtree so that the set of nodes in the path is exactly the same set of frequent items inT . Since no frequent item in any transaction can create more than one node in the tree, the root is the only extra node created not by frequent item insertion, and each nodecontains one node-link and one count information, we have the bound of the size of the tree stated in the Lemma. The height ofany p-pre x subtree is the maximum number of frequent items in any transaction withpappearing at the head of its frequent item list. Therefore, the height of the tree is bounded by the maximal number of frequent items in any transaction in the database, if we do not consider the additional level added by the root. 2 Lemma 2.2 shows an important bene t of FP-tree: the size of an FP-tree is bounded by the size of its corresponding database because each transaction will contribute at most one path to the FP-tree, with the length equal to the number of frequent items in that transaction. Since there are often a lot of sharing of frequent items among transactions, the size of the tree is usually much smaller than its original database. Unlike 5 a:4 d:1 e:1 f:1 b:4 d:1 e:1 f:1 c:4 f:3 a:3 b:3 c:3 d:1 e:1 f:1 d:3 e:3 a:1 b:1 c:1 a) FPtree follows the support ordering b) FPtree does not follow the support ordering Figure 2: FP-tree constructed based on frequency descending ordering may not always be minimal. the Apriori-like method which may generate an exponential number of candidates in the worst case, under no circumstances, may anFP-tree with an exponential number of nodes be generated. FP-tree is a highly compact structure which stores the information for frequent pattern mining. Since a single path \\a1 !a2 ! !an\"inthea1-pre x subtree registers all the transactions whose maximal frequent set is in the form of \\a1 !a2 ! !ak\" for any 1kn, the size of the FP-tree is substantially smaller than the size of the database and that of the candidate sets generated in the association rule mining. Can we achieve even better compression of the original database than the FP-tree for frequent pattern mining? Let's have an analysis. For any transactionTi inDB, only the set of frequent itemsFi will be useful for frequent pattern mining. Thus onlyFi need to be recorded in the FP-tree. Since all the frequent items inTi should be preserved as a set in frequent pattern mining, it is necessary to store all the items inFi in one path in the FP-tree. Let the set of frequent items of another transactionTj beFj. IfFi=Fj, they can be stored as one identical path, with count information registered. Thus it is necessary to register the count information since it saves redundant storage for patterns. IfFi andFj share the same pre x, their common pre x should be shared, and the counts associated with the nodes along the pre x path should be accumulated. It is essential to have the set of frequent items of each transaction starting at the root of an item pre x subtree because it avoids ambiguous interpretation of frequent patterns in di erent transactions. For example, a path h(a1 :4)!(a2:3)!(a3:2)iallows only one interpretation: It registers four (maximal) sets of frequent items in four transactions, which area1,a1a2,a1a2a3, anda1a2a3, respectively. Otherwise (i.e., if not starting at the root), there will be quite a few ambiguous interpretations, which will lead to the generation of erroneous frequent patterns. The items in the frequent item set are ordered in the support-descending order: More frequently occurring items are arranged closer to the top of the FP-tree and thus are more likely to be shared. This indicates that FP-tree structure is usually highly compact. However, this does not mean that the tree so constructed achieves maximal compactness all the time. With the knowledge of particular data characteristics, it is possible to achieve better compression. Consider the following example. Let the transactions be: fadef;bdef;cdef;a;a;a;b;b;b;c;c;cg, and the minimum support threshold be 3. The frequent item set associated with support count becomes fa :4;b :4;c :4;d :3;e :3;f :3g. Following the item frequency ordering a !b!c!d!e!f, the FP-tree constructed will contain 12 nodes, as shown in Figure 2 a). However, following another item orderingf!d!e!a!b!c, it will contain only 9 nodes, as shown in Figure 2 b). Our experiments also show that a small FP-trees is resulted by compressing some quite large database. For example, for the database Connect-4 used in MaxMiner [5], which contains 67,557 transactions with 43 items in each transaction, when the support threshold is 50% (which is used in the MaxMiner experiments [5]), the total number of occurrences of frequent items is 2,219,609, whereas the total number of nodes in the FP-tree is 13,449 which represents a reduction ratio of 165.04, while it withholds hundreds of thousands of frequent patterns! (Notice that for databases with mostly short transactions, the reduction ratio is not that high.) Therefore, it is not surprising some gigabyte transaction database containing many long patterns may even generate an 6 FP-tree which ts in main memory. Nevertheless, one cannot assume that an FP-tree can always t in main memory for any large databases. An FP-tree can be partitioned and structured in the form similar to B+-tree. Such a structure will make it highly scalable to very large databases without much performance penalty. This will be discussed in Section 5. "},{"aspect":"solution","tweet":" 3 Mining Frequent Patterns using FP-tree Construction of a compact FP-tree ensures that subsequent mining can be performed in a rather compact data structure. However, this does not automatically guarantee that subsequent mining will be highly e cient since one may still encounter the combinatorial problem of candidate generation if we simply use this FP-tree to generate and check all the candidate patterns. In this section, we will study how to explore the compact information stored in an FP-tree and develop an e cient mining method for frequent pattern mining. Although there are many kinds of frequent patterns that can be mined using FP-tree, this study will focus only on the most popularly studied one [3]: mining all patterns, i.e., the complete set of frequent patterns. Methods for mining other frequent patterns, such asmax-pattern [5], i.e., those not subsumed by other frequent patterns, will be covered by subsequent studies. We rst observe some interesting properties of the FP-tree structure which will facilitate frequent pattern mining. Property 3.1 (Node-link property) For any frequent itemai, all the possible frequent patterns that contain ai can be obtained by followingai's node-links, starting fromai's head in the FP-tree header. This property is based directly on the FP-tree construction process. It facilitates the access of all the pattern information related toai by traversing the FP-tree once followingai's node-links. To facilitate the understanding of other FP-tree properties related to mining, we rst go through an example which performs mining on the constructed FP-tree (Figure 1) in Example 1. Example 2 Let us examine the mining process based on the constructed FP-tree shown in Figure 1. Based on Property 3.1, we collect all the patterns that a nodeai participates by starting fromai's head (in the header table) and followingai's node-links. We examine the mining process by starting from the bottom of the header table. For nodep, it derives a frequent pattern (p : 3) and two paths in the FP-tree : hf :4;c:3;a:3;m:2;p:2i and hc :1;b :1;p :1i. The rst path indicates that string \\(f;c;a;m;p)\" appears twice in the database. Notice although string hf;c;ai appears three times and hfi itself appears even four times, they only appear twice together withp. Thus to study which string appear together withp, onlyp's pre x path hf :2;c:2;a:2;m:2i counts. Similarly, the second path indicates string \\(c;b;p)\" appears once in the set of transactions inDB, or p's pre x path is hc :1;b :1i. These two pre x paths ofp, \\f(f:2;c :2;a :2;m : 2), (c :1;b :1)g\", formp's sub-pattern base, which is calledp's conditional pattern base (i.e., the sub-pattern base under the condition ofp's existence). Construction of an FP-tree on this conditional pattern base (which is calledp's conditional FP-tree) leads to only one branch (c: 3). Hence only one frequent pattern (cp : 3) is derived. (Notice that a pattern is an itemset and is denoted by a string here.) The search for frequent patterns associated withpterminates. For nodem, it derives a frequent pattern (m : 3) and two paths hf :4;c :3;a:3;m:2iand hf :4;c:3;a: 3;b :1;m :1i. Noticepappears together withmas well, however, there is no need to includephere in the analysis since any frequent patterns involvingp has been analyzed in the previous examination ofp. Similar to the above analysis,m's conditional pattern base is, f(f :2;c :2;a : 2), (f :1;c :1;a :1;b :1)g. Constructing an FP-tree on it, we derivem's conditional FP-tree , hf :3;c:3;a:3i, a single frequent pattern path. Then one can call FP-tree-based mining recursively, i.e., callmine(hf :3;c:3;a:3ijm). From Figure 3, one can see that \\mine(hf :3;c :3;a :3ijm)\" involves mining three items (a), (c), (f) in sequence. The rst derives a frequent pattern (am : 3), and a call \\mine(hf :3;c :3ijam)\"; the second derives a frequent pattern (cm : 3), and a call \\mine(hf :3ijcm)\"; and the third derives only a frequent pattern (fm : 3). Further recursive call of \\mine(hf :3;c :3ijam)\" derives (1) a frequent pattern (cam : 3), (2) a recursive call \\mine(hf :3ijam)\" which derives a frequent pattern (fam : 3), and (3) another recursive call 7 f:4 root c:3 b:1 a:3 m:2 p:2 b:1 m:1 Global FP-tree c:1 b:1 p:1 (f:2, c:2, a:2) (f:1, c:1, a:1, b:1) Conditional pattern base of \"m\" item f c a Header table head of node-links Conditional FP-tree of \"m\" f:3 c:3 a:3 root Conditional pattern base of \"am\": (f:3, c:3) Conditional pattern base of \"cam\": (f:3) Conditional FP-tree of \"am\" root f:3 c:3 Conditional pattern base of \"cm\": (f:3) Conditional FP-tree of \"cm\" Figure 3: Example of mining process using FP-tree. root f:3 Conditional FP-tree of \"cam\" item conditional pattern base conditional FP-tree p f(f :2;c:2;a:2;m: 2), (c :1;b:1)g f(c:3)gjp m f(f :4;c:3;a:3;m: 2), (f :4;c:3;a:3;b:1;m:1)g f(f :3;c:3;a:3)gjm b f(f :4;c:3;a:3;b: 1), (f :4;b: 1), (c :1;b:1)g ; a f(f :3;c:3)g f(f:3;c:3)gja c f(f :3)g f(f :3)gjc f ; ; Table 2: Mining of all-patterns by creating conditional (sub)-pattern bases \\mine(hf :3ijcam)\" which derives the longest pattern (fcam : 3). Similarly, the call of \\mine(hf :3ijcm)\", derives one pattern (fcm : 3). Therefore, the whole set of frequent patterns involvingm is f(m : 3), (am : 3), (cm : 3), (fm : 3), (cam : 3), (fam : 3), (fcam : 3), (fcm :3)g. This simply indicates a single path FP-tree can be mined by outputting all the combinations of the items in the path. Similarly,nodebderives (b : 3) and three paths: hf :4;c:3;a:3;b:1i,hf:4;b:1i, and hc :1;b:1i. Sinceb's conditional pattern base: f(f :1;c:1;a: 1), (f : 1), (c :1)ggenerates no frequent item, the mining terminates. Nodeaderives one frequent pattern f(a :3)g, and one subpattern base, f(f :3;c:3)g, a single path conditional FP-tree. Thus, its set of frequent patterns can be generated by taking their combinations. Concatenating them with (a : 3), we have, f(fa : 3), (ca : 3), (fca :3)g.Nodecderives (c : 4) and one subpattern base, f(f :3)g, and the set of frequent patterns associated with (c :3)isf(fc :3)g.Nodefderives only (f : 4) but no conditional pattern base. The conditional pattern bases and the conditional FP-trees generated are summarized in Table 2. 2 The correctness and completeness of the process in Example 2 should be justi ed. We will present a few important properties related to the mining process of Example 2 and provide their justi cation. Property 3.2 (Pre x path property) To calculate the frequent patterns for a nodeai inapathP , only the pre x subpath of nodeai inP need tobeaccumulated, and the frequency count of every node in the pre x path should carry the same count as nodeai. Rationale. Let the nodes along the pathP be labeled asa1;:::;an in such an order thata1 is the root of the pre x subtree,an is the leaf of the subtree inP , andai (1in) is the node being referenced. Based on the process of construction of FP-tree presented in Algorithm 1, for each pre x nodeak (1ki), the pre x subpath of the nodeai inP occurs together withak exactlyai:count times. Thus every such pre x node should carry the same count asnodeai. Notice that a post x nodeam (forimn) along the same path also co-occurs with nodeai. However, the patterns witham will be generated at the examination of the post x node 8 root f:3 am, enclosing them here will lead to redundant generation of the patterns that would have been generated for am. Therefore, we only need to examine the pre x subpath ofai inP . 2 For example, in Example 2, nodemis involved in a path hf :4;c :3;a :3;m :2;p :2i, to calculate the frequent patterns for nodemin this path, only the pre x subpath of nodem, which ishf:4;c:3;a :3i, need to be extracted, and the frequency count ofevery node in the pre x path should carry the same count asnode m. That is, the node counts in the pre x path should be adjusted to hf :2;c:2;a:2i. Based on this property, the pre x subpath of nodeai in a pathP can be copied and transformed into a count-adjusted pre x subpath by adjusting the frequency count ofevery node in the pre x subpath to the same as the count ofnodeai. The so transformed pre x path is called the transformed pre xed path ofai for pathP . Notice that the set of transformed pre x paths ofai form a small database of patterns which co-occur withai. Such a database of patterns occurring withai is calledai's conditional pattern base, and is denoted as \\patternbase jai\". Then one can compute all the frequent patterns associated withai in thisaiconditional pattern base by creating a small FP-tree, calledai's conditional FP-tree and denoted as \\FP-tree j ai\". Subsequent mining can be performed on this small, conditional FP-tree. The processes of construction of conditional pattern bases and conditional FP-trees have been demonstrated in Example 2. This process is performed recursively, and the frequent patterns can be obtained by a pattern growth method, based on the following lemmas and corollary. Lemma 3.1 (Fragment growth) Let be an itemset inDB,B be 's conditional pattern base, and be an itemset inB. Then the support of [ inDB is equivalent to the support of inB. Rationale. According to the de nition of conditional pattern base, each (sub)transaction inB occurs under the condition of the occurrence of in the original transaction databaseDB. Therefore, if an itemset appears inB times, it appears with inDB times as well. Moreover, since all such items are collected in the conditional pattern base of , [ occurs exactly times inDB as well. Thus we have the lemma. 2 From this lemma, we can easily derive an important corollary. Corollary 3.1 (Pattern growth) Let be afrequent itemset inDB,B be 's conditional pattern base, and be an itemset inB. Then [ is frequent inDB if and only if is frequent inB. Rationale. This corollary is the case when is a frequent itemset inDB, and when the support of in 's conditional pattern baseBis no less than , the minimum support threshold. 2 From the processing e ciency point of view, mining is best performed by rst identifying the frequent 1itemset, ,inDB, constructing their conditional pattern bases, and then mining the 1-itemset, , in these conditional pattern bases, and so on. This indicates that the process of mining frequent patterns can be viewed as rst mining frequent 1-itemset and then progressively growing each such itemset by mining its conditional pattern base, which can in turn be done by rst mining its frequent 1-itemset and then progressively growing each such itemset by mining its conditional pattern base, etc. Thus we successfully transform a frequentk-itemset mining problem into a sequence ofkfrequent 1-itemset mining problems via a set of conditional pattern bases. What we need is just pattern growth. There is no need to generate any combinations of candidate sets in the entire mining process. Finally, we provide the property on mining all the patterns when the FP-tree contains only a single path. Lemma 3.2 (Single FP-tree path pattern generation) Suppose an FP-treeT has a single pathP . The complete set of the frequent patterns ofT can be generated by the enumeration of all the combinations of the subpaths ofP with the support being the minimum support of the items contained in the subpath. Rationale. Let the single pathP of the FP-tree be ha1 :s1 !a2 :s2 ! !ak :ski. Since the FP-tree contains a single pathP , the support frequencysi of each itemai (for 1ik) is the frequency ofai co-occurring with its pre x string. Thus any combination of the items in the path, such ashai;;aji (for 9 1i;jk), is a frequent pattern, with their co-occurrence frequency being the minimum support among those items. Since every item in each pathP is unique, there is no redundant pattern to be generated with such a combinational generation. Moreover, no frequent patterns can be generated outside the FP-tree. Therefore, we have the lemma. 2 Based on the above lemmas and properties, we have the following algorithm for mining frequent patterns using FP-tree and a pattern fragment growth approach. Algorithm 2 (FP-growth : Mining frequent patterns with FP-tree and by pattern fragment growth) Input: FP-tree constructed based on Algorithm 1, usingDB and a minimum support threshold . Output: The complete set of frequent patterns. Method: Call FP-growth (FP-tree;null), which is implemented as follows. Procedure FP-growth (Tree; ) f (1) IFTree contains a single pathP (2) THEN FOR EACH combination (denoted as ) of the nodes in the pathP DO (3) generate pattern [ withsupport =minimumsupportofnodesin ; (4) ELSE FOR EACHai in the header ofTree DO f (5) generate pattern =ai [ withsupport =ai:support; (6) Construct 's conditional pattern base and then 's conditional FP-treeTree ; (7) IFTree 6= ; (8) THEN Call FP-growth (Tree; ) g g Analysis. With the properties and lemmas in Sections 2 and 3, we show that the algorithm correctly nds the complete set of frequent itemsets in transaction databaseDB. As shown in Lemma 2.1, FP-tree ofDB contains the complete information ofDB in relevance to frequent pattern mining under the support threshold . If an FP-tree contains a single path, according to Lemma 3.2, its generated patterns are the combinations of the nodes in the path, with the support being the minimum support of the nodes in the subpath. Thus we have lines (1) to (3) of the procedure. Otherwise, we construct conditional pattern base and mine its conditional FP-tree for each frequent itemsetai. The correctness and completeness of pre x path transformation are shown in Property 3.2, and thus the conditional pattern bases store the complete information for frequent pattern mining. According to Lemmas 3.1 and its corollary, the patterns successively grown from the conditional FP-trees are the set of sound and complete frequent patterns. Especially, according to the fragment growth property, the support of the combined fragments takes the support of the frequent itemsets generated in the conditional pattern base. Therefore, we have lines (4) to (8) of the procedure. 2 Let's now examine the e ciency of the algorithm. The FP-growth mining process scans the FP-tree ofDB once and generates a small pattern-baseBai for each frequent itemai, each consisting of the set of transformed pre x paths ofai. Frequent pattern mining is then recursively performed on the small pattern-baseBai by constructing a conditional FP-tree forBai. As reasoned in the analysis of Algorithm 1, an FP-tree is usually much smaller than the size ofDB. Similarly, since the conditional FP-tree, \\FP-tree jai\", is constructed on the pattern-baseBai, it should be usually much smaller and never bigger thanBai. Moreover, a pattern-base Bai is usually much smaller than its original FP-tree, because it consists of the transformed pre x paths related to only one of the frequent items,ai. Thus, each subsequent mining process works on a set of usually much smaller pattern bases and conditional FP-trees . Moreover, the mining operations consists of mainly pre x count adjustment, counting, and pattern fragment concatenation. This is much less costly than generation and test of avery large number of candidate patterns. Thus the algorithm is e cient. From the algorithm and its reasoning, one can see that the FP-growth mining process is a divide-and-conquer process, and the scale of shrinking is usually quite dramatic. If the shrinking factor is around 20 100 for 10 p pc ma mac macf m {cf, cf, cf} Null {acf, acf, acf} b a {cf,cf,cf} maf mc mf mcf {pmacf, pbc, pmacf, mbacf, bf} ac af Figure 4: A lexicographical tree built for the same transactional databaseDB constructing an FP-tree from a database, it is expected to be another hundreds of times reduction for constructing each conditional FP-tree from its already quite small conditional frequent pattern base. Notice that even in the case that a database may generate an exponential number of frequent patterns, the size of the FP-tree is usually quite small and will never grow exponentially. For example, for a frequent pattern of length 100, \\a1;:::;a100\", the FP-tree construction results in only one path of length 100 for it, such as\\ha1;! !a100i\". The FP-growth algorithm will still generate about 10 30 frequent patterns (if time permits!!), such as\\a1,a2,:::,a1a2,:::,a1a2a3,:::,a1:::a100\". However, the FP-tree contains only one frequent pattern path of 100 nodes, and according to Lemma 3.2, there is even no need to construct any conditional FP-tree in order to nd all the patterns. "},{"aspect":"expcomparison","tweet":" 4.1 A comparative analysis of FP-growth and TreeProjection methods The TreeProjection algorithm proposed by Agarwal et al. [2] recently is an interesting algorithm, which constructs a lexicographical tree and projects a large database into a set of reduced, item-based sub-databases based on the frequent patterns mined so far. Since it applies a tree construction method and performs mining recursively on progressively smaller databases, it shares some similarities with FP-growth. However, the two methods have some fundamental di erences in tree construction and mining methodologies, and will lead to notable di erences on e ciency and scalability. We will explain such similarities and di erences by working through the following example. Example 3 For the same transaction database presented in Example 1, we construct the lexicographic tree according to the method described in [2]. The result tree is shown in Figure 4, and the construction process is presented as follows. By scanning the transaction database once, all frequent 1-itemsets are identi ed. As recommended in [2], the frequency ascending order is chosen as the ordering of the items. So, the order isp-m-b-a-c-f, which is exactly the reverse order of what is used in the FP-tree construction. The top level of the lexicographic tree is constructed, i.e. the root and the nodes labeled by length-1 patterns. At this stage, the root node labeled \\null\" and all the nodes which store frequent 1-itemsets are generated. All the transactions in the database are projected to the root node, i.e., all the infrequent items are removed. 11 acf c cf f Each node in the lexicographical tree contains two pieces of information: (i) the pattern that node represents, (ii) the set of items by adding which to the pattern may generate longer patterns. The latter piece information is recorded as active extensions and active items. Then, a matrix at the root node is created, as shown below. The matrix computes the frequencies of length-2 patterns, thus all pairs of frequent items are included in the matrix. The items in pairs are arranged in the ordering. The matrix is built by adding counts from every transaction, i.e., computing frequent 2-itemsets based on transactions stored in the root node. pmbacf p m 2 b 1 1 a 2 3 1 c 3 3 2 3 f 2 3 2 3 3 At the same time of building the matrix, transactions in the root are projected to level-1 nodes as follows. Lett =a1a2an be a transaction with all items listed in ordering.t is projected to nodeai (1in,1) as t 0 ai =ai+1ai+2an. From the matrix, all the frequent 2-itemsets are found as: fpc;ma;mc;mf;ac;af;cfg. The nodes in lexicographic tree for them are generated. At this stage, the only nodes for 1-itemsets which are active are those for m anda, because only they contain enough descendants to potentially generate longer frequent itemsets. All nodes up to and including level-1 except for these two nodes are pruned. In the same way, the lexicographic tree is grown level by level. From the matrix at nodem, nodes labeled mac,maf, andmcf are added, and onlyma is active in all the nodes for frequent 2-itemsets. It is easy to see that the lexicographic tree in total contains 19 nodes. 2 The number of nodes in a lexicographic tree is exactly that of the frequent itemsets. TreeProjection proposes an e cient waytoenumerate frequent patterns. The e ciency of TreeProjection can be explained by twomain factors: (1) the transaction projection limits the support counting in a relatively small space, and only related portions of transactions are considered; and (2) the lexicographical tree facilitates the management and counting of candidates and provides the exibility of picking e cient strategy during the tree generation phase as well as transaction projection phase. [2] reports that their algorithm is up to one order of magnitude faster than other recent techniques in literature. However, in comparison with the FP-growth method, TreeProjection may still su er from some problems related to e ciency, scalability, and implementation complexity. We analyze them as follows. First, TreeProjection may still encounter di culties at computing matrices when the database is huge, when there are a lot of transactions containing many frequent items, and/or when the support threshold is very low. This is because in such cases there often exist a large number of frequent items. The size of the matrices at high level nodes in the lexicographical tree can be huge, as shown in our introduction section. The study in TreeProjection [2] has developed some smart memory caching methods to overcome this problem. However, it could be wise not to generate such huge matrices at all instead of nding some smart caching techniques to reduce the cost. Moreover, even if the matrix can be cached e ciently, its computation still involves some nontrivial overhead. To compute a matrix at nodeP withnprojected transactions, the cost isO( Pn jTij i=1 2 2 ), where jTi j is the length of the transaction. If the number of transaction is large and the length of each transaction is long, the computation is still costly. The FP-growth method will never need to build up matrices and compute 2-itemset frequency since it avoids the generation of any candidatek-itemsets for anykby applying a pattern growth method. Pattern growth can be viewed as successive computation of frequent 1-itemset (of the database and conditional pattern bases) and assembling them into longer patterns. Since computing frequent 1-itemsets is much less expensive than computing frequent 2-itemsets, the cost is substantially reduced. Second, since one transaction may contain many frequent itemsets, one transaction in TreeProjection may be projected many times to many di erent nodes in the lexicographical tree. When there are many long transactions containing numerous frequent items, transaction projection becomes an nontrivial cost of TreeProjection . The 12 FP-growth method constructs FP-tree which is a highly compact form of transaction database. Thus both the size and the cost of computation of conditional pattern bases, which corresponds roughly to the compact form of projected transaction databases, are substantially reduced. Third, TreeProjection creates one node in its lexicographical tree for each frequent itemset. At the rst glance, this seems to be highly compact since FP-tree does not ensure that each frequent node will be mapped to only one node in the tree. However, each branch of the FP-tree may store many \\hidden\" frequent patterns because of the potential generations of many combinations using its pre x paths. Notice that the total number of frequentkitemsets can be very large in a large database or when the database has quite long frequent itemsets. For example, for a frequent itemset (a1;a2;;a100), the number of frequent itemsets at the 50th-level of the lexicographic 100 tree will be = 50 100! 1:0 10 50! 50! 29 . For the same frequent itemset, FP-tree and FP-growth will only need one path of 100 nodes. In summary, FP-growth mines frequent itemsets by (1) constructing highly compact FP-trees which share numerous \\projected\" transactions and hide (or carry) numerous frequent patterns, and (2) applying progressive pattern growth of frequent 1-itemsets which avoids the generation of any potential combinations of candidate itemsets implicitly or explicitly, whereas TreeProjection must generate candidate 2-itemsets for each projected database. Therefore, FP-growth is more e cient and more scalable than TreeProjection, especially when the number of frequent itemsets becomes really large. These observations and analyses are well supported by our experiments reported in this section.  "}]}