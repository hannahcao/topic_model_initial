{"user_name":" Unsupervised Prediction of Citation Influences ","user_timeline":[{"aspect":"abstract","tweet":" Abstract Publication repositories contain an abundance of information about the evolution of scientific research areas. We address the problem of creating a visualization of a research area that describes the flow of topics between papers, quantifies the impact that papers have on each other, and helps to identify key contributions. To this end, we devise a probabilistic topic model that explains the generation of documents; the model incorporates the aspects of topical innovation and topical inheritance via citations. We evaluate the model\u201a€™s ability to predict the strength of influence of citations against manually rated citations. "},{"aspect":"expanalysis","tweet":"  7.3. Duplication of Publications The citation influence model treats the cited and the citing version of the same publication as completely independent from one another. The only coupling factor is the distribution over topic specific words Ï†. We want to know, whether the model assigns similar topic mixes to the duplicated publications, despite this independent treatment. For all duplicated publications, we compare the topic mixtures of both versions via the Jensen Shannon Divergence. The average divergence between duplicated publications is 0.07, which is very low compared to the average divergence between other topic mixtures of 0.69. "},{"aspect":"expdata","tweet":""},{"aspect":"background","tweet":" 1. Introduction When reading on a new topic, researchers need to get a quick overview about a research area. This can be provided by a meaningful visualization of the citation network that spins around some given publications. For example, consider researchers who read on a new topic with some references as starting points. The questions occur which other papers describe key contributions, how the various contributions relate to one another, and how the topic evolved over the past. Clearly, Google Scholar, CiteSeer and other tools that allow to navigate in the publication graph provide a great support and have made this task much easier. Inspired by the work of Garfield (2004), we would like to create a bird\u201a€™s-eye visualization of a research area that complements in-depth navigation in the publication graph. But because the publication graph is linked densely, even a radius of two citations from a pivotal Appearing in Proceedings of the 24 th International Conference on Machine Learning, Corvallis, OR, 2007. Copyright 2007 by the author(s)/owner(s). paper contains hundreds of publications. When examining the citations in detail, one finds that not all cited work has a significant impact on a citing publication. Papers can be cited as background reading, for politeness, fear of receiving an adverse review from an aggrieved reviewer, or as related work that was argued against. For a detailed list of motivations for citing publications see Trigg (1983), Sandor et al. (2006). By contrast, a bird\u201a€™s-eye visualization should show papers that significantly impact one another ( as is depicted in Figure 3). This requires to measure the strength of a citation\u201a€™s influence on the citing work. This paper starts by formalizing the problem setting, followed by a discussion of related work. Section 4 describes baseline algorithms based on Latent Dirichlet Allocation (Blei et al., 2003). In Sections 5 and 6 we introduce generative models that directly model the influence of citations in paper collections. We also derive a Gibbs sampler that learns the model from data. Section 7 empirically evaluates the different models. The paper concludes with a discussion and outlook. "},{"aspect":"expintro","tweet":" Kï¿½ k=1 Cd,c,s(d, c, 0) (k) + Î±Î³ Cd,s(d, 0) (k) + |L(d)| Â· Î±Î³ The experiments are conducted with a subset of the CiteSeer data set 2 , using abstract, title and citation information. In Section 7.1 we exemplarily analyze the citational vicinity of one research paper. In Section 7.2 the prediction performance is evaluated on influence labels provided by domain experts. "},{"aspect":"problemdef","tweet":" 2. Problem Statement A universe of publications is given; publications consist of the full text or abstracts, as well as the citation graph structure. In the citation graph, vertices are publications, and a directed edge from node c to node d indicates that publication d cites publication c. In the following, L(d) denotes the set of publications cited by d. The goal is to find an edge-weight function Î³ measuring the strength of influence. Weights Î³d(c) should correlate to the ground truth impact that c has had on d as strongly as possible. This ground truth is not observable. But for purposes of evaluation, we sample it by referring to the assessment of research scientists. "},{"aspect":"solution","tweet":" 4. Estimating the Influence of Citations with LDA In this section, we derive a baseline model based on LDA. Unsupervised Prediction of Citation Influences Two independence assumptions lead to this model. The first is the Markovian assumption that publications with a strong impact are directly cited. In this case, ancestors to not provide additional information over the directly linked papers. The LDA model is additionally based on the assumption that the topic mix of each paper is chosen independently of the topic mix of the papers that it cites. This leads to the Latent Dirichlet Allocation model depicted in Figure 1a. Latent Dirichlet Allocation associates each token in a document with a latent variable that chooses one of the underlying topics in the corpus. This is achieved by associating each document d with a multinomial distribution Î¸d = p(t|d) over latent topic variables t (also referred to as the topic mixture). Likewise, each topic variable t is associated with a multinomial distribution Ï†t = p(w|t) over words w. In the LDA model, the strength of influence is not an integral part of the model (owing to the second independence assumption), but has to be determined in a later step using a heuristic measure. One heuristic (referred to as\u201a€œLDA-JS\u201a€) defines the strength of influence as the compatibility between the topic mixtures of citing and cited publication. The compatibility of two topic mixtures is measured by the Jensen-Shannon- Divergence. The definition of the weight function is given in Equation 1. DKL(.||.) is the Kullback-Leibler divergence. Î³d(c) = exp(\u201aˆ’DJS(Î¸dï¿½Î¸c)) , c \u201aˆˆ L(d) (1) with DJS(Î¸dï¿½Î¸c) = 1 2 DKL ï¿½ ï¿½ ï¿½ Î¸d ï¿½ ï¿½ Î¸d ï¿½ï¿½ + Î¸c ï¿½ ï¿½ 2 ï¿½ + 1 2 DKL ï¿½ Î¸c ï¿½ ï¿½ ï¿½ ï¿½ Î¸d ï¿½ï¿½ + Î¸c ï¿½ ï¿½ 2 ï¿½ A second heuristic (denoted by \u201a€œLDA-post\u201a€) uses the probability of a citation given the document p(c|d) as strenght of influence (Equation 2). We refer to p(d|t) as p(c|t), if c is a cited document, c \u201aˆˆ L(d). With Bayes\u201a€™ rule and assuming a uniform prior p(c) the posterior of a cited document given a topic can be written as p(c|t) \u201aˆ p(t|c). Î³d(c) =p(c|d) = ï¿½ p(t, c|d) = ï¿½ p(t|d) Â· p(c|t) (2) 5. Copycat Model t The Latent Dirichlet Allocation approach assumes that citations do not influence the underlying topics. In contrast to this, the copycat model approximates a citing document by a \u201a€œweighted sum\u201a€ of documents it cites. The weights of the terms capture the notion of the influence Î³. In order to deal with synonyms and t Unsupervised Prediction of Citation Influences Figure 1. Approaches for estimating the influence of citations in plate notation. The citation influence model (c) combines Latent Dirichlet Allocation (a) and the copycat model (b) via the balancing parameter Î». Figure 2. Association of words to the topical atmosphere (dashed circles) of cited publications. polysems we use latent topic variables t on a word level, just as in Latent Dirichlet Allocation. Each topic in a citing document is drawn from one of the topic mixtures of cited publications. The distribution of draws from cited publications is modeled by a multinomial over citations with parameter Î³. The model is depicted in plate notation in Figure 1b. While learning the parameters of the copycat model, a sampling algorithm associates each word of each citing publication d with a publication it cites. For example let there be a cited publication c which is cited by two publications d1 and d2. The topic mixture Î¸c is not only about all words in the cited publication c but also about some words in d1 and d2, which are associated with c (cf. Figure 2). This way, the topic mixture Î¸c is influenced by the citing publications, which in turn influences the association of words in d1 and d2 to c. All tokens that are associated with a cited publication are called the topical atmosphere of a cited publication. 5.1. Bipartite Citation Graph Following the Markov assumption that only the cited publications influence a citing publication, but not their ancestors, we transform the citation graph into a bipartite graph. The bipartite graph consists of two disjoint node sets D and C, where D contains only nodes with outgoing citation links (the citing publications) and C contains nodes with incoming links (the cited publications). Documents in the original citation graph with incoming and outgoing links are represented as two nodes d \u201aˆˆ D and c \u201aˆˆ C in the bipartite citation graph. 5.2. Mutual Influence of Citing Publications The generative process determines citation influences independently for each citing document, but during inference of the model all citing and all cited papers are considered cojointly. The model accounts for dependencies between cited and citing papers, but also between co-cited papers (papers cited by the same document) and bibliographically coupled papers (papers citing the same document). Furthermore, the topics are learned along with the influences. Because of bidirectional interdependence of links and topics caused by the topical atmosphere, publications originated in one research area (such as Gibbs sampling, which originated in physics) will also be associated with topics they are often cited by (such as machine learning). On the downside, if words in a citing publication do not fit well to the vocabulary introduced by the cited literature, those words will be associated to one of the citations randomly. As an implication, this brings new words and associated topics to the topical atmosphere of the cited publication, that are not reflected by the cited text and should be considered as noise. This will lead to unwanted effects in the prediction of citation influences. In the following chapter we introduce the citation influence model, which addresses this issue. Table 1. Variable description. Variables in the cited plate are denoted with prime. Symbol Description c \u201a€² cited publication d citing publication Î¸ topic mixture of the topical atmosphere of a cited publication Ïˆ innovation topic mixture of a citing publication Ï† characteristic word distribution for each topic Î³ distribution of citation influences Î» parameter of the coin flip, choosing to draw topics from Î¸ or Ïˆ w\u201a€™, w words in cited, citing publications respectively t\u201a€™, t topic assignments of tokens in cited, citing publications respectively c a cited publication, to which a token in a citing publication is associated s indicates whether the topic of a citing publication is drawn from inheritance or innovation Î± Dirichlet / beta parameters of the multinomial / Bernoulli distributions 6. Citation Influence Model If the model enforces each word in a citing publication to be associated with a cited publication, noise effects are introduced to the model which may impede the prediction. In addition, it is not possible to model innovation (i.e., new or evolving topics) in the copycat model. The citation influence model depicted in Figure 1c overcomes these limitations. A citing publication may choose to draw a word\u201a€™s topic from a topic mixture of a citing publication Î¸c (the topical atmosphere) or from it\u201a€™s own topic mixture Ïˆd that models innovative aspects. The choice is modeled by a flip of an unfair coin s. The parameter Î» of the coin is learned by the model, given an asymmetric beta prior ï¿½Î±Î» = (Î±Î»Î¸ , Î±Î»Ïˆ ) which prefers the topic mixture Î¸ of a cited publication. The parameter Î» yields an estimate for how well a publication fits to all its citations. In combination with Î³, the relative influence of citations, Î» Â· Î³ is a measure for the absolute strength of influence. The absolute measure allows to compare links from different citing publications. For the visualization, the citation graph can be thresholded according to the absolute measure. Unsupervised Prediction of Citation Influences 6.1. Generative Process Since the publication graph is given, the length of each document and bibliography is known. The citation influence model assumes the following generative process. In this process the influence of citations is directly modeled (captured by the model parameter Î³). \u201a€¢ for all topics t \u201aˆˆ [1 : T ] do \u201a€¢ draw the word distribution for each latent topic Ï†t = p(w|t) \u201aˆ¼ dirichlet(ï¿½Î±Ï†) \u201a€¢ for all cited documents c \u201a€² \u201aˆˆ C do \u201a€¢ draw a topic mixture Î¸c \u201a€² = p(t\u201a€² |c \u201a€² ) \u201aˆ¼ dirichlet(ï¿½Î±Î¸) \u201a€¢ for all tokens j do \u201a€¢ draw a topic t \u201a€² c \u201a€² ,j mixture \u201aˆ¼ Î¸c \u201a€² from the topic \u201a€¢ draw a word wc \u201a€² ,j \u201aˆ¼ Ï†t \u201a€² c \u201a€² from the topic ,j specific word distribution \u201a€¢ for all citing documents d \u201aˆˆ D do \u201a€¢ draw a citation mixture Î³d = p(c|d)| L(d) \u201aˆ¼ dirichlet(ï¿½Î±Î³) 1 restricted to the publications c cited by this publication d \u201a€¢ draw an innovation topic mixture Ïˆd = p(t|d) \u201aˆ¼ dirichlet(ï¿½Î±Ïˆ) \u201a€¢ draw the proportion between tokens associated with citations and those associated with the innovation topic mixture Î»d = p(s = 0|d) \u201aˆ¼ beta(Î±Î»Î¸ , Î±Î»Ïˆ ) \u201a€¢ for all tokens i do \u201a€¢ toss a coin sd,i \u201aˆ¼ bernoulli(Î»d) \u201a€¢ if sd,i = 0 \u201a€¢ draw a cited document cd,i \u201aˆ¼ multi(Î³d) \u201a€¢ draw a topic td,i \u201aˆ¼ multi(Î¸cd,i ) from the cited document\u201a€™s topic mixture \u201a€¢ else (sd,i = 1) \u201a€¢ draw the topic td,i \u201aˆ¼ multi(Ïˆd) from the innovation topic mixture \u201a€¢ draw a word wd,i \u201aˆ¼ multi(Ï†td,i ) from the topic specific word distribution For a description of each variable see Table 1. 6.2. Learning the Model via Gibbs Sampling Gibbs sampling (Gilks et al., 1996) allows to learn a model by iteratively updating each latent variable given fixed remaining variables. 1 ï¿½Î±Î³ is a symmetric prior of length |L(d)| Unsupervised Prediction of Citation Influences Figure 3. The filtered citation graph contains only edges which represent a significant influence. Table 2. Update equations for the citation influence model. p(c i|ï¿½c Â¬i, d i, s i = 0, t iÂ·) (3) = Cd,c,s(di, ci, 0) + Î±Î³ \u201aˆ’ 1 Cd,s(di, 0) + L(di)Î±Î³ \u201aˆ’ 1 Â· Cc \u201a€² ,t \u201a€² (ci, ti) + Cc,t,s(ci, ti, 0) + Î±Î¸ \u201aˆ’ 1 Cc \u201a€² (ci) + Cc,s(ci, 0) + T Î±Î¸ \u201aˆ’ 1 p(s i = 0|ï¿½s Â¬i, d i, c i, t i, Â·) (4) = C c \u201a€² ,t \u201a€² (c i, t i) + Cc,t,s(c i, t i, 0) + Î± Î¸ \u201aˆ’ 1 C c \u201a€² (c i) + Cc,s(c i, 0) + T Î± Î¸ \u201aˆ’ 1 Cd,s(di, 0) + Î±Î»Î¸ \u201aˆ’ 1 Â· Cd(di) + Î±Î»Î¸ + Î±Î»Ïˆ \u201aˆ’ 1 p(s i = 1|ï¿½s Â¬i, d i, t iÂ·) (5) = C d,t,s(d i, t i, 1) + Î± Ïˆ \u201aˆ’ 1 C d,s(d i, 1) + T Î± Ïˆ \u201aˆ’ 1 Cd,s(di, 1) + Î±Î»Ïˆ \u201aˆ’ 1 Â· Cd(di) + Î±Î»Î¸ + Î±Î»Ïˆ \u201aˆ’ 1 p(t i|ï¿½t Â¬i, w i, s i = 0, c iÂ·) (6) = Cw,t(w i, t i) + C w \u201a€² ,t \u201a€² (w i, t i) + Î± Ï† \u201aˆ’ 1 Ct(t i) + C t \u201a€² (t i) + V Î± Ï† \u201aˆ’ 1 Â· C c \u201a€² ,t \u201a€² (c i, t i) + Cc,t,s(c i, t i, 0) + Î± Î¸ \u201aˆ’ 1 C c \u201a€² (c i) + Cc,s(c i, 0) + T Î± Î¸ \u201aˆ’ 1 p(t i|ï¿½t Â¬i, w i, d i, s i = 1, c iÂ·) (7) = Cw,t(w i, t i) + C w \u201a€² ,t \u201a€² (w i, t i) + Î± Ï† \u201aˆ’ 1 Ct(t i) + C t \u201a€² (t i) + V Î± Ï† \u201aˆ’ 1 Â· C d,t,s(d i, t i, 1) + Î± Ïˆ \u201aˆ’ 1 C d,s(d i, 1) + T Î± Ïˆ \u201aˆ’ 1 The update equations of the citation influence model can be computed in constant time using count caches. The cache counts how often a combination of certain token assignments occurs. For random variables var1, ï¿½ var2, ï¿½ ..., varn, ï¿½ the notation Cvar1,var2,...,varn(val1, val2, ..., valn) = |{\u201aˆ€i : var1,i = val1 \u201aˆ§ var2,i = val2 \u201aˆ§ ... \u201aˆ§ varn,i = valn}| counts occurrences of a configuration val1, val2, ..., valn. For example, Cd,c,s(1, 2, 0) denotes the number of tokens in document 1 that are assigned to citation 2, where the coin result s is 0. The update equations which are used to learn the citation influence model are given in Table 2, see Appendix A for details about the derivation. After the sampling chain converges (i.e., after the burn-in phase), parameters that have been integrated out can be inferred from the count caches by averaging over the sampling chain after convergence. For example, Î³ is derived in Equation 8 with K denoting the length of the sampling chain (burn-in omitted). Î³d(c) = 1 K "},{"aspect":"expcomparison","tweet":" 7.1. Narrative Evaluation In order to explore the behavior of the citation influence model, we analyze the citational vicinity of the LDA paper (Blei et al., 2003) with the citation influence model. The input document collection consists of the paper along with two levels of citations in each direction. The model is trained with hyper parame- ters Î±Ï† = 0.01, Î±Î¸ = Î±Ïˆ = 0.1, Î±Î»Î¸ (8) = 3.0, Î±Î»Ïˆ = 0.1, Î±Î³ = 1.0 and 30 topics. The citation graph is filtered to only contain edges with an influence value Î³d(c) > 0.05. Figure 3 shows an extract of a visualization created by the graphviz tool dot 3 . In contrast to an unfiltered graph, the significantly in- 2 Available at http://citeseer.ist.psu.edu/oai.html 3 Available at http://www.graphviz.org Table 3. Words in the abstract of the research paper \u201a€œLatent Dirichlet Allocation\u201a€ are assigned to citations. The probabilities in parentheses indicate p(w, c|d, Â·). Cited Title Associated Words Î³ Probabilistic Latent Semantic Indexing Modelling heterogeneity with and without the Dirichlet process Introduction to Variational Methods for Graphical Methods text(0.04), latent(0.04), modeling(0.02), model(0.02), indexing(0.01), semantic(0.01), Unsupervised Prediction of Citation Influences document(0.01), collections(0.01) dirichlet(0.02), mixture(0.02), allocation(0.01), context(0.01), variable(0.0135), bayes(0.01), continuous(0.01), improves(0.01), model(0.01), proportions(0.01) variational(0.01), inference(0.01), algorithms(0.01), including(0.01), each(0.01), we(0.01), via(0.01) 0.49 0.25 0.22 fluencing papers can be identified at the first glance. The model correctly identifies the influencing work on Dirichlet processes and variational inference as well as the relatedness to pLSA, PCA, and variational methods. It also yields possible application areas such as querying music and taxonomy learning. The topic proportions are included in the visualization via a topic spectrum bar. Each of the 30 topics is represented by a unique color. Table 3 lists the three most influential cites and the words assigned to them 4 .   7.2. Prediction Performance For an evaluation against a ground truth we asked authors to manually label the strength of influence of papers they cited on a Likert scale 5 . In our experiments we compare four approaches: The 4 The CiteSeer data set only contains four publications cited in \u201a€œLatent Dirichlet Allocation\u201a€. Missing cited publications are invisible to the model. 5 Likert scale semantics used in the survey: xx: \u201a€œthis citation influenced the publication in a strong sense, such as an approach that was build upon and refined\u201a€ x: \u201a€œthis citation influenced the publication, such as very strong related work\u201a€ o: \u201a€œthis citation did not have a strong impact on the publication, such as references basic research, other not especially related approaches, or other application domains\u201a€ oo: \u201a€œthis citation had no impact on the publication at all\u201a€ ?: \u201a€œI can not judge the influence\u201a€; or: \u201a€œI have no opinion about this citation\u201a€ Figure 4. Predictive performance of the models. The error bars indicate the standard error of the AUC values averaged over the citing publications. citation influence model, the copycat model, LDA-JS, and LDA-post. The models are trained on a corpus consisting of the 22 labeled seed publications along with citations (132 abstracts). Experiments are conducted for 10, 15, 30, and 50 topics with the following hyper parameters tuned on a hold-out set. \u201a€¢ Citation influence model: Î±Ï† = 0.01, Î±Î¸ = Î±Ïˆ = 0.1, Î±Î»Î¸ = 3.0, Î±Î»Ïˆ = 0.1, Î±Î³ = 1.0 \u201a€¢ Copycat model: Î±Ï† = 0.01, Î±Î¸ = 0.1, Î±Î³ = 1.0 \u201a€¢ LDA-JS: Î±Ï† = 0.01, Î±Î¸ = 0.1 \u201a€¢ LDA-post: Î±Ï† = 0.01, Î±Î¸ = 0.1 We also evaluated an approach based on the PageRank of cited nodes (with Î³d(c) \u201aˆ P ageRank(c)) and an approach based on the cosine similarity of TF- IDF vectors of citing and cited publications (Î³d(c) \u201aˆ cos(TF IDF(d), TF IDF(c))). For each number of topics, we compare the citation influence and copycat model with the LDA-based approaches in terms of the ability to predict the influence of citations. The prediction performance of the Î³ parameter according to the manual labels of the authors on the Likert scale is evaluated in terms of the AUC (Area Under the ROC Curve) measure. The AUC values for the decision boundaries \u201a€œxx vs. x, o, oo\u201a€, \u201a€œxx, x vs. o, oo\u201a€, and \u201a€œxx, x, o vs. oo\u201a€ are averaged to yield a quality measure for each citing publication. Unlabeled citation links as well as those marked with \u201a€œ?\u201a€ are excluded from the evaluation. The predictive performance of the models is analyzed by averaging AUC values for the test set of 22 manually labeled citing publications. The results are presented in Figure 4. Error bars indicate the standard error. We perform paired-t-tests for models with 15 topics with a significance level Î± = 5%. This reveals that the citation influence model is always significantly better than the LDA-post heuristic (even for 15 topics). In contrast to this, the citation influence model is not significantly better than the copycat model, and the copycat model is not significantly better than the LDA-post heuristic. For 30 and 50 topics, where LDA starts to degenerate, the copycat model shows a significant improvement compared to the LDA-post heuristic. Furthermore, the LDA-JS heuristic is always slightly below the LDA-post heuristic in predicting citation influences. The approaches based on TF-IDF (AUC 0.45) and on PageRank (AUC 0.55) are not able to predict the strength of influence.  "}]}