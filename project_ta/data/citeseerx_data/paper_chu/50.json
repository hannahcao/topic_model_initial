{"user_name":" Algorithms for Association Rule Mining -- A General Survey and Comparison ","user_timeline":[{"aspect":"abstract","tweet":" ABSTRACT  Today there are several ecient algorithms that cope with the popular and computationally expensive task of association rule mining. Actually, these algorithms are more or less described on their own. In this paper we explain the fundamentals of association rule mining and moreover derive a general framework. Based on this we describe today 's approaches in context by pointing out common aspects and dierences. After that we thoroughly investigate their strengths and weaknesses and carry out several runtime experiments. It turns out that the runtime behavior of the algorithms is much more similar as to be expected. "},{"aspect":"expanalysis","tweet":" 5. CONCLUSION  In this paper we dealt with the algorithmic aspects of association rule mining. We restricted ourselves to the \\classic\" association rule problem, that is the generation of all association rules that exist in market basket-like data with respect to minimal thresholds for support and condence. From the broad variety of ecient algorithms that have been developed we compared the most important ones. We systematized the algorithms and analyzed their performance based on both runtime experiments and theoretic considerations. The results were quite surprising: Although we identied fundamental dierences concerning the employed strategies, the algorithms show quite similar runtime behavior in our experiments. At least there is no algorithm that is fundamentally beating out the other ones. In fact our experiments showed that the advantages and disadvantages we identied concerning the strategy to determine the support values of the frequent itemsets nearly balance out on market basket-like data. In a forthcoming paper we pursue the development of a hybrid approach that eciently combines counting occurrences and tidlist intersections [11]. "},{"aspect":"expdata","tweet":""},{"aspect":"background","tweet":" 1. INTRODUCTION 1.1 Association Rules  Since its introduction in 1993 [1] the task of association rule mining has received a great deal of attention. Today the mining of such rules is still one of the most popular patterndiscovery methods in KDD. In brief, an association rule is an expression X ) Y , where  X and Y are sets of items. The meaning of such rules is quite intuitive: Given a database D of transactions { where each transaction T 2 D is a set of items {, X ) Y expresses that whenever a transaction T contains X than T probably contains Y also. The probability or rule condence is dened as the percentage of transactions containing Y in addition to X with regard to the overall number of transactions containing X. That is, the rule condence can be understood as the conditional probability p(Y \u0012 T jX \u0012 T ). The idea of mining association rules originates from the analysis of market-basket data where rules like \\A customer who buys products x1 and x2 will also buy product y with probability c%.\" are found. Their direct applicability to business problems together with their inherent understandability { even for non data mining experts { made association rules a popular mining method. Moreover it became clear that association rules are not restricted to dependency analysis in the context of retail applications, but are successfully applicable to a wide range of business problems. When mining association rules there are mainly two problems to deal with: First of all there is the algorithmic complexity. The number of rules grows exponentially with the number of items. Fortunately today's algorithms are able to eciently prune this immense search space based on minimal thresholds for quality measures on the rules. Second, interesting rules must be picked from the set of generated rules. This might be quite costly because the generated rule sets normally are quite large { e.g. more than 100; 000 rules are not uncommon { and in contrast the percentage of useful rules is typically only a very small fraction. The work concerning the second problem mainly focuses on supporting the user when browsing the rule set, e.g. [14] and the development of further useful quality measures on the rules, e.g. [7; 6; 22].  1.2 Outline of the Paper  In this paper we deal with the algorithmic aspects of association rule mining. In fact, a broad variety of ecient algorithms to mine association rules have been developed during the last years. These approaches are more or less described separately in the corresponding literature. To overcome this situation we give a general survey of the basic ideas behind association rule mining. In Section 2 we identify the basic strategies and describe them in detail. The resulting framework is used in Section 3 to systematize and present today's most common approaches in context. Furthermore we show the common principles and dierences between the algorithms. Finally in Section 4 we complete our overview with a comparison of the algorithms concerning eciency. This comparison is based on theoretic considerations and concrete runtime experiments. In Section 5 we conclude with a short summary of our results.  1.3 Related Work  In our work we mainly restrict ourselves to what we call the \\classic association rule problem\". That is, the mining of all rules existing in a database D with respect to minimal thresholds on certain quality measures. D in this case consists of market-basket like data, that is, transactions containing 10 20 items in the average out of a total set of 1; 000 100; 000 items. Although the \\classic problem\" is still topic of further research, during recent years many algorithms for specialSIGKDD Explorations. Copyright c  2000 ACM SIGKDD, July 2000. Volume 2, Issue 1 - page 58  ized tasks have been developed: First of all, there are the approaches that enhance the association rules itself. E.g. quantitative association rules, e.g. [24], generalized association rules, e.g. [23; 12] and to some extent the work on sequential patterns, e.g. [3; 15]. Moreover there are several generalizations of the rule problem, e.g. [16; 27]. In addition algorithms were developed that mine well de- ned subsets of the rule set according to specied items or quality measures etc, e.g. general constraints [17; 25], optimized rules [8; 20], maximal frequent itemsets [28], and frequent closed itemsets [18; 19]. Moreover there are algorithms to mine dense databases [5]. These approaches are supplemented by algorithms for online mining of association rules, e.g. [10] and incremental algorithms, e.g. [26; 4].  2. BASIC PRINCIPLES <problemdef> 2.1 Formal Problem Description  Let I = fx1 ; : : : ; xng be a set of distinct literals, called items. A set X \u0012 I with k = jXj is called a k-itemset  or simply an itemset. Let a database D be a multi-set of subsets of I. Each T 2 D is called a transaction. We say that a transaction T 2 D supports an itemset X \u0012 I if  X \u0012 T holds. An association rule is an expression X ) Y , where X; Y are itemsets and X \\ Y = ; holds. The fraction of transactions T supporting an itemset X with respect to database D is called the support of X, supp(X) =  jfT 2 D j X \u0012 T gj=jDj. The support of a rule X ) Y is de- ned as supp(X ) Y ) = supp(X[Y ). The condence of this rule is dened as conf(X ) Y ) = supp(X [ Y )=supp(X), c.f. [2]. As mentioned before the main challenge when mining association rules is the immense number of rules that theoretically must be considered. In fact the number of rules grows exponentially with jIj. Since it is neither practical nor desirable to mine such a huge set of rules, the rule sets are typically restricted by minimal thresholds for the quality measures support and condence, minsupp and minconf respectively. This restriction allows us to split the problem into two separate parts [2]: An itemset X is frequent if supp(X) \u0015 minsupp. Once, F = fX \u0012 I j X frequentg, the set of all frequent itemsets together with their support values is known, deriving the desired association rules is straight forward (See [2] for minor enhancements.): For every X 2 F check the condence of all rules X n Y ) Y; Y \u0012 X; ; 6= Y 6= X  and drop those that do not achieve minconf. According to its denition above, it suces to know all support values of the subsets of X to determine the condence of each rule. The knowledge about the support values of all subsets of X  is ensured by the downward closure property of itemset support: All subsets of a frequent itemset must also be frequent, c.f. [2]. With that in mind the task of association rule mining can be reduced to the problem of nding all itemsets that are frequent with respect to a given minimal threshold minsupp.  The rest of this paper and most of the literature on association rule mining addresses exactly this topic. <\/problemdef> 2.2 Traversing the Search Space  As explained we need to nd all itemsets that satisfy minsupp. For practical applications looking at all subsets of I  is doomed to failure by the huge search space. In fact, a linearly growing number of items still implies an exponential growing number of itemsets that need to be considered. For the special case I = f1; 2; 3; 4g we visualize the search space that forms a lattice in Figure 1, c.f. [28]. The frequent {2} {3} {1, 4} {2, 3} {2, 4} {3, 4} {1, 3} {1, 2} {1} {1, 3, 4} {2, 3, 4} {1, 2, 4} {1, 2, 3} {1, 2, 3, 4} {} {4} Figure 1: Lattice for I = f1; 2; 3; 4g itemsets are located in the upper part of the gure whereas the infrequent ones are located in the lower part. Although we do not explicitly specify support values for each of the itemsets, we assume that the bold border separates the frequent from the infrequent itemsets. The existence of such a border is independent of any particular database D and  minsupp. Its existence is solely guaranteed by the downward closure property of itemset support. The basic principle of the common algorithms is to employ this border to eciently prune the search space. As soon as the border is found, we are able to restrict ourselves on determining the support values of the itemsets above the border and to ignore the itemsets below. Let map: I ! f1; : : : ; jIjg be a mapping that maps all items x 2 I one-to-one onto natural numbers. Now the items can be seen as totally ordered by the relation \\ natural numbers. In addition, for X \u0012 I let X:item :  f1; : : : ; jXjg ! I : n 7! X:itemn be a mapping with X:itemn  denoting the n-th item of the items x 2 X increasingly sorted by \\ n-prex of an itemset X with n \u0014 jXj  is then dened by P = fX:itemm j 1 \u0014 m \u0014 ng, c.f. [12]. Let the classes E(P ); P \u0012 I with E(P ) = fX \u0012 I j jXj =  jP j + 1 and P is a prex of Xg be the nodes of a tree. Two nodes are connected by an edge, if all itemsets of a class E  can be generated by joining two itemsets of the parent class  E  0  , e.g. Figure 2. Together with the downward closure property of itemset support this implies the following: If the parent class E  0  of a class E does not contain at least two frequent itemsets than E must also not contain any frequent itemset. If we encounter such a class E  0  on our way down the tree, then we have reached the border separating the infrequent from the frequent itemsets. We do not need to go behind this border so we prune E and all descendants of E from the search space. The latter procedure allows us to eciently restrict the number of itemsets to investigate. We simply determine the SIGKDD Explorations. Copyright c  2000 ACM SIGKDD, July 2000. Volume 2, Issue 1 - page 59  {2} {3} {1, 4} {2, 3} {2, 4} {3, 4} {1, 3} {1, 2} {1} {1, 3, 4} {2, 3, 4} {1, 2, 4} {1, 2, 3} {1, 2, 3, 4} {4} Figure 2: Tree for I = f1; 2; 3; 4g support values only of those itemsets that we \\visit\" on our search for the border between frequent and infrequent itemsets. Finally, the actual strategy to search for the border is at our own choice. Today's common approaches employ both breadth-rst search (BFS) or depth-rst search (DFS). With BFS the support values of all (k 1)-itemsets are determined before counting the support values of the  k-itemsets. In contrast, DFS recursively descends following the tree structure dened above.  2.3 Determine Itemset Supports  In the following an itemset that is potentially frequent and for which we decide to determine its support during lattice traversal is called a candidate itemset or simply a candidate. One common approach to determine the support value of an itemset is to directly count its occurrences in the database. For that purpose a counter is set up and initialized to zero for each itemset that is currently under investigation. Then all transactions are scanned and whenever one of the candidates is recognized as a subset of a transaction, its counter is incremented. Typically subset generation and candidate lookup is integrated and implemented on a hashtree or a similar data structure. In brief, not all subsets of each transaction are generated but only those that are contained in the candidates or that have a prex in common with at least one of the candidates, c.f. [2] for further details. Another approach is to determine the support values of candidates by set intersections. A tid is a unique transaction identier. For a single item the tidlist is the set of identiers that correspond to the transactions containing this item. Accordingly tidlists also exist for every itemset X and are denoted by X:tidlist. The tidlist of a candidate C = X[Y is obtained by C:tidlist = X:tidlist  T  Y:tidlist. The tidlists are sorted in ascending order to allow ecient intersections. Note that by buering the tidlists of frequent candidates as intermediate results, we remarkably speedup the generation of the tidlists of the following candidates. Finally the actual support of a candidate is obtained by determining jC:tlistj. "},{"aspect":"expintro","tweet":" 4.1 Experiments  To carry out performance studies we implemented the most common algorithms to mine frequent itemsets, namely Apriori, DIC, Partition, and Eclat, in C++. Actually we had to leave out DIC in the charts for reasons explained later. In addition we did not consider AprioriTID and FP-growth because these algorithms were designed to mine data that is not typical for retail environments, that is data containing quite long patterns. The experiments were performed on a SUN UltraSPARC-II workstation clocked at 248Mhz. The experiments in Figures 4 - 11 were carried out on synthetic datasets from [2; 21]. These datasets were generated with a data generator [2] that simulates the buying behavior of customers in retail business. Dataset \\T10.I4.D100K\" means an average transaction size of 10, an average size of the maximal potentially frequent itemsets of 4 and 100,000 generated transactions. The number of patterns was set to 2; 000 and the number of items to 1; 000. In addition to the experiments from [2; 21], we restricted the maximal length of generated itemsets from 1 up to 9 on the dataset \\T20.I4.D100K\" at minsupp = 0:33%, c.f. Figure 9. Figures 10 and 11 show the behavior of the algorithms on real-world applications. The basket data consists of about 70; 000 customer transactions with approximately  60; 000 dierent items. The average transaction size is \u0019 10:5 items. The car equipment data contains information about 700; 000 cars with about 6; 000 items. In the average \u0019 20 items are assigned to each car. It is important to say that all algorithms scale linearly with the database size. "},{"aspect":"problemdef","tweet":" 2.1 Formal Problem Description  Let I = fx1 ; : : : ; xng be a set of distinct literals, called items. A set X \u0012 I with k = jXj is called a k-itemset  or simply an itemset. Let a database D be a multi-set of subsets of I. Each T 2 D is called a transaction. We say that a transaction T 2 D supports an itemset X \u0012 I if  X \u0012 T holds. An association rule is an expression X ) Y , where X; Y are itemsets and X \\ Y = ; holds. The fraction of transactions T supporting an itemset X with respect to database D is called the support of X, supp(X) =  jfT 2 D j X \u0012 T gj=jDj. The support of a rule X ) Y is de- ned as supp(X ) Y ) = supp(X[Y ). The condence of this rule is dened as conf(X ) Y ) = supp(X [ Y )=supp(X), c.f. [2]. As mentioned before the main challenge when mining association rules is the immense number of rules that theoretically must be considered. In fact the number of rules grows exponentially with jIj. Since it is neither practical nor desirable to mine such a huge set of rules, the rule sets are typically restricted by minimal thresholds for the quality measures support and condence, minsupp and minconf respectively. This restriction allows us to split the problem into two separate parts [2]: An itemset X is frequent if supp(X) \u0015 minsupp. Once, F = fX \u0012 I j X frequentg, the set of all frequent itemsets together with their support values is known, deriving the desired association rules is straight forward (See [2] for minor enhancements.): For every X 2 F check the condence of all rules X n Y ) Y; Y \u0012 X; ; 6= Y 6= X  and drop those that do not achieve minconf. According to its denition above, it suces to know all support values of the subsets of X to determine the condence of each rule. The knowledge about the support values of all subsets of X  is ensured by the downward closure property of itemset support: All subsets of a frequent itemset must also be frequent, c.f. [2]. With that in mind the task of association rule mining can be reduced to the problem of nding all itemsets that are frequent with respect to a given minimal threshold minsupp.  The rest of this paper and most of the literature on association rule mining addresses exactly this topic. "},{"aspect":"solution","tweet":" 3. COMMON ALGORITHMS  In this section we brie y describe and systematize the most common algorithms. We do this by referring to the fundamentals of frequent itemset generation that we identied in the previous section. Our goal is not to go to much into detail but to show the basic principles and the dierences between the approaches.  3.1 Systematization  The algorithms that we consider in this paper are systematized in Figure 3. We characterize each of the algorithms a) by its strategy to traverse the search space and b) by its strategy to determine the support values of the itemsets. In DFS BFS Counting  Intersecting Intersecting  Counting  Apriori Eclat AprioriTID DIC FP- Partition growth Figure 3: Systematization of the Algorithms addition an algorithm may employ specic optimizations for further speedup.  3.2 BFS and Counting Occurrences  The most popular algorithm of this type is Apriori [2] where also the downward closure property of itemset support was introduced. Apriori makes additional use of this property by pruning those candidates that have an infrequent subset before counting their supports. This optimization becomes possible because BFS ensures that the support values of all subsets of a candidate are known in advance. Apriori counts all candidates of a cardinality k together in one scan over the database. The critical part is looking up the candidates in each of the transactions. For this purpose [2] introduces a so called hashtree structure. The items in each transaction are used to descend in the hashtree. Whenever we reach one of its leafs, we nd a set of candidates having a common prex that is contained in the transaction. Then these candidates are searched in the transaction that has been encoded as a bitmap before. In the case of success the counter of the candidate in the tree is incremented.  AprioriTID [2] is an extension of the basic Apriori approach. Instead of relying on the raw database AprioriTID internally represents each transaction by the current candidates it contains. With AprioriHybrid both approaches are combined, c.f. [2]. To some extent also SETM [13] is an Apriori(TID)-like algorithm which is intended to be implemented directly in SQL.  DIC is a further variation of the Apriori-Algorithm [7]. DIC softens the strict separation between counting and generating candidates. Whenever a candidate reaches minsupp, that is even when this candidate has not yet \\seen\" all transactions, DIC starts generating additional candidates based SIGKDD Explorations. Copyright c  2000 ACM SIGKDD, July 2000. Volume 2, Issue 1 - page 60  on it. For that purpose a prex-tree is employed. In contrast to the hashtree, each node { leaf node or inner node { of the prex-tree is assigned to exactly one candidate respectively frequent itemset. In contrast to the usage of a hashtree that means whenever we reach a node we can be sure that the itemset associated with this node is contained in the transaction. Furthermore interlocking support determination and candidate generation decreases the number of database scans.  3.3 BFS and TID-List Intersections  The Partition-Algorithm [21] is an Apriori-like algorithm that uses set intersections to determine support values. As described above Apriori determines the support values of all (k 1)-candidates before counting the k-candidates. The problem is that Partition of course wants to use the tidlists of the frequent (k 1)-itemsets to generate the tidlists of the k-candidates. Obviously the size of those intermediate results easily grows beyond the physical memory limitations of common machines. To overcome this Partition splits the database into several chunks that are treated independently. The size of each chunk is chosen in such a way that all intermediate tidlists t into main memory. After determining the frequent itemsets for each database chunk, an extra scan is necessary to ensure that the locally frequent itemsets are also globally frequent.  3.4 DFS and Counting Occurrences  Counting occurrences assumes candidate sets of a reasonable size. For each of those candidate sets a database scan is performed. E.g. Apriori that relies on BFS scans the database once for every candidate size k. When using DFS the candidate sets consist only of the itemsets of one of the nodes of the tree from Section 2.2. Obviously scanning the database for every node results in tremendous overhead. The simple combination of DFS with counting occurrences is therefore of no practical relevance, c.f.[11]. Recently in [9] a fundamentally new approach called FPgrowth  was introduced. In a preprocessing step FP-growth derives a highly condensed representation of the transaction data, the so called FP-tree. The generation of the FP-tree is done by counting occurrences and DFS. In contrast to former DFS-approaches, FP-growth does not follow the nodes of the tree from Subsection 2.2, but directly descends to  some part of the itemsets in the search space. In a second step FP-growth uses the FP-tree to derive the support values of all frequent itemsets.  3.5 DFS and TID-List Intersections  In [28] the algorithm Eclat is introduced, that combines DFS with tidlist intersections. When using DFS it suces to keep the tidlists on the path from the root down to the class currently investigated in memory. That is, splitting the database as done by Partition is no longer needed. Eclat employs an optimization called \\fast intersections\". Whenever we intersect two tidlists then we are only interested in the resulting tidlist if its cardinality reaches minsupp. In other words, we should break o each intersection as soon as it is sure that it will not achieve this threshold. Eclat originally generates only frequent itemsets of size \u0015  3. We modied Eclat to mine also the frequent 1- and 2-itemsets by calling it on the class that contains the 1itemsets together with their tidlists. In addition in [28] algorithms that mine only the maximal frequent itemsets are introduced, e.g. MaxEclat. An itemset  X is maximal frequent if for every frequent itemset Y X \u0012 Y ) Y = X holds. We do not consider these algorithms because although it is straight forward to derive the set of all frequent itemsets from the maximal frequent itemsets this does not hold for the corresponding support values. Without those, we are not able to derive rule condences and therefore we cannot generate association rules. "},{"aspect":"expcomparison","tweet":" 4.2 Counting Occurrences vs. Intersecting Sets  The basic question concerning the runtime of the algorithms is whether counting occurrences or intersecting tidlists shows better performance results. The advantage of counting is that only candidates that actually occur in the transactions cause any eort. In contrast, an intersection means at least passing through all tids of the smaller of the two tidlists, even if the candidate is not contained in the database at all. (\\Fast Intersections\" save some costs but we still need to pass a substantial number of tids.) But intersections also have their benets. Counting implies looking up the candidates in the transactions. Of course this can get quite expensive for candidates of higher cardinality. On the contrary when using intersections the size of the candidate under investigation does not have any in uence. In practice both eects seem to balance out on the basketSIGKDD Explorations. Copyright c  2000 ACM SIGKDD, July 2000. Volume 2, Issue 1 - page 61  0 5 10 15 20 25 30 35 40 45 2 1.51.5 1 0.750.75 0.50.5 0.330.33 0.25  minsupp in % apriori eclat partition Figure 4: T10.I2.D100K 0 5 10 15 20 25 30 35 40 45 50 2 1.51.5 1 0.750.75 0.50.5 0.330.33 0.25  minsupp in % apriori eclat partition Figure 5: T10.I4.D100K 20 40 60 80 100 120 140 160 180 2 1.51.5 1 0.750.75 0.50.5 0.330.33 0.25  minsupp in % apriori eclat partition Figure 6: T20.I2.D100K 0 20 40 60 80 100 120 140 160 2 1.51.5 1 0.750.75 0.50.5 0.330.33 0.25  minsupp in % apriori eclat partition Figure 7: T20.I4.D100K  0 50 100 150 200 250 2 1.51.5 1 0.750.75 0.50.5 0.330.33 0.25  minsupp in % apriori eclat partition Figure 8: T20.I6.D100K 0 20 40 60 80 100 120 140 1 2 3 4 55 6 7 88 9  maxsize apriori eclat partition Figure 9: Maximal Frequent Itemset Size 0 50 100 150 200 250 300 2 11 0.50.5 0.250.25 0.120.12 0.06  minsupp in % apriori eclat partition Figure 10: Basket Data 0 50 100 150 200 250 300 350 400 450 500 15 1212 9 77 55 44 3  minsupp in % apriori eclat partition Figure 11: Car Equipment SIGKDD Explorations. Copyright c  2000 ACM SIGKDD, July 2000. Volume 2, Issue 1 - page 62  like data. The runtime behavior in Figures 4 - 8 does not show any substantial dierences between the algorithm Apriori that counts occurrences and the tidlists intersecting algorithms Partition and Eclat. Only at quite low average size of the maximal potentially frequent itemsets, e.g. \\T10.I2.D100K\" in Figure 4, Apriori is somehow superior whereas at larger average size of the maximal potentially frequent itemsets, e.g. \\T20.I6.D100K\" in Figure 8, Partition and Eclat perform better. The same explanation holds for the real-world experiments. With an average size of \u0019 2:2 items at minsupp = 0:06% the frequent itemsets found in the basket data were rather short compared to the frequent itemsets from the car equipment database, that contained  \u0019 4:1 items in the average at minsupp = 3%. In Figure 9 it becomes clear what happens behind the scenes: Eclat and Partition spend most of their time with determining the support values of the 2- and 3-candidates whereas Apriori is eciently handling such small itemsets. In contrast for itemsets with size \u0015 4 the additional eort caused for Eclat and Partition is to be neglected whereas this does not hold for Apriori. "}]}