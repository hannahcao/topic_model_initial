{"user_name":" ObjectRank: Authority-Based Keyword Search in Databases \u201aˆ— ","user_timeline":[{"aspect":"abstract","tweet":" Abstract The ObjectRank system applies authority-based ranking to keyword search in databases modeled as labeled graphs. Conceptually, authority originates at the nodes (objects) containing the keywords and flows to objects according to their semantic connections. Each node is ranked according to its authority with respect to the particular keywords. One can adjust the weight of global importance, the weight of each keyword of the query, the importance of a result actually containing the keywords versus being referenced by nodes containing them, and the volume of authority flow via each type of semantic connection. Novel performance challenges and opportunities are addressed. First, schemas impose constraints on the graph, which are exploited for performance purposes. Second, in order to address the issue of authority ranking with respect to the given keywords (as opposed to Google\u201a€™s global PageRank) we precompute single keyword ObjectRanks and combine them during run time. We conducted user surveys and a set of performance experiments on multiple real and synthetic datasets, to assess the semantic meaningfulness and performance of ObjectRank. "},{"aspect":"expanalysis","tweet":" 9 Conclusion and Future Work We presented an adjustable framework to answer keyword queries using the authority transfer paradigm, which we believe is applicable to a significant number of domains (though obviously not meaningful for every database). We showed that our framework is efficient and semantically meaningful, with an experimental evaluation and user surveys respectively. We investigated how this framework can be applied with small modifications to applications other than biblio- Sales 0.3 Customer submitted 1 Complaint about Product Figure 14: Authority transfer schema graph for Complaints database. graphic, where the authority transfer intuition is applicable. For example, consider a complaints database (Figure 14), which stores the complaint reports of customers regarding products of the company. Assume we wish to rank the complaint reports according to their urgency, given that the goal of the company is to keep the \u201a€œgood\u201a€ customers satisfied, and the \u201a€œgoodness\u201a€ of a customer is the total sales associated with him/her. Then, the base set for the computation of the global ObjectRank is the set of customers, and each customer is given a base ObjectRank proportional to his/her total sales amount. A reasonable assignment of authority transfer rates is shown in Figure 14. "},{"aspect":"expdata","tweet":" In this section we experimentally evaluate the system and show that calculating the ObjectRank is feasible, both in the preprocessing and in the query execution stage. For the evaluation we use two real and a set of synthetic datasets: COMSOC is the dataset of the publications of 8 Google\u201a€™s current ranking algorithm is not disclosed. 572 the IEEE Communications Society 9 , which consists of 55, 000 nodes and 165, 000 edges. DBLPreal is a subset of the DBLP dataset, consisting of the publications in twelve database conferences. This dataset contains 13, 700 nodes and 101, 500 edges. However, these datasets are too small to evaluate the index creation algorithms. Hence, we also created a set of artificial datasets shown in Table 2, using the words of the DBLP dataset. The outgoing edges are distributed uniformly among papers, that is, each paper cites on average 10 other papers. The incoming edges are assigned by a non-uniform random function, similar to the one used in the TPC-C benchmark 10 , such that the top-10% of the most cited papers receive 70% of all the citations. name #nodes #edges DBLP30 3,000 30,000 DBLP100 10,000 100,000 DBLP300 30,000 300,000 DBLP1000 100,000 1,000,000 DBLP3000 300,000 3,000,000 Table 2: Synthetic Datasets. To store the databases in a RDBMS, we decomposed them into relations according to the relational schema shown in Figure 11. Y is an instance of a conference in a particular year. PP is a relation that describes each paper pid2 cited by a paper pid1, while PA lists the authors aid of each paper pid. Notice that the two arrows from P to PP denote primary-to-foreign-key connections from pid to pid1 and from pid to pid2. We ran our experiments using the Oracle 9i RDBMS on a Xeon 2.2-GHz PC with 1 GB of RAM. We implemented the preprocessing and queryprocessing algorithms in Java, and connect to the RDBMS through JDBC. C(cid,name) Y(yid,year,cid) P(pid,title,yid) A(aid,name) PP(pid1,pid2) PA(pid,aid) Figure 11: Relational schema. "},{"aspect":"background","tweet":" 1 Introduction Vagelis Hristidis School of Computer Science Florida International University Miami, FL 33199 vagelis@cs.fiu.edu PageRank [9] is an excellent tool to rank the global importance of the pages of the Web, proven by the success of Google 1 . However, Google uses PageRank as a tool \u201aˆ— Work supported by NSF IDM 9734548 and NSF ITR 313384. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. Proceedings of the 30th VLDB Conference, Toronto, Canada, 2004 1http://www.Google.com 564 has Conference instance Name=\u201a€œICDE\u201a€ Yannis Papakonstantinou Computer Science UC, San Diego La Jolla, CA 92093 yannis@cs.ucsd.edu Paper Authors=\u201a€œH. Gupta , V. Harinarayan, A. Rajaraman, J. Ullman\u201a€ Title=\u201a€œIndex Selection for OLAP.\u201a€ Year=\u201a€œICDE 1997\u201a€ contains Year Name=\u201a€œICDE\u201a€, Year=1997, Location=Birmingham Paper Authors=\u201a€œC. Ho, R. Agrawal, N. Megiddo, R. Srikant\u201a€ Title=\u201a€œRange Queries in OLAP Data Cubes.\u201a€ Year=\u201a€œSIGMOD 1997\u201a€ by cites cites cites by Author Name=\u201a€œR. Agrawal\u201a€ Paper Authors=\u201a€œJ. Gray, A. Bosworth, A. Layman, H. Pirahesh\u201a€ Title=\u201a€œData Cube: A Relational Aggregation Operator Generalizing Group-By, Cross-Tab, and Sub- Total.\u201a€ Year= \u201a€œICDE 1996\u201a€ contains cites Paper Authors=\u201a€œR. Agrawal, A. Gupta, S. Sarawagi\u201a€ Title=\u201a€œModeling Multidimensional Databases.\u201a€ Year=\u201a€œICDE 1997\u201a€ Figure 1: A subset of the DBLP graph to measure the global importance of the pages, independently of a keyword query. (Google also uses other IR techniques to estimate the relevance of a page to a keyword query, which is then combined with the PageRank value to calculate the final score of a page.) More recent works [19, 27] apply PageRank to estimate the relevance of pages to a keyword query. We appropriately extend and modify PageRank to perform keyword search in databases for which there is a natural flow of authority between their objects (e.g.: bibliographic or complaints databases as we explain below). Consider the example of Figure 1, which illustrates a small subset of the DBLP database in the form of a labeled graph (author, conference and year nodes except for \u201a€œR. Agrawal\u201a€, \u201a€œICDE\u201a€ and \u201a€œICDE 1997\u201a€ respectively are omitted to simplify the figure). Schema graphs, such as the one of Figure 3, describe the structure of database graphs. Given a keyword query, e.g. the single keyword query \u201a€œOLAP\u201a€, ObjectRank sorts the database objects by their importance with respect to the user-provided keywords. Figure 2 illustrates the top-10 \u201a€œOLAP\u201a€ papers in the DBLP subset (produced by our online demo at http://www.db.ucsd.edu/ObjectRank) 2 currently used by the ObjectRank prototype. Notice that many entries (the \u201a€œData Cube\u201a€ and the \u201a€œModeling Multidimen- 2 The DBLP subset currently used by ObjectRank was copied from our XKeyword [21] database (demo available at http://www.db.ucsd.edu/XKeyword) and consists of the available publications in 12 major database conferences, including SIGMOD, VLDB, PODS, ICDE, ICDT and EDBT, up to year 2001. sional Databases\u201a€ papers in Figure 1) of the top-10 list do not contain the keyword \u201a€œOLAP\u201a€ (\u201a€œOLAP\u201a€ is not even contained in their abstracts) but they clearly constitute important papers in the OLAP area, since they may be referenced by other papers of the OLAP area or may have been written by authors who have written other important \u201a€œOLAP\u201a€ papers. Conceptually, the ranking is produced in the following way: Myriads of random surfers are initially found at the objects containing the keyword \u201a€œOLAP\u201a€ and then they traverse the database graph. In particular, at any time step a random surfer is found at a node and either (i) makes a move to an adjacent node by traversing an edge, or (ii) jumps randomly to an \u201a€œOLAP\u201a€ node without following any of the links. The probability that a particular traversal happens depends on multiple factors, including the type of the edge (in contrast to the Web link-based search systems [9, 19, 27]). These factors are depicted in an authority transfer schema graph. Figure 4 illustrates the authority transfer schema graph that corresponds to the setting that produced the results of Figure 2. Assuming that the probability that the surfer moves back to an \u201a€œOLAP\u201a€ node is 15% (damping factor [9]), the collective probability to move to a referenced paper is up to 85% Ã— 70% (70% is the authority transfer rate of the citation edge as we explain below), the collective probability to move to an author of the paper is up to 85% Ã— 20%, the probability to move from the paper to the forum where the paper appeared is up to 85%Ã—10%, and so on. As is the case with the PageRank algorithm as well, as time goes on, the expected percentage of surfers at each node v converges (Section 2) to a limit r(v). Intuitively, this limit is the ObjectRank of the node. An alternative way to conceive the intuition behind ObjectRank is to consider that authority/importance flows in the database graph in the same fashion that [25] defined authority-based search in arbitrary graphs. Initially the \u201a€œOLAP\u201a€ authority is found at the objects that contain the keyword \u201a€œOLAP\u201a€. Then authority/importance flows, following the rules in the authority transfer schema graph, until an equilibrium is established that specifies that a paper is authoritative if it is referenced by authoritative papers, is written by authority authors and appears in authority conferences. Vice versa, authors and conferences obtain their authority from their papers. Notice that the amount of authority flow from, say, paper to cited paper or from paper to author or from author to paper, is arbitrarily set by a domain expert and reflects the semantics of the domain. For example, common sense says that in the bibliography domain a paper obtains very little authority (or even none) by referring to authoritative papers. On the contrary it obtains a lot of authority by being referred by authoritative papers. Our DBLP demo offers to the user more than one authority flow settings, in order to accommodate multiple user profiles/requirements. We believe the ability to customize authority flow schemes is central to ObjectRank, since we should not assume that \u201a€œone size fits all\u201a€ when it comes to opinions about authority flow. For example, there is one 565 setting for users that primarily care for papers with high global importance and another for users that primarily care for papers that are directly or indirectly heavily referenced by papers that have the keywords. We expect that multiple settings make sense in all non-trivial ObjectRank applications. Keyword search in databases has some unique characteristics, which make the straightforward application of the random walk model as described in previous work [9, 19, 27] inadequate. First, every database has different semantics, which we can use to improve the quality of the keyword search. In particular, unlike the Web, where all edges are hyperlinks 3 , the database schema exhibits the types of edges, and the attributes of the nodes. Using the schema we specify the ways in which authority flows across the nodes of the database graph. For example, the results of Figure 2 were obtained by annotating the schema graph of Figure 3 with the authority flow information that appears in Figure 4. Furthermore, previous work [9, 19, 27] assumes that, when calculating the global importance (in our framework we make a clear distinction between the global importance of a node and its relevance to a keyword query), the random surfer has the same probability to start from any page p of the base set (we call this probability base ObjectRank of p). However, this is not true for every database. For example, consider a product complaints database (Figure 14). In this case, we represent the business value of a customer by assigning to his/her node a base ObjectRank proportional to his/her total sales amount. Another novel property of ObjectRank is adjustability, which allows for the tuning of the system according to the domain- and/or user-specific requirements. For example, for a bibliographic database, a new graduate student desires a search system that returns the best reading list around the specified keywords, whereas a senior researcher looks for papers closely related to the keywords, even if they are not of a high quality. These preference scenarios are made possible by adjusting the weight of the global importance versus the relevance to the keyword query. Changing the damping factor d offers another calibration opportunity. In particular, larger values of d favor nodes pointed by high-authority nodes, while smaller values of d favor nodes containing the actual keywords (that is, nodes in the base set). The handling of queries with multiple keywords offers more flexibility to the system as we describe in Section 3. For example, we may want to assign a higher weight to the relevance of a node to an infrequent keyword. On the performance level, calculating the ObjectRank values in runtime is a computationally intensive operation, especially given the fact that multiple users query the system. This is resolved by precomputing an inverted index where for each keyword we have a sorted list of the nodes 3 Previous works [27, 10, 8] assign weights on the edges of the data graph according to the relevance of the incident nodes\u201a€™ text to the keywords. In contrast, we assign authority transfer rates on the schema graph, which captures the semantics of the database, since the relevance factor is reflected in the selection of the base set. 41.34 Data Cube: A Relational Aggregation Operator Generalizing Group-By, Cross-Tab, and Sub-Total. Jim Gray, ICDE 1996 36.62 Index Selection for OLAP. Himanshu Gupta, ICDE 1997 35.11 Range Queries in OLAP Data Cubes. Ching-Tien Ho, SIGMOD 1997 31.03 Discovery-Driven Exploration of OLAP Data Cubes. Sunita Sarawagi, EDBT 1998 30.7 OLAP and Statistical Databases: Similarities and Differences. Arie Shoshani, PODS 1997 30.23 Implementing Data Cubes Efficiently. Venky Harinarayan, SIGMOD 1996 29.42 Relative Prefix Sums: An Efficient Approach for Querying Dynamic OLAP Data Cubes. Steven Geffner, ICDE 1999 28.49 Modeling Multidimensional Databases. Rakesh Agrawal, ICDE 1997 26.96 Summarizability in OLAP and Statistical Data Bases. Hans-J. Lenz, SSDBM 1997 26.75 Data Warehousing and OLAP for Decision Support (Tutorial). Surajit Chaudhuri, SIGMOD 1997 Figure 2: Top 10 papers on \u201a€œOLAP\u201a€ returned by ObjectRank m:n cites Conference Year Paper Author 1:n 1:n m:n Figure 3: The DBLP schema graph. with non-trivial ObjectRank for this keyword. During runtime we employ the Threshold Algorithm [14] to efficiently combine the lists. However, our approach induces the cost of precomputing and storing the inverted index. Regarding the space requirements, notice that the number of keywords of a database is typically limited, and less than the number of users in a personalized search system [23]. Furthermore, we do not store nodes with ObjectRank below a threshold value (chosen by the system administrator), which offers a space versus precision tradeoff. In Section 7 we show that the index size is small relative to the database size for two bibliographic databases. Regarding the index computation, we present and experimentally evaluate two classes of optimizations. First, we exploit the structural properties of the database graph. For example, if we know that the objects of a subgraph of the schema form a Directed Acyclic Graph (DAG), then given a topological sort of the DAG, there is an efficient straightforward one-pass ObjectRank evaluation. We extend the DAG case by providing an algorithm that exploits the efficient evaluation for DAGs in the case where a graph is \u201a€œalmost\u201a€ a DAG in the sense that it contains a large DAG subgraph. In particular, given a graph G with n nodes, which is reduced to a DAG by removing a small subset of m nodes, we present an algorithm which reduces the authority calculation into a system of m equations - as opposed to the usual system of n equations. Furthermore, we present optimization techniques when the data graph has a small vertex cover, or if it can be split into a set of subgraphs and the connections between these subgraphs form a DAG. Second, notice that the naive approach would be to calculate each keyword-specific ObjectRank separately. We have found that it is substantially more efficient to first calculate the global ObjectRank, and use these scores as initial values for the keyword-specific computations. This accelerates convergence, since in general, objects with high global ObjectRank, also have high keyword-specific ObjectRanks. Furthermore, we show how storing a prefix of the inverted lists allows the faster calculation of the ObjectRanks of all nodes. The semantic and performance contributions of this paper are evaluated using two user surveys 566 0.3 0.3 0.2 Conference Year Paper Author 0.3 0.1 0.7 cites 0 cited Figure 4: The DBLP authority transfer schema graph. and a detailed experimental evaluation respectively. We have implemented a web interface, available at http://www.db.ucsd.edu/ObjectRank, to query a subset of the DBLP database using the ObjectRank technique. The essential formal background on PageRank and authority search is presented in Section 2. Section 3 presents the semantics of ObjectRank and Section 4 describes the system\u201a€™s architecture. The algorithms used to calculate ObjectRank are presented in Section 5 and are experimentally evaluated in Section 7. We present the results of two user surveys in Section 6. Furthermore, related work is discussed in Section 8. Finally, conclusions and future work are discussed in Section 9. 2 Background We describe next the essentials of PageRank and authoritybased search, and the random surfer intuition. Let (V,E) be a graph, with a set of nodes V = {v1,...,vn} and a set of edges E. A surfer starts from a random node (web page) vi of V and at each step, he/she follows a hyperlink with probability d or gets bored and jumps to a random node with probability 1 \u201aˆ’ d. The PageRank value of vi is the probability r(vi) that at a given point in time, the surfer is at vi. If we denote by r the vector [r(v1),...,r (vi),...,r(vn)] T then we have (1 \u201aˆ’ d) r = dAr + e (1) |V | 1 where A is a nÃ—n matrix with Aij = OutDeg(vj ) if there is an edge vj \u201a†’ vi in E and 0 otherwise, where OutDeg(vj) is the outgoing degree of node vj. Also, e =[1,...,1] T . The above PageRank equation is typically precomputed before the queries arrive and provides a global, keywordindependent ranking of the pages. Instead of using the whole set of nodes V as the base set, i.e., the set of nodes where the surfer jumps when bored, one can use an arbitrary subset S of nodes, hence increasing the authority associated with the nodes of S and the ones most closely associated with them. In particular, we define a base vector s =[s0,...,si,...,sn] T where si is1ifvi \u201aˆˆ S and 0 otherwise. The PageRank equation is then (1 \u201aˆ’ d) r = dAr + s (2) |S| Regardless of whether one uses Equation 1 or Equation 2 the PageRank algorithm solves this fixpoint using a simple iterative method, where the values of the (k+1)-th execution are calculated as follows: r (k+1) = dAr (k) (1 \u201aˆ’ d) + s (3) |S| 0.2 Parameter property Parameters Application - specific Combination of scores authority transfer rates, global ObjectRank calculation, damping factor normalization scheme, global ObjectRank weight, AND or OR semantics Performance epsilon, threshold Table 1: Parameters of ObjectRank The algorithm terminates when r converges, which is guaranteed to happen under very common conditions [26]. In particular, A needs to be irreducible (i.e., (V,E) be strongly connected) and aperiodic. The former is true due to the damping factor d, while the latter happens in practice. The notion of the base set S was suggested in [9] as a way to do personalized rankings, by setting S to be the set of bookmarks of a user. In [19] it was used to perform topic-specific PageRank on the Web. We take it one step further and use the base set to estimate the relevance of a node to a keyword query. In particular, the base set consists of the nodes that contain the keyword as explained next. "},{"aspect":"expintro","tweet":" The experiments are divided into two classes. First, we measure how fast the ObjectRank Execution module (Figure 6) calculates the ObjectRanks for all keywords and stores them into the ObjectRank Index, using the CreateIndex algorithm of Figure 7. The size of the ObjectRank Index is also measured. This experiment is repeated for various values of epsilon and threshold, and various dataset sizes. Furthermore, the General ObjectRank algorithm is compared to the almost-DAG algorithm, and the effect of using various initial ObjectRank values is evaluated. Second, in [6] 11 the Query module (Figure 6) is evaluated. In 9 http://www.comsoc.org 10 http://www.tpc.org/tpcc/ 11 We moved these experiments to [6] because the Query stage is less particular, we measure the execution times of the combining algorithm (Section 4) to produce the top-k results, for various values of k, various numbers of keywords m, and OR and AND semantics. "},{"aspect":"problemdef","tweet":""},{"aspect":"solution","tweet":" 3 ObjectRank Semantics In this section we formally define the framework of this work, and show how ObjectRank ranks the nodes of a database with respect to a given keyword query, given a set of calibrating (adjusting) parameters (Table 1). In particular, Section 3.1 describes how the database and the authority transfer graph are modeled. Section 3.2 shows how the keyword-specific and the global ObjectRanks are calculated and combined to produce the final score of a node. Finally, Section 3.3 presents and addresses the challenges for multiple-keyword queries. 3.1 Database Graph, Schema, and Authority Transfer Graph We view a database as a labeled graph, which is a model that easily captures both relational and XML databases. The data graph D(VD,ED) is a labeled directed graph where every node v has a label Î»(v) and a set of keywords. For example, the node \u201a€œICDE 1997\u201a€ of Figure 1 has label \u201a€œYear\u201a€ and the set of keywords {\u201a€˜\u201a€˜ICDE\u201a€™\u201a€™, \u201a€˜\u201a€˜1997\u201a€™\u201a€™, \u201a€˜\u201a€˜Birmingham\u201a€™\u201a€™}. Each node represents an object of the database and may have a sub-structure. Without loss of generality, ObjectRank assumes that each node has a tuple of attribute name/attribute value pairs. For example, the \u201a€œYear\u201a€ nodes of Figure 1 have name, year and location attributes. Notice that the keywords appearing in the attribute values comprise the set of keywords associated with the node. One may assume richer semantics by including the metadata of a node in the set of keywords. For example, the metadata \u201a€œForum\u201a€, \u201a€œYear\u201a€, \u201a€œLocation\u201a€ could be included in the keywords of a node. The specifics of modeling the data of a node are orthogonal to ObjectRank and will be neglected in the rest of the discussion. 567 Each edge e from u to v is labeled with its role Î»(e) (we overload Î») and represents a relationship between u and v. For example, every \u201a€œpaper\u201a€ to \u201a€œpaper\u201a€ edge of Figure 1 has the label \u201a€œcites\u201a€. When the role is evident and uniquely defined from the labels of u and v, we omit the edge label. For simplicity we will assume that there are no parallel edges and we will often denote an edge e from u to v as \u201a€œu \u201a†’ v\u201a€. The schema graph G(VG,EG) (Figure 3) is a directed graph that describes the structure of D. Every node has an associated label. Each edge is labeled with a role, which may be omitted, as discussed above for data graph edge labels. We say that a data graph D(VD,ED) conforms to a schema graph G(VG,EG) if there is a unique assignment Âµ such that: 1. for every node v \u201aˆˆ VD there is a node Âµ(v) \u201aˆˆ VG such that Î»(v) =Î»(Âµ(v)); 2. for every edge e \u201aˆˆ ED from node u to node v there is an edge Âµ(e) \u201aˆˆ EG that goes from Âµ(u) to Âµ(v) and Î»(e) =Î»(Âµ(e)). Authority Transfer Schema Graph. From the schema graph G(VG,EG), we create the authority transfer schema graph G A (VG,E A ) to reflect the authority flow through the edges of the graph. This may be either a trial and error process, until we are satisfied with the quality of the results, or a domain expert\u201a€™s task. In particular, for each edge eG =(u\u201a†’ v) of EG, twoauthority transfer edges, e f G =(u\u201a†’ v) and ebG =(v \u201a†’ u) are created. The two edges carry the label of the schema graph edge and, in addition, each one is annotated with a (potentially different) authority transfer rate - Î±(e f G ) and Î±(ebG ) correspondingly. We say that a data graph conforms to an authority transfer schema graph if it conforms to the corresponding schema graph. (Notice that the authority transfer schema graph has all the information of the original schema graph.) Figure 4 shows the authority transfer schema graph that corresponds to the schema graph of Figure 3 (the edge labels are omitted). The motivation for defining two edges for each edge of the schema graph is that authority potentially flows in both directions and not only in the direction that appears in the schema. For example, a paper passes its authority to its authors and vice versa. Notice however, that the authority flow in each direction (defined by the authority transfer rate) may not be the same. For example, a paper that is cited by important papers is clearly important but citing important papers does not make a paper important. Notice that the sum of authority transfer rates of the outgoing edges of a schema node u may be less than 1 4 ,ifthe administrator believes that the edges starting from u do not transfer much authority. For example, in Figure 4, conferences only transfer 30% of their authority. Authority Transfer Data Graph. Given a data graph D(VD,ED) that conforms to an authority transfer schema graph G A (VG,E A ), ObjectRank derives an authority 4 In terms of the random walk model, this would be equivalent to the disappearance of a surfer. Conference Name=\u201a€œICDE\u201a€ Paper Authors=\u201a€œH. Gupta, V. Harinarayan, A. Rajaraman, J. Ullman\u201a€ Title=\u201a€œIndex Selection for OLAP.\u201a€ Year=\u201a€œICDE 1997\u201a€ 0.3 0.3 0.1 0.15 Year Name=\u201a€œICDE\u201a€, Year=1997, Location=Birmingham Paper Authors=\u201a€œC. Ho, R. Agrawal, N. Megiddo, R. Srikant\u201a€ Title=\u201a€œRange Queries in OLAP Data Cubes.\u201a€ Year=\u201a€œSIGMOD 1997\u201a€ 0.7 0.35 0.1 0.35 0.05 0.066 0.1 0.1 Author Name=\u201a€œR. Agrawal\u201a€ Paper Authors=\u201a€œJ. Gray, A. Bosworth, A. Layman, H. Pirahesh\u201a€ Title=\u201a€œData Cube: A Relational Aggregation Operator Generalizing Group-By, Cross-Tab, and Sub- Total.\u201a€ Year= \u201a€œICDE 1996\u201a€ 0.15 0.7 Paper Authors=\u201a€œR. Agrawal, A. Gupta, S. Sarawagi\u201a€ Title=\u201a€œModeling Multidimensional Databases.\u201a€ Year=\u201a€œICDE 1997\u201a€ Figure 5: Authority transfer data graph transfer data graph DA (VD,EA D ) (Figure 5) as follows. For every edge e =(u\u201a†’ v) \u201aˆˆ ED the authority transfer data graph has two edges ef =(u \u201a†’ v) and eb =(v \u201a†’ u). The edges ef and eb are annotated with authority transfer rates Î±(ef ) and Î±(eb ). Assuming that ef is of type e f G , then Î±(e f ï¿½ Î±(e )= f G ) OutDeg(u,e f ef ),ifOutDeg(u, G ) > 0 G 0, ifOutDeg(u, e f G )=0 (4) where OutDeg(u, e f G ) is the number of outgoing edges from u, of type e f G . The authority transfer rate Î±(eb ) is defined similarly. Figure 5 illustrates the authority transfer data graph that corresponds to the data graph of Figure 1 and the authority schema transfer graph of Figure 4. Notice that the sum of authority transfer rates of the outgoing edges of a node u of type Âµ(u) may be less than the sum of authority transfer rates of the outgoing edges of Âµ(u) in the authority transfer schema graph, if u does not have all types of outgoing edges. 3.2 Importance vs. Relevance. The score of a node v with respect to a keyword query w is a combination of the global ObjectRank r G (v) of v and the keyword-specific ObjectRank rw (v). We propose the following combining function, although other functions may be used as well: r w,G (v) =r w (v) Â· (r G (v)) g (5) where g is the global ObjectRank weight, which determines how important the global ObjectRank is. Notice that g may be accessible to the users or fixed by the administrator. The calculations of the keyword-specific and the global ObjectRank are performed as follows (we assume singlekeyword queries at this point). Keyword-specific ObjectRank. Given a single keyword query w, ObjectRank finds the keyword base set S(w) (from now on referred to simply as base set when the keyword is implied) of objects that contain the keyword w and assigns an ObjectRank rw (vi) to every node vi \u201aˆˆ VD by resolving the equation r w = dAr w (1 \u201aˆ’ d) + s (6) |S(w)| where Aij = Î±(e) if there is an edge e =(vj \u201a†’ vi) in EA D and 0 otherwise, d controls the base set importance, 568 and s =[s1,...,sn] T is the base set vector for S(w), i.e., si =1if vi \u201aˆˆ S(w) and si =0otherwise. The damping factor d determines the portion of ObjectRank that an object transfers to its neighbors as opposed to keeping to itself. It was first introduced in the original PageRank paper [9], where it was used to ensure convergence in the case of PageRank sinks. However, in addition to that, in our work it is a calibrating factor, since by decreasing d, we favor objects that actually contain the keywords (i.e., are in base set)as opposed to objects that acquire ObjectRank through the incoming edges. The value for d used by PageRank [9] is 0.85, which we also adopt when we want to balance the importance of containing the actual keywords as opposed to being pointed by nodes containing the keywords. Global ObjectRank. The definition of global ObjectRank is different for different applications or even users of the same application. In this work, we focus on cases where the global ObjectRank is calculated applying the random surfer model, and including all nodes in the base set. The same calibrating parameters are available, as in the keywordspecific ObjectRank. Notice that this way of calculating the global ObjectRank, which is similar to the PageRank approach [9], assumes that all nodes (pages in PageRank) initially have the same value. However, there are many applications where this is not true, as we discuss in Section 9. 3.3 Multiple-Keywords Queries. We define the semantics of a multiple-keyword query \u201a€œw1,...,wm\u201a€ by naturally extending the random walk model. We consider m independent random surfers, where the ith surfer starts from the keyword base set S(wi). For AND semantics, the ObjectRank of an object v with respect to the m-keywords query is the probability that, at a given point in time, the m random surfers are simultaneously at v. Hence the ObjectRank r w1,...,wm AND (v) of the node v with respect to the m keywords is r w1,...,wm AND (v) = ï¿½ r wi (v) (7) i=1,...,m where r wi (v) is the ObjectRank with respect to the keyword wi. For OR semantics, the ObjectRank of v is the probability that, at a given point in time, at least one of the m random surfers will reach v. Hence, for two keywords w 1 and w2 it is r w1,w2 OR (v) =rw1 (v)+r w2 (v) \u201aˆ’ r w1 (v)r w2 (v) (8) and for more than two, it is defined accordingly. Notice that [19] also sums the topic-sensitive PageRanks to calculate the PageRank of a page. In [6] we discuss how we handle the difference in the distribution of ObjectRank values of frequent and infrequent keywords. We also explain why the HITS [25] approach, where the base set consists of all objects with at least one of the keywords, can be viewed as a special case of ObjectRank\u201a€™s semantics. 4 Architecture Figure 6: System Architecture. Figure 6 shows the architecture of the ObjectRank system, which is divided into two stages. The preprocessing stage consists of the ObjectRank Execution module, which inputs the database to be indexed, the set of all keywords that will be indexed, and a set of parameters (the rest of the adjusting parameters are input during the query stage). In particular these parameters are: (i) the damping factor d, (ii) the authority transfer rates Î±(eG)\u201a€™s of the authority transfer schema graph G A , (iii) the convergence constant epsilon which determines when the ObjectRank algorithm converges, and (iv) the threshold value which determines the minimum ObjectRank that an object must have to be stored in the ObjectRank Index. The ObjectRank Execution module creates the ObjectRank Index, which is an inverted index, indexed by the keywords. For each keyword w, it stores a list of \u201aŒ©id(u),r w (u)\u201aŒª pairs for each object u that has r w (u) \u201a‰\u2022 threshold. The pairs are sorted by descending r w (u) to facilitate an efficient querying method as we describe below. The ObjectRank Index has been implemented as an index-based table, where the lists are stored in a CLOB attribute. A hash-index is built on top of each list to allow for random access, which is required by the Query module. The Query module inputs a set of sorted \u201aŒ©id(u),r w (u)\u201aŒª pairs lists L1,...,Lm and a set of adjusting parameters, and outputs the top-k objects according to the combining function (Equation 7 or 8). In particular, these parameters are: (i) the semantics to be used (AND or OR), (ii) the normalization scheme, i.e., the exponents to use, and (iii) the global ObjectRank weight. The naive approach would be to make one pass of all lists to calculate the final ObjectRank values for each object and then sort this list by final ObjectRank. Instead, we use the Threshold Algorithm [14] which is guaranteed to read the minimum prefix of each list. Notice that the Threshold Algorithm is applicable since both combining functions (Equations 7 and 8) are monotone, and random access is possible on the stored lists. 569 Finally, the Database Access module inputs the result ids and queries the database to get the suitable information to present the objects to the user. This information is stored into an id-indexed table, that contains a CLOB attribute value for each object id. For example, a paper object CLOB would contain the paper title, the authors\u201a€™ names, and the conference name and year. 5 ObjectRank Index creation This section presents algorithms to create the ObjectRank index. Section 5.1 presents an algorithm for the case of arbitrary authority transfer data graphs D A . Sections 5.2 and 5.3 show how we can do better when D A is a directed acyclic graph (DAG) and \u201a€œalmost\u201a€ a DAG respectively (the latter property is explained in Section 5.3). Section 5.4 presents optimization opportunities based on manipulating the initial values of the iterative algorithm. Finally, in [6], we present optimizations when the authority transfer graph has a small vertex cover, or is a DAG of subgraphs. 5.1 General algorithm Figure 7 shows the algorithm that creates the ObjectRank Index. The algorithm accesses the authority transfer data graph D A many times, which may lead to a too long execution time if D A is very large. Notice that this is usually not a problem, since D A only stores object ids and a set of edges which is small enough to fit into main memory for most databases. Notice that lines 2-4 correspond to the original PageRank calculation [9] modulo the authority transfer rates information. CreateIndex(keywordsList, epsilon, threshold, Î±(.), d){ 01. For each keyword w in keywordsList do { 02. While not converged do 03. /*i.e., \u201aˆƒv, |r (k+1) (v) \u201aˆ’ r (k) (v)| >epsilon*/ 04. MakeOnePass(w,Î±(.), d); 05. StoreObjectRanks(); 06. } } MakeOnePass(w,Î±(.), d) { 07. Evaluate Equation 6 using the r from the previous iteration on the right side; } StoreObjectRanks() { 08. Sort the \u201aŒ©id(i),r(vi)\u201aŒª pairs list by r(vi) and store it in inverted index, after removing pairs with r(vi)  threshold; } Figure 7: Algorithm to create ObjectRank Index 5.2 DAG algorithm There are many applications where the authority transfer data graph is a DAG. For example a database of papers and their citations (ignoring author and conference objects), where each paper only cites previously published papers, is a DAG. Figure 8 shows an improved algorithm, which makes a single pass of the graph D A and computes the actual ObjectRank values. Notice that there is no need for epsilon any more since we derive the precise solution of Equation 6, in contrast to the algorithm of Figure 7 which calculates approximate values. The intuition is that ObjectRank is only transferred in the direction of the topological ordering, so a single pass suffices. Notice that topologically sorting a graph G(V,E) takes time Î˜(V + E) [12] in the general case. In many cases the semantics of the database can lead to a better algorithm. For example, in the papers database, we can efficiently topologically sort the papers by first sorting the conferences by date. This method is applicable for databases where a temporal or other kind of ordering is implied by the link structure. CreateIndexDAG(keywordsList, threshold, Î±(.), d){ 01. Topologically sort nodes in graph D A ; 02. /*Consecutive accesses to D \u201a€²A are in topological order.*/ 03. For each keyword w in keywordsList do { 04. MakeOnePass(w,Î±(.), d); 05. StoreObjectRanks(); 06. } } Figure 8: Algorithm to create ObjectRank Index for DAGs In the above example, the DAG property was implied by the semantics. However, in some cases we can infer this property by the structure of the authority transfer schema graph G A , as the following theorem shows. Theorem 5.1 The authority transfer data graph D A is a DAG if and only if \u201a€¢ the authority transfer schema graph G A is a DAG, or \u201a€¢ for every cycle c in G A , the subgraph D \u201a€²A of D A consisting of the nodes (and the edges connecting them), whose type is one of the schema nodes of c, is a DAG. 5.3 Almost-DAG algorithm The most practically interesting case is when the authority transfer data graph D A is almost a DAG, that is, there is a \u201a€œsmall\u201a€ set U of backedges, and if these edges are removed, D A becomes a DAG. Notice that the set U is not unique, that is, there can be many minimal (i.e., no edge can be removed from U) sets of backedges. Instead of working with the set of backedges U, we work with the set L of backnodes, that is, nodes from which the backedges start. This reduces the number of needed variables as we show below, since |L| \u201a‰¤|U|. In the papers database example (when author and conference objects are ignored), L is the set of papers citing a paper that was not published previously. Similarly, in the complaints database (Figure 14), most complaints reference previous complaints. Identifying the minimum set of backnodes is NP-complete 5 in the general case. However, the semantics of the database can lead to efficient algorithms. For example, for the databases we discuss in 5 Proven by reducing Vertex Cover to it. 570 this paper (i.e, the papers and the complaints databases), a backnode is simply an object referencing an object with a newer timestamp. The intuition of the algorithm (Figure 9) is as follows: the ObjectRank of each node can be split to the DAG-ObjectRank which is calculated ignoring the backedges, and the backedges-ObjectRank which is due to the backedges. To calculate backedges-ObjectRank we assign a variable ci to each backnode ci (for brevity, we use the same symbol to denote a backnode and its ObjectRank), denoting its ObjectRank. Before doing any keyword-specific calculation, we calculate how ci\u201a€™s are propagated to the rest of the graph D A (line 5), and store this information in C. Hence Cij is the coefficient with which to multiply cj when calculating the ObjectRank of node vi. To calculate C (lines 13-15) we assume that the backedges are the only source of ObjectRank, and make one pass of the DAG in topological order. CreateIndexAlmostDAG(keywordsList, threshold, Î±(.), d){ 01. c: vector of ObjectRanks of backnodes; 02. Identify backnodes, and topologically sort the DAG (D A without the backedges) D \u201a€²A ; 03. /*Consecutive accesses to D \u201a€²A are in topological order.*/ 04. /*Backedges are considered in D \u201a€²A for Î±(.) .*/ 05. C=BuildCoefficientsTable(); 06. For each keyword w in keywordsList do { 07. Calculate ObjectRanks vector r \u201a€² for D \u201a€²A executing MakeOnePass(w,Î±(.), d); 08. Solve c = C Â· c + r \u201a€² ; 09. /*D denotes keeping only the lines of D corresponding to backnodes.*/ 10. r = C Â· c + r \u201a€² 11. StoreObjectRanks(); 12. } } BuildCoefficientsTable(){ 13. For each node vj do 14. r(vj) =d Â· ï¿½ backnode c i points at v j (Î±(ci \u201a†’ vj) Â· ci)+ d Â· ï¿½ non\u201aˆ’backnode v l points at v j (Î±(vl \u201a†’ vj) Â· r(vl)); 15. Return C, such that r = C Â· c } Figure 9: Algorithm to create ObjectRank Index for almost DAGs Then, for each keyword-specific base set: (a) we calculate the DAG-ObjectRanks r \u201a€² (line 7) ignoring the backedges (but taking them into account when calculating the outgoing degrees), (b) calculate c i\u201a€™s solving a linear system (line 8), and (c) calculate the total ObjectRanks (line 10) by adding the backedge-ObjectRank (C Â· c) and the DAG-ObjectRank(r \u201a€² ). Each line of the system of line 8 corresponds to a backnode ci \u201a‰¡ vj (i.e., the ith backnode is the jth node of the topologically sorted authority transfer data graph D \u201a€²A ), whose ObjectRank ci is the sum of the backedge-ObjectRank (Cj Â· c) and the DAG-ObjectRank (r \u201a€² j ). The overline notation on the matrices of this equation selects the L lines from each table that correspond to the backnodes. We further explain the algorithm using an example. (a) (b) P 5 P 4 P 3 P 2 P 1 c 1 c 1 c2 P 5 P 4 P 3 P 2 P 1 c 2 Figure 10: Almost DAG. Example 1 The graph D A is shown in Figure 10 (a). Assume d =0.5 and all edges are of the same type t with authority transfer rate Î±(t) =1. First we topologically sort the graph and identify the backnodes c 1 \u201a‰¡ P5,c2 \u201a‰¡ P4. Then we create the coefficients table C (line 5), as follows: r(P1) = 0 r(P2) = 0.5 Â· 0.5 Â· c2 =0.25 Â· c2 r(P3) = 0.5 Â· c1 r(P4) = 0.5 Â· r(P2)+0.5 Â· 0.5 Â· r(P3) = 0.125 Â· c1 +0.125 Â· c2 r(P5) = 0.5 Â· 0.5 Â· r(P3)+0.5 Â· 0.5 Â· r(P4) = 0.156 Â· c1 +0.031 Â· c2 \u201a¡ \u201a¢ C = \u201a£ 0 0 0 0.25 0.5 0 0.125 0.125 0.156 0.031 Assume we build the index for one keyword w contained in nodes P1,P3. We calculate (line 7) ObjectRanks for D \u201a€²A (taken by removing the backedges (dotted lines) from D A ). \u201a¤ \u201a\u2022 \u201a¦ r(P1) = 0.5 r(P2) = 0.5 Â· 0.5 Â· r(P1) =0.125 r(P3) = 0.5 r(P4) = 0.5 Â· 0.5 Â· r(P3)+0.5 Â· r(P2) = 0.188 r(P5) = 0.5 Â· 0.5 Â· r(P4)+0.5 Â· 0.5 Â· r(P3)+ 0.5 Â· 0.5 Â· r(P1) =0.297 r \u201a€² =[0.50.125 0.50.188 0.297] T Solving the equation of line 8: ï¿½ ï¿½ 0.156 0.031 = 0.125 0.125 ï¿½ c1 c2 ï¿½ï¿½ c1 c2 ï¿½ ï¿½ 0.297 + 0.188 we get: c = [0.361 0.263] T , where the overlinenotation selects from the matrices the 5-th and the 4-th lines, which correspond to the backnodes c1 and c2 respectively. The final ObjectRanks are (line 10): r = [0.50.190 0.680 0.266 0.361] T . This algorithm can be viewed as a way to reduce the n Ã— n ObjectRank calculation system of Equation 6, where n is the size of the graph, to the much smaller |L| Ã—|L| equations system of line 8 of Figure 9. Interestingly, the two equations systems have the same format r = Ar + b, only with different coefficient tables A, b. The degree of ï¿½ 571 reduction achieved is inversely proportional to the number of backnodes. The linear, first-degree equations system of line 8 can be solved using any of the well-studied arithmetic methods like Jacobi and Gauss-Seidel [15], or even using the PageRank iterative approach which is simpler because we do not have to solve each equation with respect to a variable. The latter is shown to perform better in Section 7. 5.4 Manipulating Initial ObjectRank values All algorithms so far assume that we do a fresh execution of the algorithm for every keyword. However, intuitively we expect nodes with high global ObjectRank to also have high ObjectRank with respect to many keywords. We exploit this observation by assigning the global ObjectRanks as initial values for each keyword specific calculation. Furthermore, we investigate a space vs. time tradeoff. In particular, assume we have limitations on the index size. Then we only store a prefix (the first p nodes) of the nodes\u201a€™ list (recall that the lists are ordered by ObjectRank) for each keyword. During the query stage, we use these values as initial values for the p nodes and a constant (we experimentally found 0.03 to be the most efficient for our datasets) for the rest6 . Both ideas are experimentally evaluated in Section 7.1. 6 Relevance Feedback Survey To evaluate the quality of the results of ObjectRank, we conducted two surveys. The first was performed on the DBLP database, with eight professors and Ph.D. students from the UC, San Diego database lab, who were not involved with the project. The second survey used the publications database of the IEEE Communications Society (COMSOC) 7 and involved five senior Ph.D. students from the Electrical Engineering Department. Each participant was asked to compare and rank two to five lists of top-10 results for a set of keyword queries, assigning a score of 1 to 10, according to the relevance of the results list to the query. Each result list was generated by a different variation of the ObjectRank algorithm. One of the results lists in each set was generated by the \u201a€œdefault\u201a€ ObjectRank configuration which used the authority transfer schema graph of Figure 4 and d = 0.85. The users knew nothing about the algorithms that produced each result list. The survey was designed to investigate the quality of ObjectRank when compared to other approaches or when changing the adjusting parameters. Effect of keyword-specific ranking. First, we assess the basic principle of ObjectRank, which is the keywordspecific scores. In particular, we compared the default (that is, with the parameters set to the values discussed in Section 1) ObjectRank with the global ranking algorithm that 6 Notice that, as we experimentally found, using the global ObjectRanks instead of a constant for the rest nodes is less efficient. The reason is that if a node u is not in the top-p nodes for keyword k, u probably has a very small ObjectRank with respect to k. However u may have a great global ObjectRank. 7 http://www.comsoc.org sorts objects that contain the keywords according to their global ObjectRank (where the base-set contains all nodes). Notice that this is equivalent to what Google used to8 do for Web pages, modulo some minor difference on the calculation of the relevance score by Google. The DBLP survey included results for two keyword queries: \u201a€œOLAP\u201a€ and \u201a€œXML\u201a€. The score was 7:1 and 5:3 in favor of the keyword-specific ObjectRank for the first and second keyword query respectively. The COMSOC survey used the keywords \u201a€œCDMA\u201a€ and \u201a€œUWB (ultra wideband)\u201a€ and the scores were 4:1 and 5:0 in favor of the keyword-specific approach respectively. Effect of authority transfer rates. We compared results of the default ObjectRank with a simpler version of the algorithm that did not use different authority transfer rates for different edge types, i.e., all edge types were treated equally. In the DBLP survey, for both keyword queries, \u201a€œOLAP\u201a€ and \u201a€œXML\u201a€, the default ObjectRank won with scores 5:3 and 6.5:1.5 (the half point means that a user thought that both rankings were equally good) respectively. In the COMSOC survey, the scores for \u201a€œCDMA\u201a€ and \u201a€œUWB\u201a€ were 3.5:1.5 and 5:0 respectively. Effect of the damping factor d. We tested three different values of the damping factor d: 0.1, 0.85, and 0.99, for the keyword queries \u201a€œXML\u201a€ and \u201a€œXML AND Index\u201a€ on the DBLP dataset. Two points were given to the first choice of a user and one point to the second. The scores were 2.5 : 8 : 13.5 and 10.5 : 11.5 : 2 (the sum is 24 since there are 8 users times 3 points per query) respectively for the three d values. We see that higher d values are preferred for the \u201a€œXML\u201a€, because \u201a€œXML\u201a€ is a very large area. In contrast, small d are preferable for \u201a€œXML AND Index\u201a€, because few papers are closely related to both keywords, and these papers typically contain both of them. The results were also mixed in the COMSOC survey. In particular, the damping factors 0.1, 0.85, and 0.99 received scores of 5:6:4 and 4.5:3.5:7 for the queries \u201a€œCDMA\u201a€ and \u201a€œUWB\u201a€ respectively. Effect of changing the weights of the keywords. We compared the combining functions for AND semantics of Equations 7 with the weighted combining method described in [6] for the two-keyword queries \u201a€œXML AND Index\u201a€ and \u201a€œXML AND Query\u201a€, in the DBLP survey. The use of the normalizing exponents proposed in Section 3.3 was preferred over the simple product function with ratios of 6:2 and 6.5:1.5 respectively. In the COMSOC survey, the same experiment was repeated for the keyword query \u201a€œdiversity combining\u201a€. The use of normalizing exponents was preferred at a ratio of 3.5:1.5. "},{"aspect":"expcomparison","tweet":" 7.1 Preprocessing stage threshold time (sec) nodes/keyword size (MB) 0.3 3702 84 2.20 0.5 3702 67 1.77 1.0 3702 46 1.26 Table 3: Index Creation for DBLPreal for epsilon =0.1 threshold time (sec) nodes/keyword size (MB) 0.05 80829 9.4 1.17 0.07 80829 8.3 1.08 0.1 80829 7.7 1.03 Table 4: Index Creation for COMSOC for epsilon =0.05 epsilon time (sec) nodes/keyword size (MB) 0.05 3875 67 1.77 0.1 3702 67 1.77 0.3 3517 67 1.77 Table 5: Index Creation for DBLPreal for threshold =0.5 General ObjectRank algorithm. Tables 3 and 4 show how the storage space for the ObjectRank index decreases as the ObjectRank threshold of the stored objects increases, for the real datasets. Notice that DBLPreal and COMSOC have 12, 341 and 40, 577 keywords respectively. Also notice that much fewer nodes per keyword have ObjectRank above the threshold in COMSOC, since this dataset is more sparse and has more keywords. The time to create the index does not change with threshold since threshold is not used during the main execution loop of the CreateIndex algorithm. Tables 5 and 6 show how the index build time decreases as epsilon increases. The reason is that fewer iterations are needed for the algorithm to converge, on the cost of lower accuracy of the calculated ObjectRanks. Notice that the storage space does not change with epsilon,as long as epsilon  threshold. Table 7 shows how the execution times and the storage requirements for the ObjectRank index scale with the database size for the DBLP synthetic datasets for epsilon = 0.05 and threshold = 0.1. Notice that the average number of nodes having ObjectRank higher than the threshold increases considerably with the dataset size, because the same keywords appear multiple times. General ObjectRank vs. almost-DAG algorithm. Figure 12 compares the index creation time of the General ObjectRank algorithm (Gen-OR) and two versions of the almost-DAG algorithm, on the DBLP1000 dataset, for various number of backnodes. The algebraic version (Alg-A- DAG) precisely solves the c = C Â· c + r \u201a€² system using an off the self algebraic solver. The PageRank version (PR-A- DAG) solves this system using the PageRank [9] iterative challenging than the Preprocessing stage 573 epsilon time (sec) nodes/keyword size (MB) 0.05 80829 7.7 1.03 0.07 77056 7.7 1.03 0.1 74337 7.7 1.03 Table 6: Index Creation for COMSOC for threshold = 0.1 Time (sec) 4 3.5 3 2.5 2 1.5 1 0.5 0 50 50 100 100 150 150 200 200 PR-A- DAG Alg-A- DAG Iterative part Calculate c r = Cc + r' PR-A- DAG Alg-A- DAG PR-A- DAG Alg-A- DAG PR-A- DAG 8.27 sec Alg-A- DAG Number of Backnodes and Used Algorithm Gen-OR Figure 12: Evaluate almost-DAG algorithm. method. The measured times are the average processing time for a single keyword and do not include the time to retrieve the base-set from the inverted text index, which is common to all methods. Also, the time to calculate C is omitted, since it C is calculated once for all keywords, and it requires a single pass over the graph. The Iterative part of the execution times corresponds to the one pass we perform on the DAG subgraph to calculate r \u201a€² for almost-DAG algorithms, and to the multiple passes which consist the whole computation for the General ObjectRank algorithm. Also, notice that epsilon =0.1 for this experiment (the threshold value is irrelevant since it does not affect the processing time, but only the storage space). The time to do the topological sorting is about 20 sec which is negligible compared to the time to calculate the ObjectRanks for all keywords. Initial ObjectRanks. This experiment shows how the convergence of the General ObjectRank algorithm is accelerated when various values are set as initial ObjectRanks. In particular, we compare the naive approach, where we assign an equal initial ObjectRank to all nodes, to the globalas-initial approach, where the global ObjectRanks are used as initial values for the keyword-specific ObjectRank calculations. We found that on DBLPreal (COMSOC), for epsilon =0.1, the naive and global-as-initial approaches take 16.3 (15.8) and 12.8 (13.7) iterations respectively. dataset time (sec) nodes/keyword size (MB) DBLP30 2933 6 0.3 DBLP100 11513 21 0.7 DBLP300 45764 65 1.7 DBLP1000 206034 316 7.9 DBLP3000 6398043 1763 43.6 Table 7: Index Creation for Synthetic Datasets. List length p iterations 13700 1 13000 1.2 8000 1.8 2500 3 800 8.7 100 13.3 0 16.3 List length p iterations 55000 1 54000 2.9 30000 5.3 13000 6.5 1600 7.8 400 10.7 25 13 0 15.8 (a) DBLPreal (b) COMSOC Figure 13: Number of iterations for various lengths of precomputed lists Furthermore, we evaluate the space vs. time tradeoff described in Section 5.4. Figure 13 shows the average number of iterations for epsilon =0.1 on DBLPreal and COM- SOC for various values of the precomputed list length p. "}]}