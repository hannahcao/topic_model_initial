{"user_name":" ÔªøThe Missing Link - A Probabilistic Model of Document Content and Hypertext Connectivity ","user_timeline":[{"aspect":"abstract","tweet":" Abstract We describe a joint probabilistic model for modeling the contents and inter-connectivity of document collections such as sets of web pages or research paper archives. The model is based on a probabilistic factor decomposition and allows identifying principal topics of the collection as well as authoritative documents within those topics. Furthermore, the relationships between topics is mapped out in order to build a predictive model of link content. Among the many applications of this approach are information retrieval and search, topic identification, query disambiguation, focused web crawling, web authoring, and bibliometric analysis. "},{"aspect":"expanalysis","tweet":" In this paper, we have described a specific probabilistic model which attempts to explain both the contents and connections of documents in an unstructured document base. While we have demonstrated preliminary results in several application areas, this paper only scratches the surface of potential applications of a joint probabilistic document model. "},{"aspect":"expdata","tweet":" We used two data sets in our experiments. The WebKB data set [11], consists of approximately 6000 web pages from computer science departments, classified by school and category (student, course, faculty, etc.). The Cora data set [10] consists of the abstracts and references of approximately 34,000 computer science research papers; of these, we used the approximately 2000 papers categorized into one of seven subfields of machine learning. "},{"aspect":"background","tweet":"  2 PLSA and PHITS PLSA [7] is a statistical variant of Latent Semantic Analysis (LSA) [4] that builds a factored multinomial model based on the assumption of an underlying document generation process. The starting point of (P)LSA is the term-document matrix N of word counts, i.e., Nij denotes how often a term (single word or phrase) ti occurs in document dj. In LSA, N is decomposed by a SVD and factors are identified with the left/right principal eigenvectors. In contrast, PLSA performs a probabilistic decomposition which is closely related to the non-negative matrix decomposition presented in [9]. Each factor is identified with a state zk (1 \u201aâ§ k \u201aâ§ K) of a latent variable with associated relative frequency estimates P (ti|zk) for each term in the corpus. A document dj is then represented as a convex combination of factors with mixing weights P (zk|dj), i.e., the predictive probabilities for terms in a particular document are constrained to be of the functional form P (ti|dj) = ÔøΩ k P (ti|zk)P (zk|dj), with non-negative probabilities and two sets of normalization constraints ÔøΩ i P (ti|zk) = 1 for all k and ÔøΩ k P (zk|dj) = 1 for all j. Both the factors and the document-specific mixing weights are learned by maximizing the likelihood of the observed term frequencies. More formally, PLSA aims at maximizing L = ÔøΩ i,j Nij log ÔøΩ k P (ti|zk)P (zk|dj). Since factors zk can be interpreted as states of a latent mixing variable associated with each observation (i.e., word occurrence), the Expectation-Maximization algorithm can be applied to find a local maximum of L. PLSA has been demonstrated to be effective for ad hoc information retrieval, language modeling and clustering. Empirically, different factors usually capture distinct \u201aÄútopics\u201aÄù of a document collection; by clustering documents according to their dominant factors, useful topic-specific document clusters often emerge (using the Gaussian factors of LSA, this approach is known as \u201aÄúspectral clustering\u201aÄù). It is important to distinguish the factored model used here from standard probabilistic mixture models. In a mixture model, each object (such as a document) is usually assumed to come from one of a set of latent sources (e.g. a document is either from z1 or z2). Credit for the object may be distributed among several sources because of ambiguity, but the model insists that only one of the candidate sources is the true origin of the object. In contrast, a factored model assumes that each object comes from a mixture of sources \u201aÄî without ambiguity, it can assert that a document is half z1 and half z2. This is because the latent variables are associated with each observation and not with each document (set of observations). PHITS [3] performs a probabilistic factoring of document citations used for bibliometric analysis. Bibliometrics attempts to identify topics in a document collection, as well as influential authors and papers on those topics, based on patterns in citation frequency. This analysis has traditionally been applied to references in printed literature, but the same techniques have proven successful in analyzing hyperlink structure on the world wide web [8]. In traditional bibliometrics, one begins with a matrix A of document-citation pairs. Entry Aij is nonzero if and only if document di is cited by document dj or, equivalently, if dj contains a hyperlink to di. 2 The principal eigenvectors of AA \u201aÄ≤ are then extracted, with each eigenvector corresponding to a \u201aÄúcommunity\u201aÄù of roughly similar citation patterns. The coefficient of a document in one of these eigenvectors is interpreted as the \u201aÄúauthority\u201aÄù of that document within the community \u201aÄî how likely it is to by cited within that community. A document\u201aÄôs coefficient in the principal eigenvectors of A \u201aÄ≤ A is interpreted as its \u201aÄúhub\u201aÄù value in the community \u201aÄî how many authoritative documents it cites within the community. In PHITS, a probabilistic model replaces the eigenvector analysis, yielding a model that has clear statistical interpretations. PHITS is mathematically identical to PLSA, with one distinction: instead of modeling the citations contained within a document (corresponding to PLSA\u201aÄôs modeling of terms in a document), PHITS models \u201aÄúinlinks,\u201aÄù the citations to a document. It substitutes a citation-source probability estimate P (cl|zk) for PLSA\u201aÄôs term probability estimate. As with PLSA and spectral clustering, the principal factors of the model are interpreted as indicating the principal citation communities (and by inference, the principal topics). For a given factor/topic zk, the probability that a document is cited, P (dj|zk), is interpreted as the document\u201aÄôs authority with respect to that topic.  "},{"aspect":"expintro","tweet":"  4 Experiments Nij ÔøΩ i \u201aÄ≤ Ni \u201aÄ≤ P (zk|ti, dj), P (cl|zk) = j ÔøΩ Alj ÔøΩ j l \u201aÄ≤ Al \u201aÄ≤ P (zk|cl, dj), (4) j i Nij ÔøΩ i \u201aÄ≤ Ni \u201aÄ≤ P (zk|ti, dj) + (1 \u201aàí Œ±) j ÔøΩ Alj ÔøΩ l l \u201aÄ≤ Al \u201aÄ≤ P (zk|cl, dj). (5) j In the introduction, we described many potential applications of the the joint probabilistic model. Some, like classification, are simply extensions of the individual PHITS and PLSA models, relying on the increased power of the joint model to improve their performance. Others, such as intelligent web crawling, are unique to the joint model and require its simultaneous modelling of a document\u201aÄôs contents and connections. In this section, we first describe experiments verifying that the joint model does yield improved classification compared with the individual models. We then describe a quantity called \u201aÄúreference flow\u201aÄù which can be computed from the joint model, and demonstrate its use in guiding a web crawler to pages of interest. 3 Our experiments used a tempered version of Equation 3 to minimize overfitting; see [7] for details. accuracy 0.38 0.36 0.34 0.32 0.3 0.28 0.26 0.24 WebKB data std error 0 0.2 0.4 0.6 0.8 1 alpha accuracy 0.5 0.45 0.4 0.35 0.3 0.25 Cora data std error 0 0.2 0.4 0.6 0.8 1 alpha Figure 1: Classification accuracy on the WebKB and Cora data sets for PHITS (Œ± = 0), PLSA (Œ± = 1) and the joint model (0 < Œ± < 1).  "},{"aspect":"problemdef","tweet":"  1 Introduction No text, no paper, no book can be isolated from the all-embracing corpus of documents it is embedded in. Ideas, thoughts, and work described in a document inevitably relate to and build upon previously published material. 1 Traditionally, this interdependency has been represented by citations, which allow authors to explicitly make references to related documents. More recently, a vast number of documents have been \u201aÄúpublished\u201aÄù electronically on the world wide web; here, interdependencies between documents take the form of hyperlinks, and allow instant access to the referenced material. We would like to have some way of modeling these interdependencies, to understand the structure implicit in the contents and connections of a given document base without resorting to manual clustering, classification and ranking of documents. The main goal of this paper is to present a joint probabilistic model of document content and connectivity, i.e., a parameterized stochastic process which mimics the generation of documents as part of a larger collection, and which could make accurate predictions about the existence of hyperlinks and citations. More precisely, we present an extension of our work on Probabilistic Latent Semantic Analysis (PLSA) [4, 7] and Probabilistic HITS (PHITS) [3, 8] and propose a mixture model to perform a simultaneous decomposition of the contingency tables associated with word occurrences and citations/links into \u201aÄútopic\u201aÄù factors. Such a model can be extremely useful in many applications, a few of which are: \u201aÄ¢ Identifying topics and common subjects covered by documents. Representing 1 Although the weakness of our memory might make us forget this at times. documents in a low-dimensional space can help understanding of relations between documents and the topics they cover. Combining evidence from terms and links yields potentially more meaningful and stable factors and better predictions. \u201aÄ¢ Identifying authoritative documents on a given topic. The authority of a document is correlated with how frequently it is cited, and by whom. Identifying topicspecific authorities is a key problems for search engines [2]. \u201aÄ¢ Predictive navigation. By predicting what content might be found \u201aÄúbehind\u201aÄù a link, a content/connectivity model directly supports navigation in a document collection, either through interaction with human users or for intelligent spidering. \u201aÄ¢ Web authoring support. Predictions about links based on document contents can support authoring and maintenance of hypertext documents, e.g., by (semi-) automatically improving and updating link structures. These applications address facets of one of the most pressing challenges of the \u201aÄúinformation age\u201aÄù: how to locate useful information in a semi-structured environment like the world wide web. Much of this difficulty, which has led to the emergence of an entire new industry, is due to the impoverished explicit structure of the web as a whole. Manually created hyperlinks and citations are limited in scope \u201aÄì the annotator can only add links and pointers to other document they are aware of and have access to. Moreover, these links are static; once the annotator creates a link between documents, it is unchanging. If a different, more relevant document appears (or if the cited document disappears), the link may not get updated appropriately. These and other deficiencies make the web inherently \u201aÄúnoisy\u201aÄù \u201aÄì links between relevant documents may not exist and existing links might sometimes be more or less arbitrary. Our model is a step towards a technology that will allow us to dynamically infer more reliable inter-document structure from the impoverished structure we observe. In the following section, we first review PLSA and PHITS. In Section 3, we show how these two models can be combined into a joint probabilistic term-citation model. Section 4 describes some of the applications of this model, along with preliminary experiments in several areas. In Section 5 we consider future directions and related research.  "},{"aspect":"solution","tweet":"  3 A Joint Probabilistic Model for Content and Connectivity  Linked and hyperlinked documents are generally composed of terms and citations; as such, both term-based PLSA and citation-based PHITS analyses are applicable. Rather than applying each separately, it is reasonable to merge the two analyses into a joint probabilistic model, explaining terms and citations in terms of a common set of underlying factors. Since both PLSA and PHITS are based on a similar decomposition, one can define the following joint model for predicting citations/links and terms in documents: P (ti|dj) = ÔøΩ P (ti|zk)P (zk|dj), P (cl|dj) = ÔøΩ P (cl|zk)P (zk|dj) . (1) k Notice that both decompositions share the same document-specific mixing proportions P (zk|dj). This couples the conditional probabilities for terms and citations: each \u201aÄútopic\u201aÄù 2 In fact, since multiple citations/links may exist, we treat Aij as a count variable. k has some probability P (cl|zk) of linking to document dl as well as some probability P (ti|zk) of containing an occurrence of term ti. The advantage of this joint modeling approach is that it integrates content- and link\u201aÄìinformation in a principled manner. Since the mixing proportions are shared, the learned decomposition must be consistent with content and link statistics. In particular, this coupling allows the model to take evidence about link structure into account when making predictions about document content and vice versa. Once a decomposition is learned, the model may be used to address questions like \u201aÄúWhat words are likely to be found in a document with this link structure?\u201aÄù or \u201aÄúWhat link structure is likely to go with this document?\u201aÄù by simple probabilistic inference. The relative importance one assigns to predicting terms and links will depend on the specific application. In general, we propose maximizing the following (normalized) log\u201aÄì likelihood function with a relative weight Œ±. L = ÔøΩ ÔøΩ Œ± ÔøΩ j i +(1 \u201aàí Œ±) ÔøΩ Nij ÔøΩ i \u201aÄ≤ Ni \u201aÄ≤ j l Alj ÔøΩ l \u201aÄ≤ Al \u201aÄ≤ j log ÔøΩ P (ti|zk)P (zk|dj) k log ÔøΩ ÔøΩ P (cl|zk)P (zk|dj) The normalization by term/citation counts ensures that each document is given the same weight in the decomposition, regardless of the number of observations associated with it. Following the EM approach it is straightforward to derive a set of re-estimation equations. For the E-step one gets formulae for the posterior probabilities of the latent variables associated with each observation 3 P (zk|ti, dj) = P (ti|zk)P (zk|dk) P (ti|dj) k (2) , P (zk|cl, dj) = P (cl|zk)P (zk|dj) . (3) P (cl|dj) The class-conditional distributions are recomputed in the M-step according to P (ti|zk) = ÔøΩ j along with the mixing proportions P (zk|dj) \u201aàù Œ± ÔøΩ  "},{"aspect":"expcomparison","tweet":"  4.1 Classification Although the joint probabilistic model performs unsupervised learning, there are a number of ways it may be used for classification. One way is to associate each document with its dominant factor, in a form of spectral clustering. Each factor is then given the label of the dominant class among its associated documents. Test documents are judged by whether their dominant factor shares their label. Another approach to classification (but one that forgoes clustering) is a factored nearest neighbor approach. Test documents are judged against the label of their nearest neighbor, but the \u201aÄúnearest\u201aÄù neighbor is determined by cosines of their projections in factor space. This is the method we used for our experiments. For the Cora and WebKB data, we used seven factors and six factors respectively, arbitrarily selecting the number to correspond to the number of human-derived classes. We compared the power of the joint model with that of the individual models by varying Œ± from zero to one, with the lower and upper extremes corresponding to PHITS and PLSA, respectively. For each value of Œ±, a randomly selected 15% of the data were reserved as a test set. The models were tempered (as per [7]) with a lower limit of Œ≤ = 0.8, decreasing Œ≤ by a factor of 0.9 each time the data likelihood stopped increasing. Figure 1 illustrates several results. First, the accuracy of the joint model (where Œ± is neither 0 nor 1), is greater than that of either model in isolation, indicating that the contents and link structure of a document collection do indeed corroborate each other. Second, the increase in accuracy is robust across a wide range of mixing proportions. 4.2 Reference Flow The previous subsection demonstrated how the joint model amplifies abilities found in the individual models. But the joint model also provides features found in neither of its progenitors. A document d may be thought of as occupying a point ÔøΩz = {P (z1|d), . . . , P (zk|d)} in the joint model\u201aÄôs space of factor mixtures. The terms in d act as \u201aÄúsignposts\u201aÄù describing ÔøΩz, and the links act as directed connections between that point and others. Together, they provide a reference flow, indicating a referential connection between one topic and another. This reference flow exists between arbitrary points in the factor space, even in the absence of documents that map directly to those points. Consider a reference from document di to document dj, and two points in factor space ÔøΩzm and ÔøΩzn, not particularly associated with di or dj. Our model allows us to compute P (di|ÔøΩzm) and P (dj|ÔøΩzn), the probability that the combination of factors at ÔøΩzm student/ department faculty project dept./ faculty course Figure 2: Principal reference flow between the primary topics identified in the examined subset of the WebKB archive. and ÔøΩzn are responsible for di and dj respectively. Their product P (di|ÔøΩzm)P (dj|ÔøΩzn) is then the probability that the observed link represents a reference between those two points in factor space. By integrating over all links in the corpus we can compute, fmn = ÔøΩ i,j:AijÔøΩ=0 P (di|ÔøΩzm)P (dj|ÔøΩzn), an unnormalized \u201aÄúreference flow\u201aÄù between ÔøΩzm and ÔøΩzn. Figure 2 shows the principal reference flow between several topics in the WebKB archive. 4.3 Intelligent Web Crawling with Reference Flow Let us suppose that we want to find new web pages on a certain topic, described by a set of words composed into a target pseudodocument dt. We can project dt into our model to identify the point ÔøΩzt in factor space that represents that topic. Now, when we explore web pages, we want to follow links that will lead us to new documents that also project to ÔøΩzt. To do so, we can use reference flow. Consider a web page ds (or section of a web page 4 ). Although we don\u201aÄôt know where its links point, we do know what words it contains. We can project them as a peudodocument to find ÔøΩzs the point in factor space the page/section occupies, prior to any information about its links. We can then use our model to compute the reference flow fst indicating the (unnormalized) probability that a document at ÔøΩzs would contain a link to one at ÔøΩzt. frequency 350 300 250 200 150 100 50 true source \u201aÄôplacebo\u201aÄô 0 0 10 20 30 40 50 60 70 80 90 100 rank As a greedy solution, we could simply follow links in documents or sections that have the highest reference flow to- Figure 3: When ranked according to magward the target topic. Or if computation nitude of reference flow to a designated tar- is no barrier, we could (in theory) use refget, a \u201aÄútrue source\u201aÄù scores much higher than erence flow as state transition probabili- a placebo source document drawn at random. ties and find an optimal link to follow by treating the system as a continuous-state Markov decision process. 4 Though not described here, we have had success using our model for document segmentation, following an approach similar to that of [6]. By projecting successive n-sentence windows of a document into the factored model, we can observe its trajectory through \u201aÄútopic space.\u201aÄù A large jump in the factor mixture between successive windows indicates a probable topic boundary in document. To test our model\u201aÄôs utility in intelligent web crawling, we conducted experiments on the WebKB data set using the greedy solution. On each trial, a \u201aÄútarget page\u201aÄù dt was selected at random from the corpus. One \u201aÄúsource page\u201aÄù ds containing a link to the target was identified, and the reference flow fst computed. The larger the reference flow, the stronger our model\u201aÄôs expectation that there is a directed link from the source to the target. We ranked this flow against the reference flow to the target from 100 randomly chosen \u201aÄúdistractor\u201aÄù pages dr1, dr2 . . . , dr100. As seen in Figure 3, reference flow provides significant predictive power. Based on 2400 runs, the median rank for the \u201aÄútrue source\u201aÄù was 27/100, versus a median rank of 50/100 for a \u201aÄúplacebo\u201aÄù distractor chosen at random. Note that the distractors were not screened to ensure that they did not also contain links to the target; as such, some of the high-ranking distractors may also have been valid sources for the target in question.  "}]}