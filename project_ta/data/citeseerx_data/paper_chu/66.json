{"user_name":" Authoritative Sources in a Hyperlinked Environment ","user_timeline":[{"aspect":"abstract","tweet":" Abstract  The link structure of a hypermedia environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. Versions of this principle have been studied in the hypertext research community and (in a context predating hypermedia) through journal citation analysis in the field of bibliometrics. But for the problem of searching in hyperlinked environments such as the World Wide Web, it is clear from the prevalent techniques that the information inherent in the links has yet to be fully exploited. In this work we develop a new method for automatically extracting certain types of information about a hypermedia environment from its link structure, and we report on experiments that demonstrate its effectiveness for a variety of search problems on the www.  The central problem we consider is that of determining the relative \"authority\" of pages in such environments. This issue is central to a number of basic hypertext search tasks; for example, if the result of a query-based search consists of a large set of relevant pages, one may wish to select a small subset of the most \"definitive\" or \"authoritative\" pages to present to a user. At the same time, it is clearly difficult to formulate a definition of authority precise enough to be used in such contexts. We propose and test an algorithmic formulation of the notion of authority, based on a method for locating dense bipartite communities in the link structure. Our formulation has an interesting interpretation in terms of the eigenvectors of certain matrices associated with the link graph; this motivates additional heuristics for clustering and for computing a type of link-based similarity among hyperlinked documents.   Preliminary versions of this paper appear in the Proceedings of the ACM-SIAM Symposium on Discrete Algorithms, 1998, and as IBM Research Report RJ 10076, May 1997.     This work was performed in large part while on leave at the IBM Almaden Research Center, San Jose CA 95120. The author is currently supported by an Alfred P. Sloan Research Fellowship and by NSF Faculty Early Career Development Award CCR-9701399. "},{"aspect":"expanalysis","tweet":" Basic Results  We now give some sample results, using the queries discussed in the introduction. For (\"web browsers\") we obtain  (\"web browsers\") Authorities .225 http://www.ncsa.uiuc.edu/SDG/Software/WinMosaic/HomePage.html NCSA Windows Mosaic Home Page  .202 http://home.mcom.com/home/welcome.html Welcome to Netscape  .196 http://galaxy.einet.net/EINet/EINet.html TradeWave Corporation  .188 http://www.interport.net/slipknot/slipknot.html .... SlipKnot Home Page ....  .188 http://galaxy.einet.net/EINet/WinWeb/WinWebHome.html winWeb and MacWeb  .185 http://www.microsoft.com/ie/ Microsoft Internet Explorer Above, we list the top six authorities found by the algorithm. Each line gives the x-value of a page, followed by its url, and then its html title. Thus, we note that the above set of authorities includes home pages for NCSA Mosaic, Netscape, and Microsoft Internet Explorer (as well as home pages for other browser manufacturers). It is also worth noting that no pages from any of these three domains were included in the initial root set S provided by AltaVista. For the queries (java), (+censorship +net), and (Gates), we obtain the following authorities. (For the second query, the syntax indicates that both words must appear in the initial pages returned by the search engine.)  (java) Authorities .328 http://www.gamelan.com/ Gamelan  .251 http://java.sun.com/ JavaSoft Home Page  .190 http://www.digitalfocus.com/digitalfocus/faq/howdoi.html The Java Developer: How Do I...  13 .190 http://lightyear.ncsa.uiuc.edu/��srp/java/javabooks.html The Java Book Pages  .183 http://sunsite.unc.edu/javafaq/javafaq.html comp.lang.java FAQ (+censorship +net) Authorities .421 http://www.eff.org/ EFFweb - The Electronic Frontier Foundation  .394 http://www.eff.org/blueribbon.html The Blue Ribbon Campaign for Online Free Speech  .390 http://www.cdt.org/ The Center for Democracy and Technology  .374 http://www.vtw.org/ Voters Telecommunications Watch  .291 http://www.aclu.org/ ACLU: American Civil Liberties Union (Gates) Authorities .643 http://www.roadahead.com/ Bill Gates: The Road Ahead  .458 http://www.microsoft.com/ Welcome to Microsoft  .440 http://www.microsoft.com/corpinfo/bill-g.htm Among all these pages, the only one which occurred in the corresponding root set S was  www.roadahead.com/, under the query (Gates); it was ranked 123  rd  by AltaVista. This is natural in view of the fact that many of these pages do not contain any occurrences of the initial query term. All of the above topics were sufficiently \"broad\" that the principal set of authorities (and hubs) were relevant to the query. We will return to this issue in Section 6, and study some cases in which the principal community was not so closely tied to the initial query topic. 3 Multiple Communities  The basic search method described above is, in a sense, finding the densest community of hubs and authorities in the base set T . However, there are a number of settings in which one might be interested in finding several distinct communities among the set of pages. There may be several dense communities, only one of which is relevant to the query topic. Alternately, it is possible that there will be several communities, all of them relevant, but well-separated from one another in the graph on T for a variety of possible reasons. For example, (1) The string may have several different meanings. E.g. (jaguar) (an example drawn from [5]). (2) The string may arise as a term in the context of multiple technical communities. E.g. (\"randomized algorithms\"). (3) The string may refer to a highly polarized issue, involving groups that are not likely to link to one another. E.g. (abortion). The non-principal eigenvectors of the matrices A  T  A and AA  T  provide us with a natural way to extract multiple communities of hubs and authorities from the base set T . We begin by noting the following simple fact.  Theorem 3.1 AA  T  and A  T  A have the same multiset of eigenvalues, and their eigenvectors can be chosen so that ! i (AA  T  ) = A! i (A  T  A).  14 Proof. Let  i =  i (A  T  A) be an eigenvalue of A  T  A, and v i = ! i (A  T  A) the associated eigenvector. It suffices to show that Av i is an eigenvector of AA  T  , and that the associated eigenvalue is  i . We observe (AA  T  )(Av i ) = A((A  T  A)v i ) = A( i v i ) =  i (Av i ): Thus, each pair of eigenvectors x    i = ! i (A  T  A), y    i = ! i (AA  T  ), related as in Theorem 3.1, has the following property: applying an I operation to (x    i ; y    i ) keeps the x-weights parallel to x    i , and applying an O operation to (x    i ; y    i ) keeps the y-weights parallel to y    i . Hence, each pair of weights (x    i ; y    i ) has precisely the mutually reinforcing relationship that we are seeking in authority/hub pairs. Moreover, applying (IO) (resp. (OI)) multiplies the magnitude of x    i (resp. y    i ) by a factor of j i j; thus j i j gives precisely the extent to which the hub weights y    i and authority weights x    i  reinforce one another. Now, the non-principal eigenvectors have both positive and negative entries. Hence each pair (x    i ; y    i ) provides us with two communities of authorities and hubs: (X  +  i ; Y  +  i ), consisting of the c  pages with the most positive coordinates in x    i and y    i ; and (X  \\Gamma  i ; Y  \\Gamma  i ), consisting of the c pages with the most negative coordinates in x    i and y    i . Such communities have the same intuitive meaning as those produced in the previous section, although the algorithm to find them --- based on nonprincipal eigenvectors --- is certainly less intuitive than the method of iterated I and O operations. (It is possible to modify that method by adding a Gram-Schmidt step so as to obtain these additional communities.) Note that communities associated with eigenvectors of larger absolute value will tend to have more intuitive meaning, since they are \"denser\" as subgraphs in the link structure; we will sometimes refer to this notion as the strength of a community. Another interesting feature of the communities derived from non-principal eigenvectors is the following. Drawing on the heuristic intuition underlying spectral graph partitioning [7, 11, 34], one expects pairs of communities (X  +  i ; Y  +  i ) and (X  \\Gamma  i ; Y  \\Gamma  i ) associated with the same eigenvector to be very sparsely connected in the underlying graph. In some cases, this sparse linkage can have meaning in the context of the query topic.  Basic Results  We now give some examples of the types of communities one obtains by this methods, using the queries mentioned above. One interesting phenomenon that arises is the following. The pages with large coordinates in the first few non-principal eigenvectors tend to recur, so that essentially the same community of hubs and authorities will often be generated by several of the strongest non-principal eigenvectors. Of course, despite being similar in their large coordinates, these eigenvectors remain orthogonal due to differences in the coordinates of smaller absolute value. As a result, one obtains fewer distinct communities than might otherwise be expected from a set of non-principal eigenvectors. This notion is also reflected in the output below, where we have selected (by hand) several distinct communities from among the first few non-principal eigenvectors. The identification of \"distinct\" communities might be an interesting task to make automatic, though this does not appear to be a particularly difficult challenge.  15 We issue the first query as (jaguar jaguars), simply as one way to search for either the word or its plural. For this query, the strongest communities concerned the Atari Jaguar product, the NFL football team from Jacksonville, and the automobile.  (jaguar jaguars) Authorities: principal eigenvector .370 http://www2.ecst.csuchico.edu/��jschlich/Jaguar/jaguar.html .347 http://www-und.ida.liu.se/��t94patsa/jserver.html .292 http://tangram.informatik.uni-kl.de:8001/��rgehm/jaguar.html .287 http://www.mcc.ac.uk/ dlms/Consoles/jaguar.html Jaguar Page (jaguar jaguars) Authorities: 2  nd  non-principal vector, positive end .255 http://www.jaguarsnfl.com/ Official Jacksonville Jaguars NFL Website  .137 http://www.nando.net/SportServer/football/nfl/jax.html Jacksonville Jaguars Home Page  .133 http://www.ao.net/��brett/jaguar/index.html Brett's Jaguar Page  .110 http://www.usatoday.com/sports/football/sfn/sfn30.htm Jacksonville Jaguars (jaguar jaguars) Authorities: 3  rd  non-principal vector, positive end .227 http://www.jaguarvehicles.com/ Jaguar Cars Global Home Page  .227 http://www.collection.co.uk/ The Jaguar Collection - Official Web site  .211 http://www.moran.com/sterling/sterling.html .211 http://www.coys.co.uk/ For the query (\"randomized algorithms\"), none of the strongest communities could be said to be precisely on the query topic, though they all consisted of thematically related pages on a closely related topic. They included home pages of theoretical computer scientists, compendia of mathematical software, and pages on wavelets.  (\"randomized algorithms\") Authorities: 1  st  non-principal vector, positive end .125 http://theory.lcs.mit.edu/��goemans/ Michel X. Goemans  .122 http://theory.lcs.mit.edu/��spielman/ Dan Spielman's Homepage  .122 http://www.nada.kth.se/��johanh/ Johan Hastad  .122 http://theory.lcs.mit.edu/��rivest/ Ronald L. Rivest : HomePage (\"randomized algorithms\") Authorities 1  st  non-principal vector, negative end -.00116 http://lib.stat.cmu.edu/ StatLib Index  -.00115 http://www.geo.fmi.fi/prog/tela.html Tela  -.00107 http://gams.nist.gov/ GAMS : Guide to Available Mathematical Software  -.00107 http://www.netlib.org Netlib (\"randomized algorithms\") Authorities 4  th  non-principal vector, negative end -.176 http://www.amara.com/current/wavelet.html Amara's Wavelet Page  -.172 http://www-ocean.tamu.edu/��baum/wavelets.html Wavelet sources  -.161 http://www.mathsoft.com/wavelets.html Wavelet Resources  -.143 http://www.mat.sbg.ac.at/��uhl/wav.html Wavelets We mentioned that the two communities associated with the positive and negative ends of the same non-principal eigenvector are often \"well-separated\" in the link structure. One case in which the meaning of this separation is particularly striking is for the query (abortion). The  16 natural question is whether one of the non-principal eigenvectors produces distinct communities of pro-choice and pro-life pages. The issue is complicated by the existence of hub pages that link extensively to pages from both sides; but in fact the 2  nd  non-principal eigenvector produces a very clear separation:  (abortion) Authorities: 2  nd  non-principal vector, positive end .321 http://www.caral.org/abortion.html Abortion and Reproductive Rights Internet Resources  .219 http://www.plannedparenthood.org/ Welcome to Planned Parenthood  .195 http://www.gynpages.com/ Abortion Clinics OnLine  .172 http://www.oneworld.org/ippf/ IPPF Home Page  .162 http://www.prochoice.org/naf/ The National Abortion Federation  .161 http://www.lm.com/��lmann/feminist/abortion.html (abortion) Authorities: 2  nd  non-principal vector, negative end -.197 http://www.awinc.com/partners/bc/commpass/lifenet/lifenet.htm LifeWEB  -.169 http://www.worldvillage.com/wv/square/chapel/xwalk/html/peter.htm Healing after Abortion  -.164 http://www.nebula.net/��maeve/lifelink.html -.150 http://members.aol.com/pladvocate/ -.144 http://www.clark.net/pub/jeffd/factbot.html The Right Side of the Web  -.144 http://www.catholic.net/HyperNews/get/abortion.html 4 Similar-Page Queries  Suppose we have found a page p that is of interest --- perhaps it is an authoritative page on a topic of interest --- and we want to use the link structure of the environment to discover whether there exist pages that are \"similar\" to p. We show how a minor modification of the framework developed above provides a novel type of link-based page similarity. It is based on the following notion. If we build an appropriate \"neighborhood\" T of pages around p, and p turns out to be a good authority in some community of T , then the other authorities in the same community as p will exhibit a type of linked-based similarity to p.  The algorithm is simply the following. We first define the root set S to be k (say 200) pages that point to the initial page p. We then run the algorithm of Section 2 starting from this root set: we form the enlarged base set T , and find hubs and authorities in this set. In many cases, the results can be quite compelling. In the following examples, we begin from the home page of Honda Motor Company, www.honda.com, and the New York Stock Exchange,  www.nyse.com. (www.honda.com) Authorities: principal eigenvector .202 http://www.toyota.com/ Welcome to @Toyota  .199 http://www.honda.com/ Honda  .192 http://www.ford.com/ Ford Motor Company  .173 http://www.bmwusa.com/ BMW of North America, Inc.  .162 http://www.volvocars.com/ VOLVO  .158 http://www.saturncars.com/ Welcome to the Saturn Web Site  .155 http://www.nissanmotors.com/ NISSAN - ENJOY THE RIDE  .145 http://www.audi.com/ Audi Homepage  .139 http://www.4adodge.com/ 1997 Dodge Site  17 .136 http://www.chryslercars.com/ Welcome to Chrysler (www.nyse.com) Authorities: principal eigenvector .208 http://www.amex.com/ The American Stock Exchange - The Smarter Place to Be  .146 http://www.nyse.com/ New York Stock Exchange Home Page  .134 http://www.liffe.com/ Welcome to LIFFE  .129 http://www.cme.com/ Futures and Options at the Chicago Mercantile Exchange  .120 http://update.wsj.com/ The Wall Street Journal Interactive Edition  .118 http://www.nasdaq.com/ The Nasdaq Stock Market Home Page - Reload Often  .117 http://www.cboe.com/ CBOE - The ChicagoBoard Options Exchange  .116 http://www.quote.com/ 1- Quote.com - Stock Quotes, Business News, Financial Market  .113 http://networth.galt.com/ NETworth  .109 http://www.lombard.com/ Lombard Home Page Note the difficulties inherent in compiling such lists through text-based methods: many of the above pages consist almost entirely of images, with very little text; and the text that they do contain has very little overlap. Our approach, on the other hand, is determining, via the presence of links, what the creators of www pages tend to \"classify\" together with the given pages www.honda.com  and www.nyse.com.  In order for this method to be most effective, the initial page p should have fairly large indegree, and \"locally\" be a strong authority. Otherwise, it is likely not to show up among the top authorities in the first few communities. However, even when p does not show up among the top authorities, the resulting output is often valuable as a a type of \"broad-topic classification\" of  p. We illustrate this with the home page of the ACM Special Interest Group on Algorithms and Computation Theory, sigact.acm.org. The page itself did not rank highly in any of the first few eigenvectors; however, the principal community is quite revealing as a summary of the strongest authorities in the \"vicinity\" of SIGACT.  (sigact.acm.org) Authorities: principal eigenvector .197 http://www.siam.org/ Society for Industrial and Applied Mathematics  .166 http://dimacs.rutgers.edu/ Center for Discrete Mathematics and Theoretical Computer Science  .150 http://www.computer.org/ IEEE Computer Society  .148 http://www.yahoo.com/ Yahoo!  .145 http://e-math.ams.org/ e-MATH Home Page  .141 http://www.ieee.org/ IEEE Home Page  .140 http://glimpse.cs.arizona.edu:1994/bib/ Computer Science Bibliography Glimpse Server  .129 http://www.eccc.uni-trier.de/eccc/ ECCC - The Electronic Colloquium on Computational Complexity  .129 http://www.cs.indiana.edu/cstr/search UCSTRI --- Cover Page  .118 http://euclid.math.fsu.edu/Science/math.html The World-Wide Web Virtual Library: Mathematics One variant of this phenomenon that happens quite frequently is the following. Since the home pages of search engines and computer companies have strong representation in the vicinity of essentially every page on the www, they often dominate the list of authorities in the principal community, regardless of the topic of the initial page p. This a case in which the communities associated with non-principal eigenvectors can be particularly valuable: it is often possible to find a strong non-principal community in which the \"noise\" introduced by such pages is completely  18 eliminated, and what remains is closely related to the initial page p. This is a good example of the notion of \"on-topic\" versus \"off-topic\" communities, discussed earlier. A very clear illustration of this phenomenon is provided by a similar-page query starting from www.nytimes.com, the home page of the New York Times. First, consider the authorities in the principal community.  (www.nytimes.com) Authorities: principal eigenvector .287 http://www.yahoo.com/ Yahoo!  .181 http://www.nytimes.com/ The New York Times on the Web  .170 http://www.usatoday.com/ USA TODAY  .165 http://www.cnn.com/ CNN Interactive  .124 http://www.mckinley.com/ Welcome to Magellan!  .120 http://www.altavista.digital.com/ AltaVista Search: Main Page  .119 http://www.excite.com/ Excite  .117 http://www.microsoft.com/ Welcome to Microsoft  .108 http://www.whitehouse.gov/ Welcome to the White House  .107 http://www.lycos.com/ Lycos, Inc. Home Page The above list consists, essentially, of a mixture of two types of pages: news organizations and computer/Internet companies. As shown below, the first non-principal eigenvector separates this superposition into its two components: it has computer companies at its positive end and news organizations at its negative end.  (www.nytimes.com) Authorities: 1  st  non-principal vector, positive end .111 http://www.microsoft.com/ Welcome to Microsoft  .110 http://www.ibm.com/ IBM Corporation  .101 http://www.apple.com/ Apple Computer  .100 http://www.hp.com/ Welcome to Hewlett-Packard  .098 http://www.sun.com/ Sun Microsystems  .097 http://www.intel.com/ Welcome to Intel  .097 http://www.novell.com/ Novell World Wide: Corporate Home Page  .087 http://www.ustreas.gov/ Welcome To The Department of Treasury  .084 http://www.compuserve.com/ Welcome to CompuServe  .081 http://www.lcs.mit.edu/ MIT Lab for Computer Science Web Page (www.nytimes.com) Authorities: 1  st  non-principal vector, negative end -.220 http://www.nytimes.com/ The New York Times on the Web  -.169 http://www.usatoday.com/ USA TODAY  -.138 http://www.cnn.com/ CNN Interactive  -.091 http://www.sjmercury.com/ Mercury Center  -.080 http://www.chicago.tribune.com/ The Chicago Tribune  -.076 http://www.washingtonpost.com/ Welcome to WashingtonPost.com  -.074 http://www.cbs.com/ EYE ON THE NET @ CBS  -.066 http://www.npr.org/ Welcome to NPR  -.063 http://www.telegraph.co.uk/ Electronic Telegraph  -.061 http://nytimesfax.com/ TimesFax  "},{"aspect":"expdata","tweet":""},{"aspect":"background","tweet":" 1 1 Introduction  The link structure of a hypermedia environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. Versions of this principle have been studied in the hypertext research community [3, 13, 26, 36] and (in a context predating hypermedia) through journal citation analysis in the field of bibliometrics [37]. But for the problem of searching in hyperlinked environments such as the World Wide Web, it is clear from the prevalent techniques that the information inherent in the links has yet to be fully exploited. In this work we develop a new method for automatically extracting certain types of information about a hypermedia environment from its link structure, and we report on experiments that demonstrate its effectiveness in a variety of contexts on the www.  Our methods seem to apply fairly broadly, to structures that are implicitly, as well as explicitly, linked. In the present context, we focus on the development and testing of algorithms for searching in hypermedia, particularly on the World Wide Web. We will show some interesting connections between our algorithms and the spectral properties of certain matrices derived from the link structure of the underlying environment; it is through these connections that we will be able to develop some insight into their behavior, and to prove certain convergence properties.  Searching, in the setting of the www and the present work, could be defined as the process of discovering pages that are relevant to a given query. The quality of a search method necessarily requires human evaluation, to make concrete the various loaded terms in the previous sentence. We begin from the observation that improving the quality of search methods on the www is, at the present time, a rich and interesting problem that is in many ways orthogonal to concerns of algorithmic efficiency and storage. In particular, consider that current search engines typically index a large fraction of the www and respond on the order of seconds. Although there would be considerable utility in a search tool whose response time was on the order of minutes, provided that the results were of significantly greater value to a user, it has typically been very hard to say what  such a search tool should be computing with this extra time. Clearly we are lacking an objective function that is both concretely defined and corresponds to human notions of quality. Our work is centered around this issue of improving the quality of search results; we are seeking new methodologies for searching in large hyperlinked environments, rather than focusing on efficient implementations of existing techniques. In particular, the initial emphasis of our work is to define,  by algorithmic means, a novel type of quality measure that we refer to as the authority of a document in hypermedia; a highly authoritative document intuitively represents a high-quality response to a broad user query. Our algorithmic definition naturally provides an efficient means to compute this authority measure; moreover, an analysis of our method in terms of eigenvectors turns out motivate additional useful heuristics that would have been difficult to formulate without appealing to spectral methods. We feel that the interplay between this spectral analysis and the motivation for our heuristics is one of the interesting features of this work. "},{"aspect":"expintro","tweet":""},{"aspect":"problemdef","tweet":" Queries and Authoritative Sources  We view searching as beginning from a user-supplied query. It seems best not to take too unified a view of the notion of a query: there are many possible types of queries that one might wish to  2 pose, and it is likely that the proper handling of each type requires a different set of techniques. Consider, for example, the following types of queries.  ffl Broad-topic queries. E.g., \"Find information about web browsers.\"  ffl Specific queries. E.g., \"Has the www Consortium endorsed the HTML 3.2 specification?\"  ffl Similar-page queries. E.g., \"Find pages `similar' to www.lcs.mit.edu.\"  Concentrating on just the first two types of queries for now, we see that they present very different sorts of obstacles. The difficulty in handling specific queries is centered, roughly, around what could be called the Scarcity Problem: there are very few pages that contain the required information, and it is difficult to determine the identity of these pages. Much classical work in information retrieval has focused on this type of problem. For broad-topic queries, on the other hand, one could easily expect to find many thousand relevant pages in an environment such as the www; such a set of pages might be generated by variants of term-matching (e.g. one enters a string such as \"web browsers,\" \"Gates,\" or \"censorship\" into a search engine such as AltaVista [6]), or by more sophisticated means. Thus, there is not an issue of scarcity here. Instead, the fundamental difficulty lies in what could be called the  Abundance Problem: The number of pages that could reasonably be returned as \"relevant\" is far too large for a human user to digest. Thus, to provide effective methods for automated search under these constraints, one does not necessarily need stronger versions of classical information retrieval notions such as relevance; rather one needs a method of providing a user, from a large set of relevant pages, a small collection of the most \"authoritative\" or \"definitive\" ones. Our work here originates from these issues raised by the Abundance Problem, and the problem of discovering the most authoritative pages in a large hyperlinked environment. The problem is particularly interesting in that much of its complexity has nothing to do with the \"search\" component; rather, we face the dilemma that in order to search for authoritative documents, one must first formulate a concrete means of recognizing them. Unfortunately, \"authority\" is perhaps an even more nebulous concept than \"relevance,\" again highly subject to human judgment; and our algorithmic framework must take this into account. It is here that we bring the notion of links into the picture. We claim that an environment such as the www is explicitly annotated with precisely the type of human judgment that we need in order to formulate a notion of authority. Specifically, the creation of a link in the www represents a concrete indication of the following type of judgment: the creator of page p, by including a link to page q, has in some measure conferred authority on q. Of course, this notion is clouded by the fact that links are created for a wide variety of reasons, many of which have nothing to do with the conferral of authority. Thus we are faced with the following problem: given the vast size of the underlying environment, can we synthesize the unreliable information contained in the presence of individual links in a way that provides a set of authoritative pages relevant to an initial query?  A Crude Approximation  Naturally, we first examine the simplest implementation of the above idea: if the presence of links is an indication of authority, can one simply use the in-degree of a page as a measure of its authority?  There are several variants of this idea, and none works very well. First of all, since we are looking for pages that are relevant as well as authoritative, we must specify the subgraph of the  3 www in which we are computing the in-degree. As an example, consider the query \"java\", a string contained in more than two million pages on the www.  (1) One approach is to define a root set S as follows. For a number k (say 200), we define S to be the top k pages indexed by AltaVista (or some other term-based search engine); we then rank pages according to their in-degree in the subgraph induced by S. There are two severe problems with this approach. First, this subgraph typically has very few edges; a large fraction (if not most) of the nodes will be isolated. Second, the root set S, for any reasonable value of k, omits most of the pages that one would normally consider authoritative for the query \"java\"; they are not ranked highly enough by AltaVista's scoring function. (2) A more reasonable approach is the following. We start from the same root set S, and we then grow it to a larger base set T , consisting of all pages that either belong to S, point to a page in S, or are pointed to by a page in S. (To prevent the size of T from exploding, we arbitrarily truncate the in-degree of pages in S to some upper bound d.) We then rank pages by their in-degree in the subgraph induced by T . The base set T has two attractive features: it is extremely likely to contain many authoritative pages on the topic (since they will be pointed to by pages in S); and for sufficiently broad queries, it will still generally be \"rich\" in relevant pages. Unfortunately, the results are still far from satisfying; we list the top eight pages for \"java\"  below.  http://www.gamelan.com Gamelan  http://java.sun.com JavaSoft Home Page  http://getawaynet.com/index.html GetAwayNet Home Page  http://getawaynet.com/VacationNetwork/vnetwork.html Caribbean Vacation Network Home Page - 1-800-423-4095  http://www.sn.no/��espeset Java Programming  http://www.net4u.ch/net4u/ger/index-ger.html Net4U Tielseite  http://www.net4u.ch/net4u/eng/index-eng.html Net4U Home Page  http://www.amazon.com/exec/obidos/stores/jollyrog Welcome to Amazon.com Books! Earth's Biggest Bookstore The above list has several features that are worth comment, as they arise generally when ranking the results of a broad-topic query purely by in-degree. A promising feature of the list is that the first two pages should certainly be viewed as \"good\" answers. However, five of the six other pages are not relevant to the original query --- they are advertisements for Carribean vacations, pages for a Swiss Internet consulting company, and the home page of Amazon Books; and the sixth page is an advertisement for a single book on Java programming. Although these pages all have large in-degree, they lack any thematic unity; we have not achieved the goal of providing pages that are authoritative and relevant. "},{"aspect":"solution","tweet":" Our Approach  It is tempting to conclude that the only way to preserve relevance while looking for authoritative pages is to make use of the text of pages, as current search engines do. However, this is a tricky issue. For example, all but the last of the eight pages listed above contain the string \"java\",  most of them multiple times. Moreover, to use our three earlier examples, it would natural to want to find Netscape's home page for the query \"web browsers\", Microsoft's home page for the  4 query \"Gates\", and the Electronic Frontier Foundation's home page for the query \"censorship\".  Unfortunately, none of these pages contain the respective query term. While much work has gone into text-based methods for circumventing these relevance-related difficulties (e.g. latent semantic indexing [9] and a range of other clustering techniques in information retrieval), our goal here is to understand how much can be accomplished by focusing on link structures, for finding pages that are simultaneously authoritative and relevant. We will see that quite striking results can be achieved while making essentially no use of text whatsoever. Our approach proceeds essentially as follows. From an initial query, we form the base set T  defined previously; this set has the useful features discussed above. We now make the following observation. Authoritative pages relevant to the initial query should not only have large in-degree; since they are all authorities on a common topic, there should also be considerable overlap in the  sets of pages that point to them. Thus, in addition to highly authoritative pages, we expect to find what could be called hub pages: these are pages that have links to multiple relevant authoritative pages. It is these hub pages that \"pull together\" authorities on a common topic, and allow us to throw out unrelated pages of large in-degree. hubs authorities unrelated page of large in-degree Figure 1: A dense community of hubs and authorities. Hubs and authorities exhibit what could be called a mutually reinforcing relationship: a good  hub is a page that points to many good authorities; a good authority is a page that is pointed to by many good hubs. Clearly, if we wish to identify hubs and authorities within the base set T , we need a method for breaking this circularity. In Section 2, we describe our basic algorithm for this task --- a method that iteratively propagates \"authority weight\" and \"hub weight\" across links of the web graph, converging simultaneously to steady states for both types of weights. The final output of our algorithm, derived from an equilibrium set of weights, is a pair of sets (X; Y ), where X is a small set of authorities and Y is a small set of hubs; this is the desired small set of \"high-quality\" pages that can be returned in response to a user query. We refer to the pair of sets (X; Y ) as a  community of hubs and authorities, characterized by their mutually reinforcing relationship; one can picture this pair as the two sides of a dense directed bipartite subgraph of the base set T , with the hubs linking densely to the authorities. (A skeletal example is depicted in Figure 1; in reality, of course, the picture is not nearly this clean.) Thus our central claim is that authoritative pages can be identified as belonging to dense  5 bipartite communities in the link graph of the www, via the algorithm described in the following section. A valuable feature of our techniques here is that they are robust in several respects. Although we may be dealing with query topics with up to several million relevant pages, we are arriving at quite reliable estimates of authoritative pages by examining only the few thousand pages in the base set T . This clearly has to do with the notion of \"authority\" --- we are guided by the intuition that one can find authoritative pages starting from almost any small root set S,  provided that the www contains a sufficient number of relevant pages on the query topic. Thus, the effectiveness of our technique appears not to be hampered by the phenomenal rate of growth of the www; indeed, there are indications that it produces more reliable results for search topics with greater numbers of relevant pages. We have already discussed the motivation for returning authoritative pages in response to a search. Hub pages are also valuable, since they typically provide a well-situated starting point from which a user can gather many relevant, authoritative pages on the query topic. In the context of the www, these pages often arise as hand-assembled collections of links to resources on topics of interest to the creator of the page; the distinctive properties of such pages have been observed in a number of sources. For example, Duffy and Yacovissi write  In many respects, the mere content on the Web is less significant than the way in which the network fosters this simultaneous sense of serendipitous discovery and \"connectedness\". Consider those ubiquitous hyperlinked lists of favorite sites that every individual home page impresario apparently feels compelled to assemble. And look beyond the grass-roots Webheads too, if you want. You may be surprised at how often similar -- though usually more carefully constituted -- jump-lists turn up at the Web sites of corporations as well [8]. Overview  In Section 2, we describe the basic method and show examples of its behavior for finding authoritative pages. We show that our iterative weight-assignment algorithm can be analyzed as an eigenvector computation on a pair of matrices derived from the Web graph; among other things, this allows us to prove its convergence. For many query topics, the base set T may contain multiple dense communities of hubs and authorities, which link sparsely, if at all, to one another. This may arise for several reasons: there could \"on-topic\" versus \"off-topic\" communities (e.g. a community of pages on java together with a smaller community on Caribbean vacations); the query term could have multiple meanings or uses in different settings (e.g. \"jaguar\" [5]); or the query term could refer to a polarized issue involving groups that will not link to one another (e.g. \"abortion\"). The spectral interpretation of our algorithm provides us with a natural way to discover many of these additional communities: they correspond to coordinates of large absolute value in non-principal eigenvectors. (For this reason, we will at times refer to them as non-principal communities.) We discuss this issue in Section 3. The heuristic intuition behind this approach is analogous to the spectral partitioning of undirected graphs (e.g. [7, 11, 34]); however, it is important to note that what we are doing here is not simply a spectral partitioning of the Web graph. In particular, we are studying non-principal eigenvectors of symmetric matrices derived from the (asymmetric) adjacency matrix of the base set; and the structures we find are dense directed bipartite subgraphs, rather than simply sparse partitions of  6 the node set. In Section 4, we apply our method to the problem of similar-page queries: given a page p of large in-degree, we construct a base set in the neighborhood of p and determine the good authorities. This results in an interesting notion of page similarity, defined by the link structure. Our experiments with the technique have been directed primarily at trying to understand the nature and quality of the output that is produced, and the extent to which it corresponds to human judgment. The difficulty in assessing the results of our algorithm is clear. We are attempting to define a new measure, in a domain that is itself quite new. In evaluating output that requires the judgment of a user, one typically makes use of human-annotated benchmarks; unfortunately, such benchmarks are not directly available in the present setting. Given this, we adopt the following multi-pronged approach to evaluating the output. First, we claim that an element of res ipsa loquitur applies: we feel that many of our results are quite striking at a fairly obvious level, and for a variety of reasons would be hard to produce using standard search methods currently available on the www. We will be presenting a number of such examples in the following sections. Second, there is a sense in which we can compare the technique to existing human-constructed benchmarks: there are a number of searchable hierarchies on the www, such as yahoo[38], Galaxy [14], Zia [39], and the distributed www Virtual Library [35]. These hierarchies provide lists of authoritative pages, compiled by human moderators, for many standard search topics. In this way, they represent high-quality hub pages, and can be used for comparison with our automated method. In Section 5.1 we compare the results of our technique to the contents of these and other hierarchies, for ten topics from yahoo's Health/Medicine directory. Third, we are interested in evaluating the extent to which the communities discovered by our method have a robust identity --- that is, the extent to which several different root sets on the same \"topic\" produce similar communities of hubs and authorities. In Section 5.2, we discuss several experiments along these lines, including a comparison of root sets produced by issuing the same query to a number of different search engines, and a comparison of root sets produced by issuing a query term in a number of different language. Finally, in Section 6, we consider the problem of determining how \"broad\" a query topic must be in order for our method to produce reliable sets of hubs and authorities. In particular, we consider the following recurring phenomenon: when the query defines a topic that is relatively specific, the principal community of hubs and authorities is often relevant to a generalization of the query topic, rather than to the initial topic itself. It is fairly clear why this should happen: our algorithm is designed to locate the \"densest\" community of hubs and authorities in the base set T , without regard to the initial query, and hence it will favor topics that have large representation in the vicinity of a root set derived from the query. We refer to this process as diffusion; the focus of the query has \"diffused\" to a generalization of the original topic. We show in Section 6 that non-principal eigenvectors can be a very effective way to produce relevant results even in the face of this phenomenon. Specifically, a community relevant to the (specific) initial query often exists and corresponds to one of the non-principal eigenvectors. We noted at the outset that our primary interest was in studying the quality of search methods for hypermedia, even at the cost of algorithmic efficiency. However, the method we discuss here ultimately turns out to be relatively efficient. In particular, the main computational step of the algorithm can be reduced to a singular value decomposition of a sparse matrix with several thousand  7 non-zeroes --- a task for which highly optimized code is available. Moreover, since we need only approximate the coordinates of large absolute value in the eigenvectors we use, we find that reliable results can be obtained by using a simple iterative algorithm with a very small number of iterations. Finally, as we have already discussed, our method produces meaningful output using only a small fragment of the www in answering each query. We will not be focusing further on the question of efficiency in this paper. (Note that since we did not have the www indexed locally while performing these experiments, the time required simply to fetch the html source of several hundred or several thousand pages, so as to construct the base set, has been a greater bottleneck. This is an issue that is largely separate from the computational requirements of our algorithm. For example, in preliminary experiments on a local corpus of two million U.S. patents [19], these problems associated with processing the documents did not arise.) Related Work on Link Structures  Methodologically, our work has connections to the area of bibliometrics [37] --- the study of written documents and their citation structure. Some related work has also been done in the hypertext research community. This work has focused predominantly on the use of citations and/or explicit hyperlinks as a means of clustering and enhancing relevance judgments. Two basic measures of document similarity to emerge from the study of bibliometrics are bibliographic coupling [20] and co-citation [32]. For two documents p and q, the former quantity is equal to the number of documents cited by both p and q, and the latter quantity is the number of documents that cite both p and q. We will see that these two quantities arise inside the analysis of our method. Shaw [30, 31] uses a combination of these measures, together with some textual measures, as part of a graph-based clustering algorithm. Documents constitute the nodes of an undirected graph whose edge weights are measures of similarity; edges are deleted in order of increasing weight, producing a hierarchical clustering. (See e.g. [25].) Schwanke and Platoff [29] discuss an interesting application of these bibliometric measures for clustering purposes in a completely different realm --- that of analyzing the relationships among modules in a large software system. There has also been work in bibliometrics on using citation counts to assess the \"impact\" of scientific journals; this is more closely related to the issue we are considering here. The classic work in this area is that of Garfield [15]; see also e.g. [16, 27]. In addition to basic differences in the algorithms themselves, a fundamental difference between our method and the classical bibliometric work lies in our explicit separation of hubs and authorities, and investigation of the equilibrium that exists between them --- as discussed above, this is a phenomenon that perhaps makes more sense in the context of hypermedia such as the www, where one finds several categories of participants, than it does in the classic bibliometric setting of scientific journals, which ostensibly serve a uniform role. At the same time, it has been observed in bibliometric studies that review journals often constitute exceptions to certain basic principles [16, 27]; it would be interesting to see whether the distinction between hubs and authorities would be useful in this regard. There has been some work on using hyperlinks for clustering and searching in the hypertext research community. Rivlin, Botafogo, and Schneiderman [3, 26] use basic graph-theoretic notions such as connectivity, as well as \"compactness\" measures based on node-to-node distances, to identify clusters in the graph of a hypertext environment. Weiss et al. [36] define similarity measures among  8 pages in a hypertext environment based on the link structure; these measures are generalizations of  co-citation and bibliographic coupling to allow for arbitrarily long chains of references. Larson [22] performs a co-citation analysis of a set of pages relevant to a sample query and generates clusters by dimension-reduction techniques. Finally, Frisse [13] describes a method that is applicable in a tree-structured environment: the relevance of a page with respect to a query is also based on the relevance of its descendants in the tree. More recently, Pirolli, Pitkow, and Rao [24] have used a combination of link topology and textual similarity to group together and categorize pages on the www. Arocena, Mendelzon, and Mihaila [1] and Spertus [33] have described frameworks for constructing www queries from a combination of term-matching and link-based predicates. Page [23] has developed a method for assigning a universal \"rank\" to each page on the www, so that subsequent user searches can be focused on highly ranked pages; the rank of a page is based on a weight-propagation algorithm that corresponds roughly to simulating a short random walk on the directed link graph of the www.  Finally, Carri`ere and Kazman [4] propose a link-based method for visualizing and ranking the results of queries returned by www search engines. Their method is essentially to (1) enlarge the set S of returned pages to include any page joined to a member of S via a link (in either direction); and then (2) rank each page p in this enlarged set according to the number of pages connected to  p via links (again, in either direction). Although the notion of augmenting search engine results to a \"one-step neighborhood\" is a basic step in our method, the algorithmic component of our work differs significantly from that of [4]. In particular, we make crucial use of the directionality of hyperlinks, including the explicit distinction of hubs and authorities; our ranking of pages is not obtained by a direct counting of neighbors in the link structure; and our framework naturally allows for the construction of multiple communities of ranked pages. 2 The Method  We first give a description of the basic algorithm. The algorithm can be run on an arbitrary set of hyperlinked pages, and we represent such a set as a directed graph G = (V; E) in the natural way:  V consists of the n pages in the environment, and a directed edge (p; q) 2 E indicates the presence of a link from p to q. In the applications discussed below, G will typically be the subgraph induced on our base set T . We associate a non-negative authority weight x p and a non-negative hub weight y p with each page p 2 V . We maintain the invariant that the weights of each type are normalized so their squares sum to 1:  P  p2V x  2  p = 1, and  P  p2V y  2  p = 1: We view the pages with larger x- and y-values  as being \"better\" authorities and hubs respectively.  The Iterative Algorithm  Recall the mutually reinforcing relationship between hubs and authorities. Numerically, it is natural to express this as follows: if p points to many pages with large x-values, then it should receive a large y-value; and if p is pointed to by many pages with large y-values, then it should receive a large x-value. This motivates the definition of two operations on the weights, which we denote by  9 page p x[p] := sum of y[q], for all q pointing to p q1 q2 q3 page p y[p] := sum of x[q], for all q pointed to by p q1 q2 q3 Figure 2: The basic operations. I and O. Given weights fx p g, fy p g, the I operation updates the x-weights as follows.  x p /  X  q:(q;p)2E  y q : The O operation updates the y-weights as follows.  y p /  X  q:(p;q)2E  x q : Thus I and O are the basic means by which hubs and authorities reinforce one another. (See Figure 2.) Now, to find the desired \"equilibrium\" values for x and y, one can apply the I and O operations in an alternating fashion, and see whether a fixed point is reached. Indeed, we can now state a version of our basic algorithm. Let z denote the vector (1; 1; 1; : : : ; 1).  Initially set x / z; y / z.  For i = 1; 2; 3; : : :  10 Apply the I operation Apply the O operation Normalize x and y  The sequence of (x; y) pairs produced converges to a limit (x    ; y    )  (see Theorem 2.1). Return (x    ; y    ) as the authority and hub weights. The basic convergence result is not difficult to prove; we develop it here. For a pair of authority /hub weight vectors (x; y), let (OI)(x; y) denote the result of applying the I operation followed by the O operation (and normalizing the vectors obtained). Let (OI)  n  (x; y) denote the result of doing this n times. Finally, as above, let z denote the vector in R  n  in which each coordinate is equal to 1, and let (x n ; y n ) = (OI)  n  (z; z):  For the proof, we need the following additional notions. For an n \\Theta n symmetric matrix M , let   1 (M);  2 (M); : : : ;  n (M) denote the eigenvalues of M (all of which are real), indexed in order of decreasing absolute value. Let ! i (M) denote the eigenvector associated with  i . For the sake of simplicity, we will make the following technical assumption about all the matrices we deal with: (y) j 1 (M)j ? j 2 (M)j. When this assumption holds, we refer to ! 1 (M) as the principal eigenvector, and all other ! i (M) as non-principal eigenvectors. When the assumption does not hold, the analysis becomes less clean, but it is not affected in any substantial way.  Theorem 2.1 The sequences x 1 ; x 2 ; x 3 ; : : : and y 1 ; y 2 ; y 3 ; : : : converge (to limits x    and y    respectively) . Proof. Write V = fp 1 ; p 2 ; : : : ; p n g, and let A denote the adjacency matrix of the graph G; the (i; j)  th  entry of A is equal to 1 if (p i ; p j ) is an edge of G, and is equal to 0 otherwise. One easily verifies that the I and O operations can be written x / A  T  y and y / Ax respectively. Thus x n  is the unit vector in the direction of (A  T  A)  n\\Gamma1  A  T  z, and y n is the unit vector in the direction of (AA  T  )  n  z.  Now, a standard result of linear algebra (see e.g. [17]) states that if M is a symmetric n \\Theta n  matrix, and v is a vector not orthogonal to the principal eigenvector ! 1 (M ), then the unit vector in the direction of M  n  v converges to ! 1 (M) as n increases without bound. Also (as a corollary), if M  has only non-negative entries, then the principal eigenvector of M has only non-negative entries. Consequently, z is not orthogonal to ! 1 (AA  T  ), and hence the sequence fy n g converges to a limit y    . Similarly, one can show that if  1 (A  T  A) 6= 0 (as dictated by Assumption (y)), then A  T  z  is not orthogonal to ! 1 (A  T  A). It follows that the sequence fx n g converges to a limit x    . The proof of Theorem 2.1 yields the following additional result (in the above notation).  Theorem 2.2 (Subject to Assumption (y).) x    is the principal eigenvector of A  T  A, and y    is the principal eigenvector of AA  T  .  11 set S set T Figure 3: Expanding the root set into a base set. As indicated above, the output of our process from the user's point of view would a pair of sets (X; Y ): the c pages with the largest x    -values and the c pages with the largest y    -values, for a small constant c. This represents the algorithm's estimate of the strongest authorities and hubs. Theorem 2.2 directly allows one to develop methods for computing x    and y    that are more efficient than the iteration described above. We have stuck to the above exposition for two reasons. First, it emphasizes the underlying motivation for our approach in terms of the reinforcing I and  O operations. Second, one does not have to run the above process of iterated I=O operations to convergence; one can instead compute weights fx p g and fy p g by starting from the \"flat\" vector z  and performing a fixed bounded number of I and O operations. In many of our experiments, even using a small number of iterations gives good results. It is interesting to note that the (i; j)  th  entry of A  T  A gives the number of pages that point to both p i and p j ; the (i; j)  th  entry of AA  T  gives the number of pages pointed to by both p i and  p j . Thus, these individual matrix entries correspond to the notions of co-citation and bibliographic coupling discussed above. Computing Hubs and Authorities  Given a user-supplied query string, our overall method for finding authoritative pages is now the following. (We require parameters k and d, which in our experiments we assign default values of 200 and 50 respectively.) (1) We supply the query string to a term-based search engine such as AltaVista; this returns a set S of k pages, which we refer to as the root set.  12 (2) We then enlarge the root set to a base set T . Recall from the introduction that T consists of all pages that belong to S, point to a page in S, or are pointed to by a page in S --- with the restriction that we allow a single page in S to bring at most d pages pointing to it into T . This latter point is crucial since a number of www pages have an in-degree in the hundreds of thousands, and we want to keep T reasonably small. (3) We define the graph G 0 to be the (directed) subgraph of the www induced on the set T . We now distinguish between two types of www links. We say that a link is transverse if it is between pages with different domain names, and intrinsic if it is between pages with the same domain name. By \"domain name,\" we mean here the first level in the url string associated with a page. Since intrinsic links are very often created simply to help users navigate the infrastructure of a site, they tend to convey much less information than transverse links about the authority of the pages they point to. Thus, we delete all intrinsic links from the graph G 0 , keeping only the edges corresponding to transverse links; this results in a graph G. (Clearly there are a variety of more sophisticated ways of handling the information provided by the  url hierarchy; this remains an interesting issue.) (4) Finally, we run the iterative algorithm described above on the graph G, obtaining sets of hubs and authorities.  "},{"aspect":"expcomparison","tweet":" 19 5 Searchable Hierarchies and Other Experiments  We have now seen the application of the basic framework for discovering authoritative pages on a query topic, finding multiple communities of thematically related pages, and finding pages similar to an initial query page. In some respects, it is easier to verify the effectiveness of the algorithm at the latter two tasks than at the first --- one can confirm, for example, that the pages listed together with www.honda.com are all automobile manufacturers; or that the two communities listed for the query (abortion) represent different sides of the issue. The notion of authority, however, remains somewhat more elusive; although one can attempt to evaluate whether the pages returned seem to be \"authoritative,\" can one find a way to allow for a more concrete evaluation of the algorithm? We claim that the searchable hierarchies available on the www provide us with one approach towards achieving this; this will be the topic of Section 5.1. We mentioned examples of such searchable hierarchies in the introduction (e.g. yahoo[38], Galaxy [14], Zia [39], and the www  Virtual Library [35]); they are lists of authoritative pages, compiled by humans, on a variety of broad search topics. Such hierarchies provide us both with externally generated lists of query topics, and with lists of authoritative pages on these topics against which to compare our results. In Section 5.2, we take up a different issue: we show some respects in which different methods of producing a root set for the same query topic lead to very similar communities of hubs and authorities.   5.1 Searchable Hierarchies  We have tested the algorithm on the topics in the yahoo directories Health/Medicine, Science/Physics,  and Entertainment/Movies/Genres, as well as portions of several others. To illustrate the comparisons one can make between the output of our method and the contents of www searchable hierarchies, we consider ten topics drawn from the yahoo Health/Medicine directory: acupuncture, anatomy, anesthesiology, audiology, cardiology, dermatology, endocrinology, epidemiology, gastroenterology, and hematology. For the sake of concreteness, we consider only the top 20 hubs and authorities in the principal community. (See Figure 4.) Although the queries were drawn explicitly from yahoo, pages from several different searchable hierarchies appeared as high-scoring hub pages for many of the topics. These included both general--purpose hierarchies which attempt to represent all topics, and specialized medical hierarchies which contain pages only for a range of medical key words. The former category contains sites such as those mentioned above; the latter category contains sites such as the MedMark index at medmark.bit.co.kr, and the MedWeb index at www.gen.emory.edu/medweb.  The table in Figure 4 provides the following information: from among the top 20 hubs and top 20 authorities in the principal community for each query term, it lists the set of pages referenced by each of yahoo, Galaxy, and Zia. Referenced pages are labeled by their rank, with the prefix `A' or `H'; a dash indicates that the hierarchy did not contain a page for the associated topic. We make the following observations about the experiment.  ffl Of the three hierarchies, the relevant page from yahoo itself was among the top 20 hub pages once (as H9 under (audiology)), the relevant Zia page was among the top 20 hub pages once (as H10 under (audiology)), and the relevant Galaxy page was among the top 20 hub pages twice (as  20 query yahoo Zia Galaxy acupuncture A3, A4, A6 A3, A6 --- --- anatomy A1, A2, A4, A18 --- --- --- --- anesthesiology A4 A4, A7, H0 --- --- audiology A0, A1, A3, A7, A14 A0, A1, A3, A7, A14 --- --- cardiology A16 A16 A3, A5, A9, A10, A12 dermatology A4 endocrinology A10, A14 epidemiology H5 H5 --- --- gastroenterology A0, A2 A2, A3 A3, H9, H12 hematology A0 A0 A0, A1, A2, A5, A7, A11, A12, A13, A15, A17, A18 Figure 4: Hubs/authorities referenced by 3 searchable hierarchies, on topics from Health/Medicine. H14 under (gastroenterology) and H10 under (hematology)).  ffl The identity of the top hub page (H0) can be summarized as follows. The www Virtual Library hierarchy contained pages for two of the topics --- anesthesiology and epidemiology. For both of these topics, it emerged as the top hub page. For five of the queries, the top hub page was the relevant page from the MedMark hierarchy at medmark.bit.co.kr. For the remaining three topics, the top hub page contained pointers to many pages relevant to the query, but did not appear to belong to a larger searchable hierarchy. Thus the top hub pages (and authorities) were relevant to the initial query for all ten topics. This can clearly be attributed in some measure to the quantity of professionally assembled hub pages for these topics.  ffl The general-purposes hierarchies yahoo, Galaxy, and Zia did not score as highly as some of the focused medical hierarchies, though they consistently referenced authorities in the top 20. The  www Virtual Library, while also a general purpose hierarchy, scored extremely well.  "}]}