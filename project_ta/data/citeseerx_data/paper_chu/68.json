{"user_name":" Efficient Algorithms for Discovering Association Rules ","user_timeline":[{"aspect":"abstract","tweet":" Abstract  Association rules are statements of the form \"for 90 % of the rows of the relation, if the row has value 1 in the columns in set W , then it has 1 also in column B\".  Agrawal, Imielinski, and Swami introduced the problem of mining association rules from large collections of data, and gave a method based on successive passes over the database. We give an improved algorithm for the problem. The method is based on careful combinatorial analysis of the information obtained in previous passes; this makes it possible to eliminate unnecessary candidate rules. Experiments on a university course enrollment database indicate that the method outperforms the previous one by a factor of 5. We also show that sampling is in general a very efficient way of finding such rules. Keywords: association rules, covering sets, algorithms, sampling.  "},{"aspect":"expanalysis","tweet":"  6 Concluding remarks  Association rules are a simple and natural class of database regularities, useful in various analysis or prediction tasks. We have considered the problem of finding the association rules that hold in a given relation. Following the work of [1], we have given an algorithm that uses all existing information between database passes to avoid checking the coverage of redundant sets. The algorithm gives clear empirical improvement when compared against the previous results, and it is simple to implement. See also [2] for similar results. The algorithm can be extended to handle nonbinary attributes by introducing new 189  indicator variables and using their special properties in the candidate generation process. We have also analyzed the theoretical properties of the problem of finding association rules. We showed that sampling is an efficient technique for finding rules of this type, and that algorithms working in main memory can give extremely good approximations. In Appendix A we give some additional theoretical results. We also give a simple lower bound for a special case of the problem, and note that an algorithm from the different framework of [7] actually matches this bound. We have considered finding association rules from sequential data in [8]. Several problems remain open. Some of the pruning ideas in [1] are probably quite useful in certain situations; recognizing when to use such methods would help in practice. An algorithmic problem is how to find out as efficiently as possible what candidate sets occur in a given database row. Currently we simply check each candidate separately, i.e., if AB and AC are two candidates, the A entry of each row is checked twice. On certain stages of the search the candidates are heavily overlapping, and it could be useful to utilize this information. A general problem in data mining is how to choose the interesting rules among the large collection of all rules. The use of support and confidence thresholds is one way of pruning uninteresting rules, but some other methods are still needed. In the course enrollment database many of the discovered rules correspond to normal process in the studies. This could be eliminated by considering a partial ordering among courses and by saying that a rule W ) B is not interesting if B is greater than all elements of W with respect to this ordering.  "},{"aspect":"expdata","tweet":" 5.1 Data  We have used two datasets to evaluate the algorithms. The first is a course enrollment database, including registration information of 4734 students (one tuple per student). Each row contains the courses that a student has registered for, with a total of 127 possible courses. On the average, each row contains 4 courses. A simplified version of the database includes only the students with at least 2 courses (to be interesting for generating rules); this database consists of 2836 tuples, with an average of 6.5 items per tuple. The figures and tables in this paper represent this latter course enrollment data. The second database is a telephone company fault management database, containing some 30,000 records of switching network notifications. The total number of attributes is 210. The database is basically a string of events, and we map it to relational form by considering it in terms of overlapping windows. The experiments on this data support the conclusions achieved with the course database. The database sizes we have used are representative for sampling which would result in very good approximations, as was concluded in Section 4. 186  Size Support oe  0.40 0.20 0.18 0.16 0.14 0.12 0.10 0.08 1 2 11 13 14 14 14 16 18 2 10 17 26 35 53 68 79 3 4 5 12 22 52 102 192 4 1 1 1 5 19 69 171 5 1 19 76 6 1 29 7 3  P  2 26 36 53 76 139 275 568 Table 1: Number of covering sets. Support oe  0.40 0.20 0.18 0.16 0.14 0.12 0.10 0.08 Count 0 26 30 48 81 196 544 1426 Max size 0 4 4 4 4 5 6 7 Table 2: Number and maximal size of rules (fl = 0:7).  "},{"aspect":"background","tweet":"  1 Introduction  Data mining (database mining, knowledge discovery in databases) has recently been recognized as a promising new field in the intersection of databases, artificial intelligence, and machine learning (see, e.g., [11]). The area can be loosely defined as finding interesting rules or exceptions from large collections of data. Recently, Agrawal, Imielinski, and Swami introduced a class of regularities, association rules, and gave an algorithm for finding such rules from a database with binary data [1]. An association rule is an expression W ) B, where W is a set of attributes and B a single attribute. The intuitive meaning of such a rule is that in the rows of the database where the attributes in W have value true, also the attribute B tends to have value true. For instance, a rule might state that students taking courses CS101 and CS120, often also take the course CS130. This sort of information can be used, e.g., in assigning classrooms for the courses. Applications of association rules include customer behavior    On leave from Nokia Research Center. 181  analysis for example in a supermarket or banking environment, and telecommunications alarm diagnosis and prediction. In this paper we study the properties of association rule discovery in relations. We give a new algorithm for the problem that outperforms the method in [1] by a factor of 5. The algorithm is based on the same basic idea of repeated passes over the database as the method in [1]. The difference is that our algorithm makes careful use of the combinatorial information obtained from previous passes and in this way avoids considering many unnecessary sets in the process of finding the association rules. Our experimental data consists of two databases, namely university course enrollment data and the fault management database of a switching network. The empirical results show a good, solid performance for our method. A same type of improvement has independently been suggested in [2]. We also study the theoretical properties of the problem of finding the association rules that hold in a relation. We give a probabilistic analysis of two aspects of the problem, showing that sampling is an efficient way of finding association rules, and that in random relations almost all association rules are small. We also give a simple information-theoretic lower bound for finding one rule, and show that an algorithm suggested by Loveland in [7] in a different framework actually meets this lower bound. The rest of this paper is organized as follows. Section 2 introduces the problem and the notations. Section 3 describes our algorithm for finding association rules. The analysis of sampling is given in Section 4. Empirical results and a comparison to the results of [1] are given in Section 5. Section 6 is a short conclusion. Appendix A contains the probabilistic analyses of random relations and the lower bound result. Appendix B gives an overview of the implementation. We refer to [1] for references about related work.  "},{"aspect":"expintro","tweet":" 5 Experiments  To evaluate the efficiency of our methods, we compare the original algorithm in [1] to our algorithm. Candidate generation is performed by extending sets in L s with other sets in L s to achieve (at most) e-extensions. We compare a less aggressive extending strategy with e = 1 and a more aggressive strategy with e = s, where the size of the candidate sets is doubled during each iteration step. (We refer to our algorithm as offline candidate determination; the variants are noted in the following as OCD 1 and OCD s .) In addition to the original algorithm of [1] (noted in the following by AIS orig ), we also implemented a minor modification of it that refrains from extending any set with an item that is not a covering set by itself (noted in the following by AIS mod ). Details about the implementations can be found in Appendix B. "},{"aspect":"problemdef","tweet":"  2 Problem  First we introduce some basic concepts, using the formalism presented in [1]. Let R =  fI 1 ; I 2 ; : : : ; I mg be a set of attributes, also called items, over the binary domain f0; 1g. The input r = ft 1 ; : : : ; t n g for the data mining method is a relation over the relation schema fI 1 ; I 2 ; : : : ; I mg, i.e., a set of binary vectors of size m. Each row can be considered as a set of properties or items (that is, t[i] = 1 , I i 2 t).  Let W ` R be a set of attributes and t 2 r a row of the relation. If t[A] = 1 for all  A 2 W , we write t[W ] =  1. An association rule over r is an expression W ) B, where  W ` R and B 2 R n W . Given real numbers fl (confidence threshold) and oe (support threshold), we say that r satisfies W ) B with respect to fl and oe, if  jfi j t i [WB] =  1gj  oen  and  jfi j t i [WB] =  1gj  jfi j t i [W ] =  1gj   fl:  That is, at least a fraction oe of the rows of r have 1's in all the attributes of WB, and at least a fraction fl of the rows having a 1 in all attributes of W also have a 1 in B. Given 182  a set of attributes X, we say that X is covering  1  (with respect to the database and the given support threshold oe), if  jfi j t i [X] =  1gj  oen:  That is, at least a fraction oe of the rows in the relation have 1's in all the attributes of  X.  As an example, suppose support threshold oe = 0:3 and confidence threshold fl = 0:9, and consider the example database  ABCD;ABEFG;ABHIJ;BK:  Now, three of four rows contain the set fABg, so the support is jfi j t i [AB] =  1gj=4 = 0:75; supports of A, B, and C are 0.75, 1, and 0.25, correspondingly. Thus, fAg, fBg, and  fABg have supports larger than the threshold oe = 0:3 and are covering, but fCg is not. Further on, the database satisfies fAg ) B, as fABg is covering, and the confidence  0:75 0:75  is larger than fl = 0:9. The database does not satisfy fBg ) A because the confidence  0:75 1 is less than the threshold fl.  The coverage is monotone with respect to contraction of the set: if X is covering and  B ` X, then t i [B] =  1 for any i 2 fi j t i [X] =  1g, and therefore B is also covering. On the other hand, association rules do not have monotonicity properties with respect to expansion or contraction of the left-hand side: if W ) B holds, then WA ) B does not necessarily hold, and if WA ) B holds, then W ) B does not necessarily hold. In the first case the rule WA ) B does not necessarily have sufficient support, and in the second case the rule W ) B does not necessarily hold with sufficient confidence.  "},{"aspect":"solution","tweet":"  3 Finding association rules  3.1 Basic algorithm  The approach in [1] to finding association rules is to first find all covering attribute sets X, and then separately test whether the rule X n fBg ) B holds with sufficient confidence.  2  We follow this approach and concentrate on the algorithms that search for covering subsets. To know if a subset X ` R is not covering, one has to read at least a fraction 1 \\Gamma oe of the rows of the relation, that is, for small values of support threshold oe almost all of the relation has to be considered. During one pass through the database we can, of course, check for several subsets whether they are covering or not. If the database is large, it is important to make as few passes over the data as possible. The extreme method would be to do just one pass and check for each of the 2  m  subsets of R whether they are covering or not. This is infeasible for all but the smallest values of m.  1  Agrawal et al. use the term large [1].  2  It is easy to see that this approach is in a sense optimal: the problem of finding all covering subsets of R can be reduced to the problem of finding all association rules that hold with a given confidence. Namely, if we are given a relation r, we can find the covering sets by adding an extra column B with all 1's to r and then finding the association rules that have B on the right-hand side and hold with certainty 1. 183  The method of [1] makes multiple passes over the database. During a database pass, new candidates for covering sets are generated, and support information is collected to evaluate which of the candidates actually are covering. The candidates are derived from the database tuples by extending previously found covering sets in the frontier. For each database pass, the frontier consists of those covering sets that have not yet been extended. Each set in the frontier is extended up to the point where the extension is no longer expected to be covering. If such a candidate unexpectedly turns out to be covering, it is included in the frontier for the next database pass. The expected support required for this decision is derived from the frequency information of the items of the set. Originally the frontier contains only the empty set. An essential property of the method of [1] is that both candidate generation and evaluation are performed during the database pass. The method of [1] further uses two techniques to prune the candidate space during the database pass. These are briefly described in Appendix B. We take a slightly different approach. Our method tries to use all available information from previous passes to prune candidate sets between the database passes; the passes are kept as simple as possible. The method is as follows. We produce a set L s as the collection of all covering sets of size s. The collection C s+1 will contain the candidates for L s+1 : those sets of size s + 1 that can possibly be in L s+1 , given the covering sets of L s . 1. C 1 := ffAg j A 2 Rg;  2. s := 1; 3. while C s 6= ; do  4. database pass: let L s be the elements of C s that are covering; 5. candidate generation: compute C s+1 from L s ; 6. s := s + 1; 7. od; The implementation of the database pass on line 4 is simple: one just uses a counter for each element of C s . In candidate generation we have to compute a collection C s+1  that is certain to include all possible elements of L s+1 , but which does not contain any unnecessary elements. The crucial observation is the following. Recall that L s denotes the collection of all covering subsets X of R with jXj = s. If Y 2 L s+e and e  0, then  Y includes  `  s + e s  '  sets from L s . This claim follows immediately from the fact that all subsets of a covering set are covering. The same observation has been made independently in [2]. Despite its triviality, this observation is powerful. For example, if we know that  L 2 = fAB;BC;AC;AE;BE;AF;CGg; we can conclude that ABC and ABE are the only possible members of L 3 , since they are the only sets of size 3 whose all subsets of size 2 are included in L 2 . This further means that L 4 must be empty. In particular, if X 2 L s+1 , then X must contain s+1 sets from L s . Thus a specification for the computation of C s+1 is to take all sets with this property:  C s+1 = fX ` R j jXj = s + 1 and X includes s + 1 members of L s g (1) 184  This is, in fact, the smallest possible candidate collection C s+1 in general. For any  L s , there are relations such that the collection of covering sets of size s is L s , and the collection of coverings sets of size s + 1 is C s+1 , as specified above.  3  The computation of the collection C s+1 so that (1) holds is an interesting combinatorial problem. A trivial solution would be to inspect all subsets of size s + 1, but this is obviously wasteful. One possibility is the following. First compute a collection C  0  s+1 by forming pairwise unions of such covering sets of size s that have all but one attribute in common:  C  0  s+1 = fX [ X  0  j X;X  0  2 L s ; jX \" X  0  j = s \\Gamma 1g: Then C s+1 ` C  0  s+1 , and C s+1 can be computed by checking for each set in C  0  s+1 whether the defining condition of C s+1 holds. The time complexity is  O(sjL s j  2  + jC  0  s+1 jsjL s j);  further, jC  0  s+1 j = O(jL s j  2  ), but this bound is very rough. An alternative method is to form unions of sets from L s and L 1 :  C  00  s+1 = fX [ X  0  j X 2 L s ; X  0  2 L 1 ; X  0  6` Xg;  and then compute C s+1 by checking the inclusion condition. (Note that the work done in generating C s+1 does not depend on the size of the database, but only on the size of the collection L s .) Instead of computing C s+1 from L s , one can compute several families  C s+1 ; : : : ; C s+e for some e ? 1 directly from L s . The computational complexity of the algorithms can be analyzed in terms of the quantities jL s j, jC s j, jC  0  s j, and the size n of the database. The running time is linear in  n and exponential in the size of the largest covering set. For reasons of space we omit here the more detailed analysis. The database passes dominate the running time of the methods, and for very large values of n the algorithms can be quite slow. However, in the next section we shall see that by analyzing only small samples of the database we obtain a good approximation of the covering sets. 4 Analysis of sampling  We now consider the use of sampling in finding covering sets. We show that small samples are usually quite good for finding covering sets. Let  be the support of a given set X of attributes. Consider a random sample with replacement of size h from the relation. Then the number of rows in the sample that contain X is a random variable x distributed according to B(h;  ), i.e., binomial distribution of h trials, each having success probability  . The Chernoff bounds [3, 6] state that for all a we have  P r[x ? h + a] ! e  \\Gamma2a  2  =h  :  3  Results on the possible relative sizes of L s and C s+1 can be found in [4]. 185  That is, the probability that the estimated support is off by at least ff is  P r[x ? h( + ff)] ! e  \\Gamma2ff  2  h  2  =h  = e  \\Gamma2ff  2  h  ;  i.e., bounded by a quantity exponential in h. For example, if ff = 0:02, then for h = 3000 the probability is e  \\Gamma2:4   0:09. (Similar results could be even more easily obtained by using the standard normal approximation.) This means that sampling is a powerful way of finding association rules. Even for fairly low values of support threshold oe, a sample consisting of 3000 rows gives an extremely good approximation of the coverage of an attribute set. Therefore algorithms working in main memory are quite useful for the problem of finding association rules. Appendix A contains an analysis of covering sets in random relations and a lower bound result for the problem of finding association rules.  "},{"aspect":"expcomparison","tweet":"  5.2 Results  Each algorithm finds, of course, the same covering sets and the same rules. The number of covering sets found with different support thresholds is presented in Table 1. Correspondingly, the number of association rules is presented in Table 2; we used a confidence threshold fl of 0.7 during all the experiments. The tables show that the number of covering sets (and rules) increases very fast with a decreasing support threshold. Figure 1a presents the total time (in seconds) as a function of the inverse of the support threshold oe; we prefer to use the inverse of oe since it corresponds to the common sense idea of the \"rarity\" of the items. Figure 1a shows clearly that OCD is much more time efficient than AIS. The time requirement of OCD is typically 10--20 % of the time requirement of AIS, and the advantage of OCD increases as we lower oe. However, the difference between the algorithms is notable even with a large oe. The difference between the two variants of OCD is not large, and the modification we implemented in AIS did not affect its performance significantly. When we look at the total time as a function of jLj, the number of covering sets (presented in Figure 1b), we observe that both algorithms behave more or less linearly with respect to jLj, but the time requirements of OCD increase much more slowly. As an abstract measure of the amount of database processing we examine the effective volume of the candidates, denoted by V eff . It is the total volume of candidates that are evaluated, weighted by the number of database tuples that must be examined to evaluate them. This measure is representative of the amount of processing needed during the database passes, independent of implementation details. 187  0 1000 2000 3000 4000 5000 6000 2 4 6 8 10 12 14 Time Inverse of support oe  (a)  AIS OCD 1  OCD s q q q q q q q q q q q q q q q q a a a a a a a a a a a a a a a a 0 1000 2000 3000 4000 5000 6000 0 100 200 300 400 500 600 Time Number of covering sets (b)  AIS OCD  q q q q q q q q q q q q q q q q a a a a a a a a a a a a a a a a Figure 1: Total time in seconds (a) as a function of the inverse of support and (b) as a function of the number of covering sets. 0 1000 2000 3000 4000 5000 6000 2 4 6 8 10 12 14  V eff Inverse of support oe  (a)  AIS OCD 1  OCD s q q q q q q q q q q q q q q q q a a a a a a a a a a a a a a a a 0 1000 2000 3000 4000 5000 6000 0 100 200 300 400 500 600  V eff Number of covering sets (b)  AIS OCD 1  OCD s q q q q q q q q q q q q q q q q a a a a a a a a a a a a a a a a Figure 2: Effective volume of candidates (a) as a function of the inverse of support and (b) as a function of the number of covering sets. The principal reason for the performance difference of AIS and OCD can be seen in Figures 2a and b. They present the behavior of V eff as the support threshold oe decreases and jLj, the number of covering sets, increases. The number of candidate sets (and their volume) considered by OCD is always smaller than that of AIS and the difference increases as more sets are found. These figures also explain why the more aggressively extending variant OCD s does not perform better than the basic OCD 1 : even though a more aggressive strategy can reduce the number of passes, it also results in so many more candidates that the reduction in time is offset. Table 3 presents the number of candidates considered by each method. The numbers for AIS are much higher, as it may generate the same candidates over and over again during the database pass. On the other hand, OCD only generates any candidate once (during the generation time) and checks that its subsets are covering before evaluating it against the database. While the number of potential candidates generated by OCD is much smaller than that for AIS, still fewer candidates need to be evaluated during the database pass. If sampling is not used or the samples are large, the data does not remain in the 188  Support oe  0.40 0.20 0.18 0.16 0.14 0.12 0.10 0.08  OCD 1 128 196 242 289 362 552 950 1756  OCD s 128 217 300 434 625 1084 1799 4137  AIS orig 8517 37320 42175 48304 52415 65199 95537 118369  AIS mod 9106 38068 42983 48708 53359 66704 96992 120749 Table 3: Generated candidates. 0 200 400 600 800 1000 0 1 2 3 4 5 6  V eff Iteration pass  AIS OCD 1 q q q q q q q q q q q q q q Figure 3: Effective volume of candidates during each iteration pass. main memory between passes, but it has to be reloaded for each pass. Thus it would be important to minimize the number of data passes. Also, if we want to overlap the database reads and the processing, the amount of processing performed during each database pass should be small. Figure 3 presents a typical profile of V eff during the passes of one run (with oe = 0:1). While the area beneath the curve corresponds to the total volume, the height of the curve at each point describes how much processing is needed during that pass. High peaks correspond to passes where the overlapping of I/O and processing may be endangered if the database is large. Since the confidence threshold fl affects only the number of rules generated from the covering sets, we have not varied it in our experiments. On the other hand, suitable values for the support threshold oe depend very much on the database.  "}]}