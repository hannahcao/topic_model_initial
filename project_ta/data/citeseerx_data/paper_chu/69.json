{"user_name":" To appear in MLJ 34(1-3), February 1999  Similarity-Based Models of Word Cooccurrence Probabilities ","user_timeline":[{"aspect":"abstract","tweet":" Abstract  In many applications of natural language processing (NLP) it is necessary to determine the likelihood of a given word combination. For example, a speech recognizer may need to determine which of the two word combinations \"eat a peach\" and \"eat a beach\" is more likely. Statistical NLP methods determine the likelihood of a word combination from its frequency in a training corpus. However, the nature of language is such that many word combinations are infrequent and do not occur in any given corpus. In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on \"most similar\" words. We describe probabilistic word association models based on distributional word similarity, and apply them to two tasks, language modeling and pseudo-word disambiguation. In the language modeling task, a similarity-based model is used to improve probability estimates for unseen bigrams in a back-off language model. The similarity-based method yields a 20% perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition error. We also compare four similarity-based estimation methods against back-off and maximumlikelihood estimation methods on a pseudo-word sense disambiguation task in which we controlled for both unigram and bigram frequency to avoid giving too much weight to easy-todisambiguate high-frequency configurations. The similarity-based methods perform up to 40% better on this particular task.  "},{"aspect":"expanalysis","tweet":" For perplexity evaluation, we tuned the similarity model parameters by minimizing perplexity on an additional sample of 57.5 thousand words of WSJ text, drawn from the ARPA HLT development  4  The ARPA WSJ development corpora come in two versions, one with verbalized punctuation and the other without. We used the latter in all our experiments.  k t fi fl training reduction (%) test reduction (%) 60 2.5 4.0 0.15 18.4 20.51 50 2.5 4.0 0.15 18.38 20.45 40 2.5 4.0 0.2 18.34 20.03 30 2.5 4.0 0.25 18.33 19.76 70 2.5 4.0 0.1 18.3 20.53 80 2.5 4.5 0.1 18.25 20.55 100 2.5 4.5 0.1 18.23 20.54 90 2.5 4.5 0.1 18.23 20.59 20 1.5 4.0 0.3 18.04 18.7 10 1.5 3.5 0.3 16.64 16.94 Table 2: Perplexity Reduction on Unseen Bigrams for Different Model Parameters test set. The best parameter values found were k = 60, t = 2:5, fi = 4 and fl = 0:15. For these values, the improvement in perplexity for unseen bigrams in a held-out 18 thousand word sample (the ARPA HLT evaluation test set) is just over 20%. Since unseen bigrams comprise 10.6% of this sample, the improvement on unseen bigrams corresponds to an overall test set perplexity improvement of 2.4% (from 237.4 to 231.7). Table 2 shows reductions in training and test perplexity, sorted by training reduction, for different choices of the number k of closest neighbors used. The values of fi, fl and t are the best ones found for each k.  5  From equation (9), it is clear that the computational cost of applying the similarity model to an unseen bigram is O(k). Therefore, lower values for k (and t) are computationally preferable. From the table, we can see that reducing k to 30 incurs a penalty of less than 1% in the perplexity improvement, so relatively low values of k appear to be sufficient to achieve most of the benefit of the similarity model. As the table also shows, the best value of fl increases as k decreases; that is, for lower k, a greater weight is given to the conditioned word's frequency. This suggests that the predictive power of neighbors beyond the closest 30 or so can be modeled fairly well by the overall frequency of the conditioned word. The bigram similarity model was also tested as a language model in speech recognition. The test data for this experiment were pruned word lattices for 403 WSJ closed-vocabulary test sentences. Arc scores in these lattices are sums of an acoustic score (negative log likelihood) and a language model score, which in this case was the negative log probability provided by the baseline bigram model. From the given lattices, we constructed new lattices in which the arc scores were modified to use the similarity model instead of the baseline model. We compared the best sentence hypothesis in each original lattice with the best hypothesis in the modified one, and counted the word disagreements in which one of the hypotheses was correct. There were a total of 96 such disagreements; the similarity model was correct in 64 cases, and the back-off model in 32. This advantage for the similarity model is statistically significant at the 0.01 level. The overall reduction in error rate is small, from 21.4% to 20.9%, because the number of disagreements is small compared with the overall number of errors in the recognition setup employed in these experiments. Table 3 shows some examples of speech recognition disagreements between the two models. The hypotheses are labeled `B' for back-off and `S' for similarity, and the bold-face words are errors. The similarity model seems to be better at modeling regularities such as semantic parallelism in  5  Values of fi and t refer to base 10 logarithms and exponentials in all calculations.  B commitments . . . from leaders felt the three point six billion dollars S commitments . . . from leaders fell to three point six billion dollars B followed by France the US agreed in Italy S followed by France the US Greece . . . Italy B he whispers to made a  S he whispers to an aide B the necessity for change exist  S the necessity for change exists B without . . . additional reserves Centrust would have reported S without . . . additional reserves of Centrust would have reported B in the darkness past the church S in the darkness passed the church Table 3: Speech Recognition Disagreements between Models lists and avoiding a past tense form after \"to.\" On the other hand, the similarity model makes several mistakes in which a function word is inserted in a place where punctuation would be found in written text.  "},{"aspect":"expdata","tweet":" We evaluated the above model by comparing its test-set perplexity and effect on speech recognition accuracy with the baseline bigram back-off model developed by MIT Lincoln Laboratories for the Wall Street Journal (WSJ) text and dictation corpora provided by ARPA's HLT program (Paul, 1991).  4  The baseline back-off model follows the Katz design, except that, for the sake of compactness, all frequency one bigrams are ignored. The counts used in this model and in ours were obtained from 40.5 million words of WSJ text from the years 1987-89. "},{"aspect":"background","tweet":" 1 Introduction  Data sparseness is an inherent problem in statistical methods for natural language processing. Such methods use statistics on the relative frequencies of configurations of elements in a training corpus to learn how to evaluate alternative analyses or interpretations of new samples of text or speech. The most likely analysis will be taken to be the one that contains the most frequent configurations. The problem of data sparseness, also known as the zero-frequency problem (Witten & Bell, 1991), arises when analyses contain configurations that never occurred in the training corpus. Then it is not possible to estimate probabilities from observed frequencies, and some other estimation scheme that can generalize from the training data has to be used.  In language processing applications, the sparse data problem occurs even for very large data sets. For example, Essen and Steinbiss (1992) report that in a 75%-25% split of the million-word LOB corpus, 12% of the bigrams in the test partition did not occur in the training portion. For trigrams, the sparse data problem is even more severe: for instance, researchers at IBM (Brown, DellaPietra, deSouza, Lai, & Mercer, 1992) examined a training corpus consisting of almost 366 million English words, and discovered that one can expect 14.7% of the word triples in any new English text to be absent from the training sample. Thus, estimating the probability of unseen configurations is crucial to accurate language modeling, since the aggregate probability of these unseen events can be significant. We focus here on a particular kind of configuration, word cooccurrence. Examples of such cooccurrences include relationships between head words in syntactic constructions (verb-object or adjective-noun, for instance) and word sequences (n-grams). In commonly used models, the probability estimate for a previously unseen cooccurrence is a function of the probability estimates for the words in the cooccurrence. For example, in word bigram models, the probability P (w 2 jw 1 ) of a conditioned word w 2 that has never occurred in training following the conditioning word w 1  is typically calculated from the probability of w 2 , as estimated by w 2 's frequency in the corpus (Jelinek, Mercer, & Roukos, 1992; Katz, 1987). This method makes an independence assumption on the cooccurrence of w 1 and w 2 : the more frequent w 2 is, the higher the estimate of P (w 2 jw 1 ) will be, regardless of w 1 . Class-based and similarity-based models provide an alternative to the independence assumption. In these models, the relationship between given words is modeled by analogy with other words that are in some sense similar to the given ones. For instance, Brown et al. (1992) suggest a class-based n-gram model in which words with similar cooccurrence distributions are clustered into word classes. The cooccurrence probability of a given pair of words is then estimated according to an averaged cooccurrence probability of the two corresponding classes. Pereira, Tishby, and Lee (1993) propose a \"soft\" distributional clustering scheme for certain grammatical cooccurrences in which membership of a word in a class is probabilistic. Cooccurrence probabilities of words are then modeled by averaged cooccurrence probabilities of word clusters. Dagan, Marcus, and Markovitch (1993, 1995) present a similarity-based model, which avoids building clusters. Instead, each word is modeled by its own specific class, a set of words that are most similar to it. Using this scheme, they predict which unobserved cooccurrences are more likely than others. Their model, however, does not provide probability estimates and so cannot be used as a component of a larger probabilistic model, as would be required in, say, speech recognition. Class-based and similarity-based methods for cooccurrence modeling may at first sight seem to be special cases of clustering and weighted nearest-neighbor approaches used widely in machine learning and pattern recognition (Aha, Kibler, & Albert, 1991; Cover & Hart, 1967; Duda & Hart, 1973; Stanfill & Waltz, 1986; Devroye, Gyorfi, & Lugosi, 1996; Atkeson, Moore, & Schaal, 1997). There are important differences between those methods and ours. Clustering and nearest-neighbor techniques often rely on representing objects as points in a multidimensional space with coordinates determined by the values of intrinsic object features. However, in most language-modeling settings, all we know about a word are the frequencies of its cooccurrences with other words in certain configurations. Since the purpose of modeling is to estimate the probabilities of cooccurrences, the same cooccurrence statistics are the basis for both the similarity measure and the model predictions. That is, the only means we have for measuring word similarity are the predictions words make about what words they cooccur with, whereas in typical instance or (non-distributional) clustering learning methods, word similarity is defined from intrinsic features independently of the predictions  (cooccurrence probabilities or classifications) associated with particular words (see for instance the work of Cardie (1993), Ng and Lee (1996), Ng (1997), and Zavrel and Daelemans (1997)).  1.1 Main Contributions  Our main contributions are a general scheme for using word similarity to improve the probability estimates of back-off models, and a comparative analysis of several similarity measures and parameter settings in two important language processing tasks, language modeling and disambiguation, showing that similarity-based estimates are indeed useful. In our initial study, a language-model evaluation, we used a similarity-based model to estimate unseen bigram probabilities for Wall Street Journal text and compared it to a standard back-off model (Katz, 1987). Testing on a held-out sample, the similarity model achieved a 20% perplexity reduction over back-off for unseen bigrams. These constituted 10.6% of the test sample, leading to an overall reduction in test-set perplexity of 2.4%. The similarity-based model was also tested in a speech-recognition task, where it yielded a statistically significant reduction (32 versus 64 mistakes in cases where there was disagreement with the back-off model) in recognition error. In the disambiguation evaluation, we compared several variants of our initial method and the  cooccurrence smoothing method of Essen and Steinbiss (1992) against the estimation method of Katz in a decision task involving unseen pairs of direct objects and verbs. We found that all the similaritybased models performed almost 40% better than back-off, which yielded about 49% accuracy in our experimental setting. Furthermore, a scheme based on the Jensen-Shannon divergence (Rao, 1982; Lin, 1991)  1  yielded statistically significant improvement in error rate over cooccurrence smoothing. We also investigated the effect of removing extremely low-frequency events from the training set. We found that, in contrast to back-off smoothing, where such events are often discarded from training with little discernible effect, similarity-based smoothing methods suffer noticeable performance degradation when singletons (events that occur exactly once) are omitted. The paper is organized as follows. Section 2 describes the general similarity-based framework; in particular, Section 2.3 presents the functions we use as measures of similarity. Section 3 details our initial language modeling experiments. Section 4 describes our comparison experiments on a pseudo-word disambiguation task. Section 5 discusses related work. Finally, Section 6 summarizes our contributions and outlines future directions.  "},{"aspect":"expintro","tweet":" 3 Language Modeling  The goal of our first set of experiments, described in this section, was to provide proof of concept by showing that similarity-based models can achieve better language modeling performance than backoff. We therefore only used one similarity measure. The success of these experiments convinced  3  Actually, they present two alternative definitions. We use their model 2-B, which they found yielded the best experimental results.  name range base LM constraints tune?  D [0; 1] P (w 2 jw  0  1 ) 6= 0 if P (w 2 jw 1 ) 6= 0 yes  J [0; log 2] none yes  L [0; 2] none yes  PC [0;  1 2 maxw 2 P (w 2 )] Bayes consistency no Table 1: Summary of Similarity Function Properties us that similarity-based methods are worth examining more closely; the results of our second set of experiments, comparing several similarity functions on a pseudo-word disambiguation task, are described in the next section. Our language modeling experiments used a similarity-based model, with the KL divergence as (dis)similarity measure, as an alternative to unigram frequency when backing off in a bigram model. That is, we used the bigram language model defined by:  P (w 2 jw 1 ) =  (  P d (w 2 jw 1 ) c(w 1 ; w 2 ) ? 0  ff(w 1 )P r (w 2 jw 1 ) c(w 1 ; w 2 ) = 0  P r (w 2 jw 1 ) = flP (w 2 ) + (1 \\Gamma fl)P SIM (w 2 jw 1 )  P SIM (w 2 jw 1 ) =  X  w  0  1 2S(w 1 )  W (w 1 ; w  0  1 ) norm(w 1 )  P (w 2 jw  0  1 ) (9) W (w 1 ; w  0  1 ) = 10  \\Gammafi D(w 1 jjw  0  1 )  ;  where V 1 = V 2 = V , the entire vocabulary. As noted earlier, the estimates of P (w 2 jw  0  1 ) must be smoothed to avoid division by zero when computing D(w 1 jjw  0  1 ); we employed the standard Katz bigram back-off model for that purpose. Since jV j = 20; 000 in this application, we considered only a small fraction of V in computing P SIM , using the tunable thresholds k and t described in Section 2.2 for this purpose. The standard evaluation metric for language models is the likelihood of the test data according to the model, or, more intuitively, the test-set perplexity N  v u u t  N  Y  i=1  P (w i jw i\\Gamma1 )  \\Gamma1  ; which represents the average number of alternatives presented by the (bigram) model after each test word. Thus, a better model will have a lower perplexity. In our task, lower perplexity will indicate better prediction of unseen bigrams. "},{"aspect":"problemdef","tweet":""},{"aspect":"solution","tweet":"  2 Distributional Similarity Models  We wish to model conditional probability distributions arising from the cooccurrence of linguistic objects, typically words, in certain configurations. We thus consider pairs (w 1 ; w 2 ) 2 V 1 \\Theta V 2  for appropriate sets V 1 and V 2 , not necessarily disjoint. In what follows, we use subscript i for the i  th  element of a pair; thus P (w 2 jw 1 ) is the conditional probability (or rather, some empirical estimate drawn from a base language model, the true probability being unknown) that a pair has second element w 2 given that its first element is w 1 ; and P (w 1 jw 2 ) denotes the probability estimate, according to the base language model, that w 1 is the first word of a pair given that the second word is w 2 . P (w) denotes the base estimate for the unigram probability of word w.  1  To the best of our knowledge, this is the first use of this particular distribution dissimilarity function in statistical language processing. The function itself is implicit in earlier work on distributional clustering (Pereira et al., 1993) and has been used by Tishby (p.c.) in other distributional similarity work. Finch (1993) discusses its use in word clustering, but does not provide an experimental evaluation on actual data.  A similarity-based language model consists of three parts: a scheme for deciding which word pairs require a similarity-based estimate, a method for combining information from similar words, and, of course, a function measuring the similarity between words. We give the details of each of these three parts in the following three sections. We will only be concerned with similarity between words in V 1 , which are the conditioning events for the probabilities P (w 2 jw 1 ) that we want to estimate.  2.1 Discounting and Redistribution  Data sparseness makes the maximum likelihood estimate (MLE) for word pair probabilities unreliable. The MLE for the probability of a word pair (w 1 ; w 2 ), conditional on the appearance of word  w 1 , is simply  PML (w 2 jw 1 ) =  c(w 1 ; w 2 )  c(w 1 )  ; (1) where c(w 1 ; w 2 ) is the frequency of (w 1 ; w 2 ) in the training corpus and c(w 1 ) is the frequency of  w 1 . However, PML is zero for any unseen word pair, that is, any such pair would be predicted as impossible. More generally, the MLE is unreliable for events with small nonzero counts as well as for those with zero counts. In the language modeling literature, the term smoothing is used to refer to methods for adjusting the probability estimates of small-count events away from the MLE to try to alleviate its unreliability. Our proposals address the zero-count problem exclusively, and we rely on existing techniques to smooth other small counts. Previous proposals for the zero-count problem (Good, 1953; Jelinek et al., 1992; Katz, 1987; Church & Gale, 1991) adjust the MLE so that the total probability of seen word pairs is less than one, leaving some probability mass to be redistributed among the unseen pairs. In general, the adjustment involves either interpolation, in which the MLE is used in linear combination with an estimator guaranteed to be nonzero for unseen word pairs, or discounting, in which a reduced MLE is used for seen word pairs, with the probability mass left over from this reduction used to model unseen pairs. The back-off method of Katz (1987) is a prime example of discounting:   P (w 2 jw 1 ) =  (  P d (w 2 jw 1 ) c(w 1 ; w 2 ) ? 0  ff(w 1 )P r (w 2 jw 1 ) c(w 1 ; w 2 ) = 0  ; (2) where P d represents the Good-Turing discounted estimate (Katz, 1987) for seen word pairs, and  P r denotes the model for probability redistribution among the unseen word pairs. ff(w 1 ) is a normalization factor. Since an extensive comparison study by Chen and Goodman (1996) indicated that back-off is better than interpolation for estimating bigram probabilities, we will not consider interpolation methods here; however, one could easily incorporate similarity-based estimates into an interpolation framework as well. In his original back-off model, Katz used P (w 2 ) as the model for predicting   P (w 2 jw 1 ) for unseen word pairs, that is, his model backed off to a unigram model for unseen bigrams. However, it is conceivable that backing off to a more detailed model than unigrams would be advantageous. Therefore, we generalize Katz's formulation by writing P r (w 2 jw 1 ) instead of P (w 2 ), enabling us to use similarity-based estimates for unseen word pairs instead of unigram frequency. Observe that similarity estimates are used for unseen word pairs only. We next investigate estimates for P r (w 2 jw 1 ) derived by averaging information from words that are distributionally similar to w 1 .  2.2 Combining Evidence  Similarity-based models make the following assumption: if word w  0  1 is \"similar\" to word w 1 , then  w  0  1 can yield information about the probability of unseen word pairs involving w 1 . We use a weighted average of the evidence provided by similar words, or neighbors, where the weight given to a particular word w  0  1 depends on its similarity to w 1 . More precisely, let W (w 1 ; w  0  1 ) denote an increasing function of the similarity between w 1 and  w  0  1 , and let S(w 1 ) denote the set of words most similar to w 1 . Then the general form of similarity model we consider is a W-weighted linear combination of predictions of similar words:  P SIM (w 2 jw 1 ) =  X  w  0  1 2S(w 1 )  W (w 1 ; w  0  1 ) norm(w 1 )  P (w 2 jw  0  1 ) ; (3) where norm(w 1 ) =  P  w  0  1 2S(w 1 ) W (w 1 ; w  0  1 ) is a normalization factor. According to this formula, w 2  is more likely to occur with w 1 if it tends to occur with the words that are most similar to w 1 . Considerable latitude is allowed in defining the set S(w 1 ), as is evidenced by previous work that can be put in the above form. Essen and Steinbiss (1992) and Karov and Edelman (1996) (implicitly) set S(w 1 ) = V 1 . However, it may be desirable to restrict S(w 1 ) in some fashion for efficiency reasons, especially if V 1 is large. For instance, in the language modeling application of Section 3, we use the closest k or fewer words w  0  1 such that the dissimilarity between w 1 and w  0  1 is less than a threshold value t; k and t are tuned experimentally. One can directly replace P r (w 2 jw 1 ) in the back-off equation (2) with P SIM (w 2 jw 1 ). However, other variations are possible, such as interpolating with the unigram probability P (w 2 ):  P r (w 2 jw 1 ) = flP (w 2 ) + (1 \\Gamma fl)P SIM (w 2 jw 1 ) :  This represents, in effect, a linear combination of the similarity estimate and the back-off estimate: if fl = 1, then we have exactly Katz's back-off scheme. In the language modeling task (Section 3) we set fl experimentally; to simplify our comparison of different similarity models for sense disambiguation (Section 4), we set fl to 0. It would be possible to make fl depend on w 1 , so that the contribution of the similarity estimate could vary among words. Such dependences are often used in interpolated models (Jelinek & Mercer, 1980; Jelinek et al., 1992; Saul & Pereira, 1997) and are indeed advantageous. However, since they introduce hidden variables, they require a more complex training algorithm, and we did not pursue that direction in the present work.  2.3 Measures of Similarity  We now consider several word similarity measures that can be derived automatically from the statistics of a training corpus, as opposed to being derived from manually-constructed word classes (Yarowsky, 1992; Resnik, 1992, 1995; Luk, 1995; Lin, 1997). Sections 2.3.1 and 2.3.2 discuss two related information-theoretic functions, the KL divergence and the Jensen-Shannon divergence. Section 2.3.3 describes the L 1 norm, a geometric distance function. Section 2.3.4 examines the confusion probability, which has been previously employed in language modeling tasks. There are, of course, many other possible functions; we have opted to restrict our attention to this reasonably diverse set. For each function, a corresponding weight function W (w 1 ; w  0  1 ) is given. The choice of weight function is to some extent arbitrary; the requirement that it be increasing in the similarity between  w 1 and w  0  1 is not extremely constraining. While clearly performance depends on using a good weight  function, it would be impossible to try all conceivable W (w 1 ; w  0  1 ). Therefore, in section 4.5, we describe experiments evaluating similarity-based models both with and without weight functions. All the similarity functions we describe depend on some base language model P (w 2 jw 1 ), which may or may not be the Katz discounted model  P (w 2 jw 1 ) from Section 2.1 above. While we discuss the complexity of computing each similarity function, it should be noted that in our current implementation, this is a one-time cost: we construct the jV 1 j \\Theta jV 1 j matrix of word-to-word similarities before any parameter training takes place. 2.3.1 KL divergence  The Kullback-Leibler (KL) divergence is a standard information-theoretic measure of the dissimilarity between two probability mass functions (Kullback, 1959; Cover & Thomas, 1991). We can apply it to the conditional distributions induced by words in V 1 on words in V 2 : D(w 1 kw  0  1 ) =  X  w 2  P (w 2 jw 1 ) log  P (w 2 jw 1 )  P (w 2 jw  0  1 )  : (4) D(w 1 kw  0  1 ) is non-negative, and is zero if and only if P (w 2 jw 1 ) = P (w 2 jw  0  1 ) for all w 2 . However, the KL divergence is non-symmetric and does not obey the triangle inequality. For D(w 1 kw  0  1 ) to be defined it must be the case that P (w 2 jw  0  1 ) ? 0 whenever P (w 2 jw 1 ) ? 0. Unfortunately, this generally does not hold for MLEs based on samples; we must use smoothed estimates that redistribute some probability mass to zero-frequency events. But this forces the sum in (4) to be over all w 2 2 V 2 , which makes this calculation expensive for large vocabularies. Once the divergence D(w 1 kw  0  1 ) is computed, we set  WD (w 1 ; w  0  1 ) = 10  \\Gammafi D(w 1 jjw  0  1 )  :  The role of the free parameter fi is to control the relative influence of the neighbors closest to  w 1 : if fi is high, then WD (w 1 ; w  0  1 ) is non-negligible only for those w  0  1 that are extremely close to  w 1 , whereas if fi is low, distant neighbors also contribute to the estimate. We chose a negative exponential function of the KL divergence for the weight function by analogy with the form of the cluster membership function in related distributional clustering work (Pereira et al., 1993) and also because that is the form for the probability that w 1 's distribution arose from a sample drawn from the distribution of w  0  1 (Cover & Thomas, 1991; Lee, 1997). However, these reasons are heuristic rather than theoretical, since we do not have a rigorous probabilistic justification for similarity-based methods.  2.3.2 Jensen-Shannon divergence  A related measure is the Jensen-Shannon divergence (Rao, 1982; Lin, 1991), which can be defined as the average of the KL divergence of each of two distributions to their average distribution: J(w 1 ; w  0  1 ) = 1 2    D  `  w 1  fl fl fl fl  w 1 + w  0  1  2  '  +D  `  w  0  1  fl fl fl fl  w 1 + w  0  1  2  '  ; (5) where (w 1 + w  0  1 )=2 is shorthand for the distribution 1 2 (P (w 2 jw 1 ) + P (w 2 jw  0  1 )) :  Since the KL divergence is nonnegative, J(w 1 ; w  0  1 ) is also nonnegative. Furthermore, letting p(w 2 ) =  P (w 2 jw 1 ) and p  0  (w 2 ) = P (w 2 jw  0  1 ), it is easy to see that J(w 1 ; w  0  1 ) = H  `  p + p  0  2  '  \\Gamma  1 2  H(p) \\Gamma  1 2  H(p  0  ) ; (6) where H(q) = \\Gamma  P  w q(w) log q(w) is the entropy of the discrete density q. This equation shows that  J gives the information gain achieved by distinguishing the two distributions p and p  0  (conditioning on contexts w 1 and w  0  1 ) over pooling the two distributions (ignoring the distinction between w 1 and  w  0  1 ). It is also easy to see that J can be computed efficiently, since it depends only on those conditioned words that occur in both contexts. Indeed, letting C = fw 2 : p(w 2 ) ? 0; p  0  (w 2 ) ? 0g, and grouping the terms of (6) appropriately, we obtain  J(w 1 ; w  0  1 ) = log 2 + 1 2  X  w 2 2C  \\Phi  h  \\Gamma  p(w 2 ) + p  0  (w 2 )  \\Delta  \\Gamma h(p(w 2 )) \\Gamma h(p  0  (w 2 ))  \\Psi  ; where h(x) = \\Gammax log x. Therefore, J(w 1 ; w  0  1 ) is bounded, ranging between 0 and log 2, and smoothed estimates are not required because probability ratios are not involved. As in the KL divergence case, we set W J (w 1 ; w  0  1 ) = 10  \\Gammafi J(w 1 ;w  0  1 )  ; fi plays the same role as before.  2.3.3 L 1 norm  The L 1 norm is defined as  L(w 1 ; w  0  1 ) =  X  w 2  fi fi P (w 2 jw 1 ) \\Gamma P (w 2 jw  0  1 )  fi fi : (7) By grouping terms as before, we can express L(w 1 ; w  0  1 ) in a form depending only on the \"common\"  w 2 :  L(w 1 ; w  0  1 ) = 2 \\Gamma  X  w 2 2C  p(w 2 ) \\Gamma  X  w 2 2C  p  0  (w 2 ) +  X  w 2 2C  jp(w 2 ) \\Gamma p  0  (w 2 )j : It follows from the triangle inequality that 0  L(w 1 ; w  0  1 )  2, with equality to 0 if and only if there are no words w 2 such that both P (w 2 jw 1 ) and P (w 2 jw  0  1 ) are strictly positive. Since we require a weighting scheme that is decreasing in L, we set  WL (w 1 ; w  0  1 ) = (2 \\Gamma L(w 1 ; w  0  1 ))  fi  ;  with fi again free.  2  As before, the higher fi is, the more relative influence is accorded to the nearest neighbors. It is interesting to note the following relations between the L 1 norm, the KL-divergence, and the Jensen-Shannon divergence. Cover and Thomas (1991) give the following lower bound:  L(w 1 ; w  0  1 )   q  D(w 1 jjw  0  1 ) \\Delta 2 log b ;  where b is the base of the logarithm function. Lin (1991) notes that L is an upper bound for J :  J(w 1 ; w  0  1 )  L(w 1 ; w  0  1 ) :  2  We experimented with using 10  \\GammafiL(w 1 ;w  0  1 )  as well, but it yielded poorer performance results.  2.3.4 Confusion probability  Extending work by Sugawara, Nishimura, Toshioka, Okochi, and Kaneko (1985), Essen and Steinbiss (1992) used confusion probability to estimate word cooccurrence probabilities.  3  They report 14% improvement in test-set perplexity (defined below) on a small corpus. The confusion probability was also used by Grishman and Sterling (1993) to estimate the likelihood of selectional patterns. The confusion probability is an estimate of the probability that word w  0  1 can be substituted for word w 1 , in the sense of being found in the same contexts: PC (w  0  1 jw 1 ) =  X  w 2  P (w 1 jw 2 )P (w  0  1 jw 2 )P (w 2 )  P (w 1 ) = WC (w 1 ; w  0  1 ) (P (w 1 ) serves as a normalization factor). In contrast to the distance functions described above, PC  has the curious property that w 1 may not necessarily be the \"closest\" word to itself, that is, there may exist a word w  0  1 such that PC (w  0  1 jw 1 ) ? PC (w 1 jw 1 ); see Section 4.4 for an example. The confusion probability can be computed from empirical estimates provided all unigram estimates are nonzero (as we assume throughout). In fact, the use of smoothed estimates such as those provided by Katz's back-off scheme is problematic, because those estimates typically do not preserve consistency with respect to marginal estimates and Bayes's rule (that is, it may be that  P  w 2 P (w 1 jw 2 )P (w 2 ) 6= P (w 1 )). However, using consistent estimates (such as the MLE), we can safely apply Bayes's rule to rewrite PC as follows:  PC (w  0  1 jw 1 ) =  X  w 2  P (w 2 jw 1 )  P (w 2 )  \\Delta P (w 2 jw  0  1 )P (w  0  1 ) : (8) As with the Jensen-Shannon divergence and the L 1 norm, this sum requires computation only over the \"common\" w 2 's. Examination of Equation (8) reveals an important difference between the confusion probability and the functions D, J , and L described in the previous sections. Those functions rate w  0  1 as similar to w 1 if, roughly, P (w 2 jw  0  1 ) is high when P (w 2 jw 1 ) is. PC (w  0  1 jw 1 ), however, is greater for those w  0  1  for which P (w  0  1  ; w 2 ) is large when P (w 2 jw 1 )=P (w 2 ) is. When this ratio is large, we may think of  w 2 as being exceptional, since if w 2 is infrequent, we do not expect P (w 2 jw 1 ) to be large.  2.3.5 Summary  Several features of the measures of similarity listed above are summarized in Table 1. \"Base LM constraints\" are conditions that must be satisfied by the probability estimates of the base language model. The last column indicates whether the weight W (w 1 ; w  0  1 ) associated with each similarity function depends on a parameter that needs to be tuned experimentally.  "},{"aspect":"expcomparison","tweet":" 4.5 Performance of Similarity-Based Methods  Figure 1 shows the results of our experiments on the five test sets, using MLE-1 as the base language model. The parameter fi was always set to the optimal value for the corresponding training set. RAND, which is shown for comparison purposes, simply chooses the weights W (w 1 ; w  0  1 ) randomly.  S(w 1 ) was set equal to V 1 in all cases. The similarity-based methods consistently outperformed Katz's back-off method and the MLE (recall that both yielded error rates of about .5) by a large margin, indicating that information from other word pairs is very useful for unseen pairs when unigram frequency is not informative. The similarity-based methods also do much better than RAND, which indicates that it is not enough to simply combine information from other words arbitrarily: word similarity should be taken into account. In all cases, J edged out the other methods. The average improvement in using J instead of PC is .0082; this difference is significant to the .1 level (p ! :085), according to the paired t-test. The results for the MLE-o1 case are depicted in Figure 2. Again, we see the similarity-based methods achieving far lower error rates than the MLE, back-off, and RAND methods, and again,  J always performed the best. However, omitting singletons amplified the disparity between J and  PC : the average difference was :024, which is significant to the .01 level (paired t-test). An important observation is that all methods, including RAND, suffered a performance hit if singletons were deleted from the base language model. This seems to indicate that seen bigrams  L J PC  GUY 0.0 GUY 0.0 role 0.05 kid 1.17 kid 0.15 people 0.025 lot 1.40 thing 0.16 fire 0.021 thing 1.41 lot 0.17 GUY 0.018 reason 1.417 mother 0.182 work 0.016 break 1.42 answer 0.1832 man 0.012 ball 1.439 reason 0.1836 lot 0.0113 answer 1.44 doctor 0.187 job 0.01099 tape 1.449 boost 0.189 thing 0.01092 rest 1.453 ball 0.19 reporter 0.0106 Table 10: 10 closest words to the word \"guy\" for L, J , and PC , using MLE-o1 as the base language model. should be treated differently from unseen bigrams, even if the seen bigrams are extremely rare. We thus conclude that one cannot create a compressed similarity-based language model by omitting singletons without hurting performance, at least for this task. We now analyze the role of the parameter fi. Recall that fi appears in the weight functions for the Jensen-Shannon divergence and the L 1 norm:  W J (w 1 ; w  0  1 ) = 10  \\Gammafi J(w 1 ;w  0  1 )  ; WL (w 1 ; w  0  1 ) = (2 \\Gamma L(w 1 ; w  0  1 ))  fi  :  It controls the relative influence of the most similar words: their influence increases with higher values of fi.  Figure 3 shows how the value of fi affects disambiguation performance. Four curves are shown, each corresponding to a choice of similarity function and base language model. The error bars depict the average and range of error rates over the five disjoint test sets. It is immediately clear that to get good performance results, fi must be set much higher for the Jensen-Shannon divergence than for the L 1 norm. This phenomenon results from the fact that the range of possible values for J is much smaller than that for L. This \"compression\" of J values requires a large fi to scale differences of distances correctly. We also observe that setting fi too low causes substantially worse error rates; however, the curves level off rather than moving upwards again. That is, as long as a sufficiently large value is chosen, setting fi suboptimally does not greatly impact performance. Furthermore, the shape of the curves is the same for both base language models, suggesting that the relation between fi and test-set performance is relatively insensitive to variations in training data. The fact that higher values of fi seem to lead to better error rates suggests that fi's role is to filter out distant neighbors. To test this hypothesis, we experimented with using only the k most similar neighbors. Figure 4 shows how the error rate depends on k for different fixed values of fi.  The two lowest curves depict the performance of the Jensen-Shannon divergence and the L 1 norm when fi is set to the optimal value with respect to average test set performance; it appears that the more distant neighbors have essentially no effect on error rate because their contribution to the sum (9) is negligible. In contrast, when too low a value of fi is chosen (the upper two curves), distant neighbors are weighted too heavily. In this case, including more distant neighbors causes serious degradation of performance. Interestingly, the behavior of the confusion probability is different from these two cases: adding  0.25 0.3 0.35 0.4 0.45 0.5  Error Rates on Test Sets, Base Language Model MLE1 T1 T2 T3 T4 T5 RAND P_C L J Figure 1: Error rates for each test set, where the base language model was MLE-1. The methods, going from left to right, are RAND , PC , L, and J . The performances shown are for settings of fi  that were optimal for the corresponding training set. fi ranged from 4:0 to 4:5 for L and from 20 to 26 for J . more neighbors actually improves the error rate. This seems to indicate that the confusion probability is not correctly ranking similar words in order of informativeness. However, an alternative explanation is that PC is at a disadvantage only because it is not being employed in the context of a tunable weighting scheme. To distinguish between these two possibilities, we ran an experiment that dispensed with weights altogether. Instead, we took a vote of the k most similar neighbors: the alternative chosen as more likely was the one preferred by a majority of the most similar neighbors (note that we ignored the  degree to which alternatives were preferred). The results are shown in Figure 5. We see that the k most similar neighbors according to J and L were always more informative than those chosen according to the confusion probability, with the largest performance gaps occurring for low k (of course, all methods performed the same for k = 1000, since in that case they were using the same set of neighbors). This graph provides clear evidence that the confusion probability is not as good a measure of the informativeness of other words.  "}]}