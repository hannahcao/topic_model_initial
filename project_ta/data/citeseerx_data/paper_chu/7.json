{"user_name":" Probabilistic Latent Semantic Indexing  Proceedings of the Twenty-Second Annual International SIGIR Conference on Research and Development in Information Retrieval ","user_timeline":[{"aspect":"abstract","tweet":" Abstract  Probabilistic Latent Semantic Indexing is a novel approach to automated document indexing which is based on a statistical latent class model for factor analysis of count data. Fitted from a training corpus of text documents by a generalization of the Expectation Maximization algorithm, the utilized model is able to deal with domain--specific synonymy as well as with polysemous words. In contrast to standard Latent Semantic Indexing (LSI) by Singular Value Decomposition, the probabilistic variant has a solid statistical foundation and defines a proper generative data model. Retrieval experiments on a number of test collections indicate substantial performance gains over direct term matching methods as well as over LSI. In particular, the combination of models with different dimensionalities has proven to be advantageous. "},{"aspect":"expanalysis","tweet":" 4.5 Factor Representation: An Example  In order to visualize the factor solution found by PLSA we present an elucidating example. We have performed exper-  0.2 0.4 0.6 0.8 1 0.2 0.4 0.6 0.8 1 0.2 0.4 0.6 0.8 1 0.2 0.4 0.6 0.8 1  aid food medical people un war 0.2 0.4 0.6 0.8 1 0.2 0.4 0.6 0.8 1 0.2 0.4 0.6 0.8 1 0.2 0.4 0.6 0.8 1  aid food medical people un war 0.2 0.4 0.6 0.8 1 0.2 0.4 0.6 0.8 1 0.2 0.4 0.6 0.8 1 0.2 0.4 0.6 0.8 1  aid food medical people un war 0.2 0.4 0.6 0.8 1 0.2 0.4 0.6 0.8 1 0.2 0.4 0.6 0.8 1 0.2 0.4 0.6 0.8 1  aid food medical people un war Figure 2: Folding in a query conisting of the terms \"aid\", \"food\", \"medical\", \"people\", \"UN\", and \"war\": evolution of posterior probabilities and the mixing proportions P (zjq) (rightmost column in each bar plot) for the four factors depicted in Table 2 after 1 (first row), 2 (second row), 3 (third row), and 20 (fourth row) iterations. iments with the TDT-1 collection, which contains 15,862 documents of broadcast news stories [8].  1  Stop words have been eliminated by a standard stop word list, no stemming or further preprocessing has been performed. Table 1 shows a reduced representation of 4 factors from a 128 factor solution.  The first two factors have been selected as the ones with the highest probablity to generate the word \"flight\", the last two factors have the highest probability to generate the word \"love\". It is interesting to see that the first two factors indeed capture two different types of usage for the term \"flight\": flights with planes and flights with space ships/shuttles. Similarly the last two factors capture two distinguishable contexts in which the word \"love\" occurs in  1  Since the TDT-1 collection contains documents on topics and events most readers will be familiar with, this collection has been preferred over the test collections utilized in Section 6.  MED CRAN CACM CISI precision improvement precision improvement precision improvement precision improvement cos+tf 44.3 - 29.9 - 17.9 - 12.7 - LSI 51.7 +16.7    28.7-4.0    16.0-11.6 12.7 \\Sigma0:0  PLSI-U 63.1 +42.4 32.8 +9.7 19.2 +7.2 14.0 +10.2 PLSI-Q 63.9 +44.2 35.1 +17.4 22.9 +27.9 18.8 +48.0 PLSI-U    67.5 +52.4 33.3 +11.4 19.5 +8.9 14.7 +15.7 PLSI-Q    66.3 +49.7 37.5 +25.4 26.8 +49.7 20.1 +58.3  cos+tfidf 49.0 - 35.2 - 21.9 - 20.2 - LSI 64.6 +31.8 38.7 +9.9 23.8 +8.7 21.9 +8.4 PLSI-U 69.5 +41.8 38.9 +10.5 25.3 +15.5 23.3 +15.3 PLSI-Q 63.2 +29.0 38.6 +9.7 26.6 +21.5 23.1 +14.4 PLSI-U    72.1 +47.1 40.4 +14.8 27.6 +26.0 24.6 +21.8  PLSI-Q    66.3 +35.3 40.1 +13.9 28.3 +29.2 24.4 +20.8 Table 3: Average precision results and relative improvement w.r.t. the baseline method (cos+tf and cos+tfidf, respectively) for the 4 standard test collections. Compared are LSI, PLSI, and the two PLSI variants (PLSI-U, PLSI-Q) as well as results obtained by combining PLSI models (PLSI-U    and PLSI-Q    , respectively). An asterix for LSI indicates that no performance gain could be achieved over the baseline, the result at 256 dimensions with a 1 : 2 combination with the baseline score is reported in this case. the TDT-1 collection: real love in the context of family life as opposed to staged love in the sense of \"Hollywood\".  "},{"aspect":"expdata","tweet":" 6 Experimental Results  The performance of PLSI has been systematically compared with the standard term matching method based on the raw term frequencies (tf) and their combination with the inverse document frequencies (tfidf), as well as with LSI. We have utilized the following four medium--sized standard document collection: (i) MED (1033 document abstracts from the National Library of Medicine), (ii) CRAN (1400 document abstracts on aeronautics from the Cranfield Institute of Technology) , (iii) CACM (3204 abstracts from the CACM journal) , and (iv) CISI (1460 abstracts in library science from the Institute for Scientific Information). The condensed results in terms of average precision recall (at the 9 recall levels 10% \\Gamma 90%) are summarized in Table 3. A selection of average precision recall curves can be found in Figure 3.  "},{"aspect":"background","tweet":" 1 Introduction  With the advent of digital databases and communication networks, huge repositories of textual data have become available to a large public. Today, it is one of the great challenges in the information sciences to develop intelligent interfaces for human--machine interaction which support computer users in their quest for relevant information. Although the use of elaborate ergonomic elements like computer graphics and visualization has proven to be extremely fruitful to facilitate and enhance information access, progress on the more fundamental question of machine intelligence  is ultimately necessary to ensure substantial progress on this issue. In order for computers to interact more naturally  with humans, one has to deal with the potential ambivalence, impreciseness, or even vagueness of user requests, and has to recognize the difference between what a user might say or do and what she or he actually meant or intended.  One typical scenario of human--machine interaction in information retrieval is by natural language queries: the user formulates a request, e.g., by providing a number of keywords or some free-form text, and expects the system to return the relevant data in some amenable representation, e.g., in form of a ranked list of relevant documents. Many retrieval methods are based on simple word matching strategies to determine the rank of relevance of a document with respect to a query. Yet, it is well known that literal term matching has severe drawbacks, mainly due to the ambivalence of words and their unavoidable lack of precision as well as due to personal style and individual differences in word usage.  Latent Semantic Analysis (LSA) [1] is an approach to automatic indexing and information retrieval that attempts to overcome these problems by mapping documents as well as terms to a representation in the so--called latent semantic  space. LSA usually takes the (high dimensional) vector space representation of documents based on term frequencies [14] as a starting point and applies a dimension reducing linear projection. The specific form of this mapping is determined by a given document collection and is based on a Singular Value Decomposition (SVD) of the corresponding term/document matrix. The general claim is that similarities between documents or between documents and queries can be more reliably estimated in the reduced latent space representation than in the original representation. The rationale is that documents which share frequently co-occurring terms will have a similar representation in the latent space, even if they have no terms in common. LSA thus performs some sort of noise reduction and has the potential benefit to detect synonyms as well as words that refer to the same topic. In many applications this has proven to result in more robust word processing. Although LSA has been applied with remarkable success in different domains including automatic indexing (Latent Semantic Indexing, LSI) [1, 3], it has a number of deficits, mainly due to its unsatisfactory statistical foundation. The primary goal of this paper is to present a novel approach to LSA and factor analysis -- called Probabilistic Latent Semantic Analysis (PLSA) -- that has a solid statistical foundation, since it is based on the likelihood principle and defines a proper generative model of the data. This implies in particular that standard techniques from statistics can be applied for questions like model fitting, model combination, and complexity control. In addition, the factor representation obtained by PLSA allows to deal with polysemous words and to explicitly distinguish between different meanings and different types of word usage.  "},{"aspect":"expintro","tweet":""},{"aspect":"problemdef","tweet":""},{"aspect":"solution","tweet":" 2 The Aspect Model  The core of PLSA is a statistical model which has been called  aspect model [7, 15]. The latter is a latent variable model for general co-occurrence data which associates an unobserved class variable z 2 Z = fz1 ; : : : ; zKg with each observation, i.e., with each occurrence of a word w2W = fw1 ; : : : ; wMg  in a document d2D=fd1 ; : : : ; dN g. In terms of a generative model it can be defined in the following way:  ffl select a document d with probability P (d),  ffl pick a latent class z with probability P (zjd),  ffl generate a word w with probability P (wjz). As a result one obtains an observed pair (d; w), while the latent class variable z is discarded. Translating this process into a joint probability model results in the expression  P (d; w) = P (d)P (wjd) ; where (1)  P (wjd) =  X  z2Z  P (wjz)P (zjd) : (2) Essentially, to derive (2) one has to sum over the possible choices of z which could have generated the observation. The aspect model is a statistical mixture model [9] which is based on two independence assumptions: First, observation pairs (d; w) are assumed to be generated independently; this essentially corresponds to the `bag--of--words' approach. Secondly, the conditional independence assumption is made that conditioned on the latent class z, words w are generated independently of the specific document identity d.  Given that the number of states is smaller than the number of documents (K  N ), z acts as a bottleneck variable in predicting w conditioned on d.  Notice that in contrast to document clustering models document--specific word distributions P (wjd) are obtained by a convex combination of the aspects or factors P (wjz). Documents are not assigned to clusters, they are characterized by a specific mixture of factors with weights P (zjd). These mixing weights offer more modeling power and are conceptually very different from posterior probabilities in clustering models and (unsupervised) naive Bayes models (cf. [7]). Following the likelihood principle, one determines P (d),  P (zjd), and P (wjz) by maximization of the log--likelihood function  L =  X  d2D  X  w2W  n(d; w) log P (d; w) ; (3) where n(d; w) denotes the term frequency, i.e., the number of times w occurred in d. It is worth noticing that an equivalent symmetric version of the model can be obtained by inverting the conditional probability P (zjd) with the help of Bayes' rule, which results in  P (d; w) =  X  z2Z  P (z)P (wjz)P (djz): (4) This is just a re-parameterized version of the generative model described by (1), (2).  3 Model Fitting with Tempered EM  The standard procedure for maximum likelihood estimation in latent variable models is the Expectation Maximization (EM) algorithm [2]. EM alternates two steps: (i) an expectation (E) step where posterior probabilities are computed for the latent variables z, based on the current estimates of the parameters, (ii) an maximization (M) step, where parameters are updated for given posterior probabilities computed in the previous E--step. For the aspect model in the symmetric parameterization Bayes' rule yields the E--step P (zjd; w) =  P (z)P (djz)P (wjz)  P  z  0 P (z  0  )P (djz  0  )P (wjz  0  )  ; (5) which is the probability that a word w in a particular document or context d is explained by the factor corresponding to z. By standard calculations one arrives at the following M--step re-estimation equations P (wjz) =  P  d n(d; w)P (zjd;w)  P  d;w  0 n(d; w  0  )P (zjd; w  0  )  ; (6) P (djz) =  P  w  n(d; w)P (zjd;w)  P  d  0  ;w n(d  0  ; w)P (zjd  0  ; w) ; (7) P (z) = 1  R  X  d;w  n(d; w)P (zjd;w); R j  X  d;w  n(d; w) : (8) Alternating (5) with (6)--(8) defines a convergent procedure that approaches a local maximum of the log--likelihood in (3). So far we have focused on maximum likelihood estimation or, equivalently, word perplexity reduction. One has, however, to distinguish between the predictive performance of the model on training data and the expected performance on unseen test data. In particular, it is to naive to assume that a model will generalize well on new data just based on the fact that it might achieve low perplexity on training data. To derive conditions under which generalization on unseen data can be guaranteed is actually the fundamental problem of statistical learning theory. Here, we propose a generalization of maximum likelihood for mixture models -- called tempered EM (TEM) -- which is based on entropic regularization and is closely related to a method known as  deterministic annealing [13]. Since a principled derivation of TEM is beyond the scope of this paper (the interested reader is referred to [12, 7]), we will present the necessary modification of standard EM in an  ad hoc manner. Essentially, one introduces a control parameter  fi (inverse computational temperature) and modifies the E-step in (5) according to P fi (zjd; w)= P (z) [P (djz)P (wjz)]  fi  P  z  0 P (z  0  ) [P (djz  0  )P (wjz  0  )]  fi : (9) Notice that fi = 1 results in the standard E--step, while for  fi ! 1 the likelihood part in Bayes' formula is discounted (additively on the log--scale). It can be shown, that TEM minimizes an objective function known as the free energy [11] and hence defines a convergent algorithm. While temperature--based generalizations of EM and related algorithms for optimization are often used  as a homotopy or continuation method to avoid unfavorable local extrema, the main advantage of TEM in our context is to avoid overfitting. Somewhat contrary to the spirit of annealing as a continuation method we propose to utilize (9) to temper EM by \"heating\". In order to determine the optimal value of fi we propose to make use of some held--out portion of the data. This idea can be implemented by the following scheme: i. Set fi / 1 and perform EM until the performance on held--out data deteriorates (early stopping).  ii. Decrease fi, e.g., by setting fi / jfi with some rate parameter j ! 1. iii. As long as the performance on held-out data improves continue TEM iterations at this value of fi.  iv. Stop on fi, i.e., stop when decreasing fi does not yield further improvements, otherwise goto step (ii). v. Perform some final iterations using both, training and held-out data. In our experiments, the typical number of iterations TEM performed starting from randomized initial conditions was 40 \\Gamma 60, where each iteration requires one pass through the data, i.e., of the order of R \\Delta K arithmetical operations.  4 Probabilistic Latent Semantic Analysis 4.1 Latent Semantic Analysis  As mentioned in the introduction, the key idea of LSA [1] is to map documents (and by symmetry terms) to a vector space of reduced dimensionality, the latent semantic  space. This mapping is computed by decomposing the term/document matrix N with SVD, N = U\\SigmaV  t  , where  U and V are orthogonal matrices U  t  U = V  t  V = I and the diagonal matrix \\Sigma contains the singular values of N.  The LSA approximation of N is computed by thresholding all but the largest K singular values in \\Sigma to zero (= ~ \\Sigma),  which is rank K optimal in the sense of the L2-matrix norm as is well-known from linear algebra, i.e., one obtains the approximation ~ N = U ~ \\SigmaV  t   U\\SigmaV  t  = N. Note that the  L2--norm approximation does not prohibit entries of ~  N to be negative.  4.2 Geometry of the Aspect Model  Now consider the class-conditional multinomial distributions  P (\\Deltajz ) over the vocabulary in the aspect model which can be represented as points on the M \\Gamma 1 dimensional simplex of all possible multinomials. Via its convex hull, this set of K points defines a K \\Gamma 1 dimensional sub-simplex. The modeling assumption expressed by (2) is that all conditional distributions P (\\Deltajd) are approximated by a multinomial representable as a convex combination of the class-conditionals  P (\\Deltajz ). In this geometrical view, the mixing weights P (zjd) correspond exactly to the coordinates of a document in that sub-simplex. A simple sketch of the geometry is shown in Figure 1. This demonstrates that despite of the discreteness of the latent variables introduced in the aspect model, a continuous latent space is obtained within the space of all multinomial distributions. Since the dimensionality of the sub-simplex is K \\Gamma 1 as opposed to M \\Gamma 1 for the complete  + P(w|d) P(w|z ) P(w|z ) P(w|z ) 3  spanned sub-simplex simplex 2 1  0 embedding KL divergence projection Figure 1: Sketch of the probability sub-simplex spanned by the aspect model. probability simplex, this can also be thought of in terms of dimensionality reduction and the sub-simplex can be identified with a probabilistic latent semantic space. 4.3 Mixture Decomposition vs. Singular Value Decomposition   To stress this point and to clarify the relation to LSA, let us rewrite the aspect model as parameterized by (4) in matrix notation. Hence define matrices by  U= (P (d i jzk)) i;k ,   V= (P (w j jzk )) j;k , and  \\Sigma = diag(P (zk ))k . The joint probability model P can then be written as a matrix product  P =  U  \\Sigma  V  t  . By comparing this decomposition with the SVD decomposition in LSA, one can point out the following re-interpretation of concepts of linear algebra: i. The weighted sum over outer products between rows of   U and   V reflects conditional independence in PLSA. ii. The left/right eigenvectors in SVD are seen to correspond to the factors P (wjz) and the component distributions  P (djz) of the aspect model. iii. The mixing proportions P (z) in PLSA substitute the singular values of the SVD in LSA. Despite this similarity, there is also a fundamental difference between PLSA and LSA, which is the objective function utilized to determine the optimal decomposition /approximation. In LSA, this is the L2--norm or Frobenius norm, which corresponds to an implicit additive Gaussian noise assumption on counts. In contrast, PLSA relies on the likelihood function of multinomial sampling and aims at an explicit maximization of the predictive power of the model. On the modeling side this offers important advantages, for example, the mixture approximation P of the cooccurrence table is a well-defined probability distribution and factors have a clear probabilistic meaning in terms of mixture component distributions.  4.4 Kullback--Leibler Projection vs. Orthogonal Projection  Returning to the geometrical view of the aspect model as sketched in Figure 1, it is interesting to reveal the projection principle which is implicitly used in the aspect model.  \"plane\" \"space shuttle\" \"family\" \"Hollywood\" plane space home film airport shuttle family movie crash mission like music flight astronauts love new safety launch kids best aircraft station mother hollywood air crew life love passenger nasa happy actor board satellite friends entertainment airline earth cnn star Table 1: Four factors from a 128 factor decomposition of the TDT-1 corpus. Factor are represented by their 10 most probable words, i.e., the words are ordered according to  P (wjz). \"Bosnia\" \"Iraq\" \"Rwanda\" \"Kobe\" un iraq refugees building bosnian iraqi aid city serbs sanctions rwanda people bosnia kuwait relief rescue serb un people buildings sarajevo council camps workers nato gulf zaire kobe peacekeepers saddam camp victims nations baghdad food area peace hussein rwandan earthquake Table 2: Four additional factors from the 128 factor decomposition of the TDT-1 corpus (cf. Table 1). Rewriting the log--likelihood in (3) one arrives at L =  X  d2D  n(d)  \" X  w2W  n(d; w) n(d)  log P (wjd) + log P (d)  #  : (10) The first term in brackets corresponds to the negative Kullback--Leibler (KL) divergence (or cross--entropy) between the empirical distribution of words in a document  P (wjd) j n(d; w)=n(d) and the model distribution P (wjd). For fixed factors P (wjz) maximizing the log--likelihood w.r.t the mixing proportions P (zjd) thus amounts to projecting  P (wjd) on the subspace spanned by the factors based on the KL--divergence. This is very different from any type of squared deviation which would result in an orthogonal projection (cf. [10] for more details on the geometry of statistical models).  "},{"aspect":"expcomparison","tweet":""}]}